[
  {
    "objectID": "viz/Table/Table.html",
    "href": "viz/Table/Table.html",
    "title": "Pretty Tables",
    "section": "",
    "text": "Kays R, Dunn RR, Parsons AW, Mcdonald B, Perkins T, Powers S, Shell L, McDonald JL, Cole H, Kikillus H, Woods L, Tindle H, Roetman P (2020) The small home ranges and large local ecological impacts of pet cats. Animal Conservation. doi:10.1111/acv.12563\nMcDonald JL, Cole H (2020) Data from: The small home ranges and large local ecological impacts of pet cats [United Kingdom]. Movebank Data Repository. doi:10.5441/001/1.pf315732\nThomas Mock (2022). Tidy Tuesday: A weekly data project aimed at the R ecosystem.GitHub"
  },
  {
    "objectID": "Resources.html",
    "href": "Resources.html",
    "title": "Data Viz",
    "section": "",
    "text": "Pretty Tables\n\n\n\n\n\n\n\nKarol Orozco\n\n\nFebruary 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHurricanes\n\n\n\n\n\n\nKarol Orozco\n\n\nFebruary 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPet Cats UK\n\n\n\n\n\n\nKarol Orozco\n\n\nFebruary 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVoters Registrations\n\n\n\n\n\n\nKarol Orozco\n\n\nFebruary 5, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "viz/TidyTuesday/PetCats.html",
    "href": "viz/TidyTuesday/PetCats.html",
    "title": "Pet Cats UK",
    "section": "",
    "text": "This dataset is full of exciting columns that provide insight into the secret lives of cats. I must admit that dogs are my favorite pets, but many of my friends love cats, and I always hear stories about how independent their pets are, which made me want to learn more about them.\nWell, I was born in Colombia, and during my time there, I always remember the cats running and fighting on my roof, which would keep me up at night for a few hours. Then, a few weeks later, I heard tiny meows and noticed the presence of some kittens, which made me suspect that it was another kind of fun that was going on there.\nAccording to the American Society for the Prevention of Cruelty to Animals ASPCA, a female cat can have an average of four to six kittens per litter and may have one to two litters per year. Some cat owners prefer to spay or neuter their cats.\nThe TidyTuesday, weekly data project, posted on January 31st, 2023, a dataset from the Movebank for Animal Tracking Data, which includes cats’ characteristics (such as age, sex, neuter status, hunting habits) and time-stamped GPS pings. I’ll take a look at the reproductive status of the cats and improve the graph offered by the Twitter user @OluwafemOyedele.\nYou can find the data on the TidyTuesday Github site- “Pet Cats UK”.\nHere is the list of cat names, in case you need some inspiration."
  },
  {
    "objectID": "viz/TidyTuesday/PetCats.html#reference",
    "href": "viz/TidyTuesday/PetCats.html#reference",
    "title": "Pet Cats UK",
    "section": "Reference",
    "text": "Reference\nKays R, Dunn RR, Parsons AW, Mcdonald B, Perkins T, Powers S, Shell L, McDonald JL, Cole H, Kikillus H, Woods L, Tindle H, Roetman P (2020) The small home ranges and large local ecological impacts of pet cats. Animal Conservation. doi:10.1111/acv.12563\nMcDonald JL, Cole H (2020) Data from: The small home ranges and large local ecological impacts of pet cats [United Kingdom]. Movebank Data Repository. doi:10.5441/001/1.pf315732\nThomas Mock (2022). Tidy Tuesday: A weekly data project aimed at the R ecosystem.GitHub"
  },
  {
    "objectID": "viz/Voter Registrations Graph/voter.html",
    "href": "viz/Voter Registrations Graph/voter.html",
    "title": "Voters Registrations",
    "section": "",
    "text": "Let’s practice our visualization skills with ggplot2 and recreate the graph from the [“Voter Registrations Are Way, Way Down During The Pandemic”] (https://fivethirtyeight.com/features/voter-registrations-are-way-way-down-during-the-pandemic/) article.\nFirst, Let’s get and prepare the data"
  },
  {
    "objectID": "viz/Voter Registrations Graph/voter.html#the-graph",
    "href": "viz/Voter Registrations Graph/voter.html#the-graph",
    "title": "Voters Registrations",
    "section": "The Graph",
    "text": "The Graph\n\n\nShow the code\nggplot(vreg, aes(x= Month, y= change, fill = Color))+\n  \n    geom_col()+ \n \n  geom_hline( yintercept = 0, color= \"black\")+\n\n  geom_rect(data = data.frame(Jurisdiction = \"Arizona\"), \n              aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n              color = \"darkgrey\", \n              fill = \"white\", \n              alpha = 0, \n              linetype = \"dotted\", \n              inherit.aes = FALSE)+\n  geom_rect(data = data.frame(Jurisdiction = \"California\"), \n            aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n            color = \"darkgrey\", \n            fill = \"white\", \n            alpha = 0, \n            linetype = \"dotted\", \n            inherit.aes = FALSE)+\n   geom_rect(data = data.frame(Jurisdiction = \"Colorado\"), \n             aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n             color = \"darkgrey\", \n             fill = \"white\", \n             alpha = 0, \n             linetype = \"dotted\", \n             inherit.aes = FALSE)+\n  geom_rect(data = data.frame(Jurisdiction = \"Delaware\"), \n            aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n            color = \"darkgrey\", fill = \"white\", \n            alpha = 0, \n            linetype = \"dotted\", \n            inherit.aes = FALSE)+\n  geom_rect(data = data.frame(Jurisdiction = \"Florida\"), \n            aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n            color = \"darkgrey\", \n            fill = \"white\", \n            alpha = 0, \n            linetype = \"dotted\", inherit.aes = FALSE)+\n  geom_rect(data = data.frame(Jurisdiction = \"Georgia\"), \n            aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n            color = \"darkgrey\", \n            fill = \"white\", \n            alpha = 0, \n            linetype = \"dotted\", \n            inherit.aes = FALSE)+\n  geom_rect(data = data.frame(Jurisdiction = \"Illinois\"), \n            aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n            color = \"darkgrey\", \n            fill = \"white\", \n            alpha = 0, \n            linetype = \"dotted\", \n            inherit.aes = FALSE)+\n  \n    facet_wrap(~Jurisdiction, scales = \"free_y\")+\n\n  scale_fill_identity(guide= FALSE)+\n  scale_x_discrete(limits=c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\"), \n                   breaks=c(\"Jan\",\"May\"))+\n  scale_y_continuous(labels = label_number_si(a =! 0), n.breaks = 4)+\n\n  labs(\n        x=\"\",\n        y=\"\",\n      title = \"Voter registration dropped dramatically during the pandemic\",\n      subtitle = \"Difference in the number of newly registered voters for each month in 2020 compared to the same month in 2016\",\n      caption = \"Some states treat voters who move between counties within a state as new registrants because they're unregistered from their old county and nearly registered in the new ones.          \",\n      \n      tag= \"FiveThirtyEight\") +\n\n    theme_classic()+\n    theme(\n          axis.line.y=element_blank(),\n          axis.line.x = element_blank(),\n          axis.ticks = element_blank(),\n          axis.text.y = element_text(size = 6.5, color = \"gray\"), \n          axis.text.x = element_text(size= 6.5, color = \"gray\"),\n          \n          \n          plot.title = element_text(size =9, face = \"bold\", hjust = 0.55),\n          plot.title.position = \"plot\",\n          plot.subtitle= element_text(size = 8, hjust = 0.55),\n          \n          plot.caption = element_text(hjust = 0, size = 6, color = \"grey50\",\n                                      margin = margin(r=5)),\n          plot.background = element_rect(fill= \"white\"),\n          plot.tag.position = \"bottom\",\n          plot.tag = element_text(size= 5, color = \"gray\", hjust =0.1, \n                                  margin=margin(t=1, \n                                                r=5, \n                                                b=1, \n                                                l=20, \n                                                unit=\"pt\")),\n          panel.grid.minor.y = element_blank(),\n          panel.grid.major.y = element_line(size= 0.1, color= \"lightgrey\",\n                                            linetype= \"solid\"),\n          panel.background = element_rect(fill = \"white\"),\n          panel.border = element_blank(),\n          panel.spacing = unit(1, \"lines\"),\n          \n          strip.background= element_rect(fill= \"white\", linetype = \"blank\"),\n          strip.text = element_text(color= \"black\", face= \"bold\"),\n          strip.text.x = element_text(face = \"bold\", size= 7),\n\n          \n          legend.title = element_blank(),\n          legend.position = \"none\"\n    \n)"
  },
  {
    "objectID": "viz/Hurricanes Graph/hurricanes.html",
    "href": "viz/Hurricanes Graph/hurricanes.html",
    "title": "Hurricanes",
    "section": "",
    "text": "Hello there,\nToday, I will be recreating the graph from the article “Why Past Hurricane Seasons Don’t Tell Us Much About The Future” by Anna Wiederkehr.\nYou can find the raw data in the National Oceanic and Atmospheric Administration’s HURDAT2 database, which has records of all Atlantic basin tropical cyclones dating back to 1851.\nThis exercise focuses on practicing our data visualization skills with ggplot2, so I’ll skip the data manipulation steps. You can find the dataset for the graph in my GitHub repository.\n\n\nShow the code\n## library(tidyverse)\n## library(ggplot2)\n## library(geomtextpath)\n## library(RCurl)\n## library(grid)\n## library(gridExtra)\n\nx <- getURL(\"https://raw.githubusercontent.com/karolo89/Raw_Data/main/Hurricane.csv\")\nhurricane <- read.csv(text = x)\n\n\n\n\nShow the code\ngraph1 <- ggplot(hurricane, aes(year,average, color = type)) +\n  geom_step(size=.7) +\n  \n## Adding the text\n  geom_textline(aes(label = ifelse(type == \"avg_h_15year\", \n                    \"All Hurricanes\",\n                    \"Major Hurricanes\"), \n                    y = average + .8), \n                text_smoothing = 50, \n                fontface = 2,\n                hjust = 0,  \n                linetype = 0, \n                size = 3) +\n  annotate(\"text\", x = 1886, y = 2.2, label = \"Category 3 - 5\", hjust = 0,\n           color = \"#3b2d74\", size= 2.5) +\n\n## Scales\n  \n  scale_x_continuous(breaks = seq(1860,2020,20)) +\n  scale_y_continuous(limits = c(0,8.5),\n                     breaks = seq(2,8,2),\n                     expand = c(0,0)) +\n  scale_color_manual(values = c(\"#735ad2\", \"#3b2d74\")) +\n\n## Labs\n  \n  labs(\n    y = \" \",\n    x = \" \", \n    title = \"15-year average recorded Atlantic basin hurricanes, 1851-2019\") +\n  \n## Theme \n  \n  theme_minimal() +\n  \n  theme(\n    \n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    \n    ## Customize x axis\n    axis.ticks.x = element_line(colour = \"#e1e1e1\"),\n    axis.ticks.length.x =unit(0.3,\"cm\"),\n    axis.line.x.bottom = element_line(colour = \"#313131\"),\n    \n    ## axis text\n\n    axis.text = element_text(size= 6, color= \"#8f8f8f\", \n                             face = \"bold\"),\n    \n    ## We don't want a legend\n    \n    legend.position = \"none\",\n    \n    ## title\n    plot.title = element_text(size =8, face = \"bold\", colour= \"#545454\",\n                              hjust = 0.50, vjust = 1)\n  )\ngraph1\n\n\n\n\n\nI used the https://imagecolorpicker.com/en tool to get the hex colors from the original graph.\nWe are still missing the footer, so let’s add it.\n\n\nShow the code\nfooter<- grobTree( \n                  textGrob(\"      FiveThirtyEight\", x=unit(.05, \"npc\"), \n                      gp=gpar(col=\"#868686\", \n                              family=\"sans\", \n                              fontsize= 5.5,\n                              fontface= \"bold\"),\n                      hjust=0,\n                      vjust = -2),\n                  \n                  textGrob(\"SOURCES: HURDAT2, VECCHI ET AL      \",\n                        x=unit(1, \"npc\"), \n                        gp=gpar(col=\"#868686\", \n                                family= \"sans\",\n                                fontsize=5.5,\n                              fontface= \"bold\"),\n                        hjust=1,\n                      vjust = -2))\n                  \n# Plot All Together\n\n plt.final <- grid.arrange(graph1, footer, heights=unit(c(0.72, 0.03), \n                                                    c(\"npc\", \"npc\")))"
  },
  {
    "objectID": "Projects.html",
    "href": "Projects.html",
    "title": "My Work",
    "section": "",
    "text": "Time Series\n\n\n\n\n\n\nKarol Orozco\n\n\nDec 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHouse Prices in Portland, OR\n\n\n\n\n\n\nKarol Orozco\n\n\nDec 1, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/RegressionModel/RegressionModel.html",
    "href": "posts/RegressionModel/RegressionModel.html",
    "title": "House Prices in Portland, OR",
    "section": "",
    "text": "The goal is to build a classification model to predict the type of median housing prices in Portland, OR and its metropolitan area."
  },
  {
    "objectID": "posts/RegressionModel/RegressionModel.html#get-the-data",
    "href": "posts/RegressionModel/RegressionModel.html#get-the-data",
    "title": "House Prices in Portland, OR",
    "section": "Get the Data",
    "text": "Get the Data\n\n\nShow the code\nraw_pdx <- read.csv(\"https://raw.githubusercontent.com/karolo89/Raw_Data/main/PORTLAND%20HOUSE.csv\", stringsAsFactors=TRUE)"
  },
  {
    "objectID": "posts/RegressionModel/RegressionModel.html#prepare-the-data",
    "href": "posts/RegressionModel/RegressionModel.html#prepare-the-data",
    "title": "House Prices in Portland, OR",
    "section": "Prepare the Data",
    "text": "Prepare the Data\nThis data has 25731 obs. of 32 variables\n\n\nShow the code\n## raw_pdx <- raw_pdx%>%select(-id)\n\nhead(raw_pdx)\n\n\n  id yearBuilt     City latitude longitude zipcode bathrooms bedrooms\n1  1      2007 Fairview 45.54357 -122.4418   97024         3        3\n2  2      2001 Fairview 45.54758 -122.4532   97024         3        3\n3  3      1982  Gresham 45.48823 -122.4444   97080         3        4\n4  4      1953 Portland 45.52663 -122.4641   97230         1        3\n5  5      1967  Gresham 45.51124 -122.4315   97030         3        6\n6  6      1967  Gresham 45.48799 -122.4162   97080         2        3\n  DateListed  DateSold daysOnZillow      homeType lastSoldPrice livingArea\n1  4/26/2021 5/21/2021           25     TOWNHOUSE        315400       1806\n2   3/1/2021 4/23/2021           53 SINGLE_FAMILY        400000       1518\n3  5/24/2021  6/4/2021           11 SINGLE_FAMILY        512000       2724\n4  5/24/2021  6/4/2021           11 SINGLE_FAMILY        348000       1217\n5  5/18/2021  6/1/2021           14     APARTMENT        510000       2400\n6  5/18/2021  6/1/2021           14 SINGLE_FAMILY        404200       1150\n  lotSize  price priceHistory.1.price propertyTaxRate hasCooling hasFireplace\n1    1555 315400               212000            1.12      FALSE         TRUE\n2    3484 400000               375000            1.12       TRUE         TRUE\n3    9583 512000               479000            1.12       TRUE         TRUE\n4   13939 348000               339500            1.12         NA         TRUE\n5    8545 510000               252450            1.12       TRUE           NA\n6    7000 404200               204500            1.12      FALSE         TRUE\n  hasGarage hasHeating hasView ElementarySchooldistance ElementarySchools\n1     FALSE       TRUE   FALSE                      0.4        Elementary\n2     FALSE       TRUE    TRUE                      1.2        Elementary\n3     FALSE       TRUE    TRUE                      0.8        Elementary\n4     FALSE       TRUE   FALSE                      0.8        Elementary\n5     FALSE       TRUE   FALSE                      0.3        Elementary\n6     FALSE       TRUE   FALSE                      0.4        Elementary\n  ElementarySchoolrating MiddleSchooldistance schoolsMiddlelevel\n1                      5                  1.1             Middle\n2                      5                  1.0             Middle\n3                      5                  1.7             Middle\n4                      2                  0.7             Middle\n5                      2                  0.9             Middle\n6                      2                  0.4             Middle\n  MiddleSchoolsrating HighSchooldistance HighSchoollevel HighSchoolRating\n1                   2                2.6            High                3\n2                   2                3.4            High                3\n3                   6                1.4            High                3\n4                   2                3.8            High                3\n5                   6                0.3            High                3\n6                   6                1.4            High                3\n\n\n\n\nShow the code\n# convert variables\n\nraw_pdx <-  raw_pdx %>% \n  \n  mutate(\n    \n    yearBuilt = as.numeric(yearBuilt),\n    bathrooms = as.numeric(bathrooms),\n    bedrooms = as.numeric(bedrooms),\n    daysOnZillow = as.numeric(daysOnZillow),\n    lastSoldPrice = as.numeric(lastSoldPrice ),\n    livingArea = as.numeric(livingArea),\n    lotSize= as.numeric(lotSize),\n    price = as.numeric(price),\n    priceHistory.1.price= as.numeric(priceHistory.1.price),\n    \n    \n    ElementarySchoolrating = as.factor(ElementarySchoolrating),\n    MiddleSchoolsrating = as.factor(MiddleSchoolsrating),\n    HighSchoolRating= as.factor(HighSchoolRating),\n    zipcode = as.factor(zipcode)\n\n    \n  )\n\n\n\nMissing data\n\n\nShow the code\nis.na(raw_pdx) %>% colSums()\n\n\n                      id                yearBuilt                     City \n                       0                      546                        0 \n                latitude                longitude                  zipcode \n                      13                       13                        0 \n               bathrooms                 bedrooms               DateListed \n                     484                      770                        0 \n                DateSold             daysOnZillow                 homeType \n                       0                        6                        0 \n           lastSoldPrice               livingArea                  lotSize \n                       0                      465                     2890 \n                   price     priceHistory.1.price          propertyTaxRate \n                       0                     1419                       11 \n              hasCooling             hasFireplace                hasGarage \n                    4496                     4624                        0 \n              hasHeating                  hasView ElementarySchooldistance \n                       1                        0                       25 \n       ElementarySchools   ElementarySchoolrating     MiddleSchooldistance \n                       0                       25                       36 \n      schoolsMiddlelevel      MiddleSchoolsrating       HighSchooldistance \n                       0                       36                     2242 \n         HighSchoollevel         HighSchoolRating \n                       0                     2242 \n\n\n\n\nShow the code\nclean_data <- raw_pdx %>%\n  filter(!is.na(yearBuilt))%>%\n  filter(!is.na(longitude))%>%\n  filter(!is.na(bedrooms))%>%\n  filter(!is.na(daysOnZillow))%>%\n  filter(!is.na(livingArea))%>%\n  filter(!is.na(priceHistory.1.price))%>%\n  filter(!is.na(hasFireplace))%>%\n  filter(!is.na(latitude))%>%\n  filter(!is.na(hasHeating))%>%\n  filter(!is.na(hasCooling))%>%\n  filter(!is.na(bathrooms))%>%\n  filter(!is.na(lotSize))%>%\n  filter(!is.na(propertyTaxRate))%>%\n  filter(!is.na(ElementarySchooldistance))%>%\n  filter(!is.na(MiddleSchooldistance))%>%\n  filter(!is.na(HighSchooldistance))%>%\n  filter(!is.na(ElementarySchoolrating))%>%\n  filter(!is.na(MiddleSchoolsrating))%>%\n  filter(!is.na(HighSchoolRating))\n\n\n\n\nShow the code\nsummary(clean_data)\n\n\n       id          yearBuilt             City         latitude    \n Min.   :    1   Min.   :   0   Portland   :5232   Min.   :45.26  \n 1st Qu.: 6207   1st Qu.:1965   Beaverton  :1456   1st Qu.:45.42  \n Median :15156   Median :1989   Hillsboro  :1194   Median :45.47  \n Mean   :13622   Mean   :1981   Lake Oswego: 942   Mean   :45.47  \n 3rd Qu.:20440   3rd Qu.:2003   Tigard     : 913   3rd Qu.:45.52  \n Max.   :25730   Max.   :2021   Gresham    : 845   Max.   :45.62  \n                                (Other)    :3906                  \n   longitude         zipcode        bathrooms        bedrooms     \n Min.   :-123.1   97229  :  834   Min.   : 0.00   Min.   : 0.000  \n 1st Qu.:-122.8   97045  :  713   1st Qu.: 2.00   1st Qu.: 3.000  \n Median :-122.7   97007  :  706   Median : 3.00   Median : 3.000  \n Mean   :-122.7   97086  :  632   Mean   : 2.78   Mean   : 3.568  \n 3rd Qu.:-122.6   97123  :  599   3rd Qu.: 3.00   3rd Qu.: 4.000  \n Max.   :-122.3   97068  :  573   Max.   :10.00   Max.   :10.000  \n                  (Other):10431                                   \n     DateListed          DateSold      daysOnZillow              homeType    \n 8/9/2019 :   71   5/28/2021 :  159   Min.   :  1   APARTMENT        :   38  \n 6/4/2021 :   66   6/30/2021 :  142   1st Qu.: 80   CONDO            :  136  \n 3/30/2021:   65   10/30/2020:  137   Median :192   HOME_TYPE_UNKNOWN:    2  \n 8/6/2019 :   64   7/31/2020 :  128   Mean   :187   SINGLE_FAMILY    :13659  \n 8/19/2019:   62   4/30/2021 :  127   3rd Qu.:294   TOWNHOUSE        :  653  \n 8/23/2019:   60   9/30/2020 :  122   Max.   :422                            \n (Other)  :14100   (Other)   :13673                                          \n lastSoldPrice       livingArea       lotSize             price        \n Min.   :    443   Min.   :  416   Min.   :       0   Min.   :    500  \n 1st Qu.: 450000   1st Qu.: 1664   1st Qu.:    4791   1st Qu.: 450000  \n Median : 551000   Median : 2206   Median :    7405   Median : 551000  \n Mean   : 634458   Mean   : 2399   Mean   :   17065   Mean   : 634827  \n 3rd Qu.: 710000   3rd Qu.: 2892   3rd Qu.:   10018   3rd Qu.: 710000  \n Max.   :6300000   Max.   :14014   Max.   :18992160   Max.   :6300000  \n                                                                       \n priceHistory.1.price propertyTaxRate hasCooling      hasFireplace   \n Min.   :    895      Min.   :1.010   Mode :logical   Mode :logical  \n 1st Qu.: 415000      1st Qu.:1.080   FALSE:807       FALSE:645      \n Median : 525000      Median :1.120   TRUE :13681     TRUE :13843    \n Mean   : 594080      Mean   :1.113                                  \n 3rd Qu.: 679992      3rd Qu.:1.130                                  \n Max.   :6888000      Max.   :1.130                                  \n                                                                     \n hasGarage       hasHeating       hasView        ElementarySchooldistance\n Mode :logical   Mode :logical   Mode :logical   Min.   :0.0000          \n FALSE:14150     FALSE:24        FALSE:8574      1st Qu.:0.4000          \n TRUE :338       TRUE :14464     TRUE :5914      Median :0.6000          \n                                                 Mean   :0.8057          \n                                                 3rd Qu.:1.0000          \n                                                 Max.   :9.4000          \n                                                                         \n  ElementarySchools ElementarySchoolrating MiddleSchooldistance\n           :    0   7      :2862           Min.   : 0.000      \n Elementary:13077   5      :2359           1st Qu.: 0.800      \n Primary   : 1411   6      :2323           Median : 1.300      \n                    8      :1606           Mean   : 1.546      \n                    3      :1368           3rd Qu.: 2.100      \n                    4      :1332           Max.   :11.900      \n                    (Other):2638                               \n  schoolsMiddlelevel MiddleSchoolsrating HighSchooldistance HighSchoollevel\n           :    0    5      :2347        Min.   : 0.100         :    0     \n Elementary:    3    8      :2262        1st Qu.: 1.000     High:14488     \n High      :    0    3      :2238        Median : 1.700                    \n Middle    :14485    6      :2104        Mean   : 1.911                    \n                     7      :1655        3rd Qu.: 2.500                    \n                     4      :1595        Max.   :10.800                    \n                     (Other):2287                                          \n HighSchoolRating\n 5      :3804    \n 8      :2365    \n 3      :1962    \n 6      :1787    \n 9      :1746    \n 4      :1230    \n (Other):1594    \n\n\n\n\nShow the code\nclean_data <- \n  clean_data %>% \n  mutate(price_category = case_when( \n    price < 551000 ~ \"below\",\n    price >= 551000 ~ \"above\")) %>% \n  mutate(price_category = as.factor(price_category))"
  },
  {
    "objectID": "posts/RegressionModel/RegressionModel.html#take-a-look-at-the-data",
    "href": "posts/RegressionModel/RegressionModel.html#take-a-look-at-the-data",
    "title": "House Prices in Portland, OR",
    "section": "Take a look at the Data",
    "text": "Take a look at the Data\n\n\nShow the code\nclean_data %>% \n  count(price_category, \n        name =\"total\") %>%\n  mutate(percent = total/sum(total)*100,\n         percent = round(percent, 2)) %>%\n gt() %>%\n  tab_header(\n    title = \"Portland, OR and its Metropolitan Area Median House Prices\",\n    subtitle = \"Above and below 551,000$\"\n  ) %>%\n  cols_label(\n    price_category = \"Price\",\n    total = \"Total\",\n    percent = \"Percent\"\n  ) %>% \n  fmt_number(\n    columns = vars(total),\n    suffixing = TRUE\n  )  \n\n\nWarning: Since gt v0.3.0, `columns = vars(...)` has been deprecated.\n• Please use `columns = c(...)` instead.\nSince gt v0.3.0, `columns = vars(...)` has been deprecated.\n• Please use `columns = c(...)` instead.\n\n\n\n\n\n\n  \n    \n      Portland, OR and its Metropolitan Area Median House Prices\n    \n    \n      Above and below 551,000$\n    \n  \n  \n    \n      Price\n      Total\n      Percent\n    \n  \n  \n    above\n7.25K\n50.02\n    below\n7.24K\n49.98\n  \n  \n  \n\n\n\n\n\n\nShow the code\nqmplot(x = longitude, \n       y = latitude, \n       data = clean_data, \n       geom = \"point\", \n       color = price_category, \n       alpha = 0.4) +\n  scale_alpha(guide = 'none')\n\n\nℹ Using `zoom = 10`\n\n\nℹ Map tiles by Stamen Design, under CC BY 3.0. Data by OpenStreetMap, under ODbL.\n\n\n\n\n\n\n\nShow the code\nhouses_pdx <-\n  clean_data %>% \n  select( # select our predictors\n    longitude, \n    latitude, \n    price_category,\n    bathrooms, \n    yearBuilt, \n    homeType,\n    bedrooms, \n    livingArea, \n    lotSize,\n    MiddleSchooldistance,\n    ElementarySchooldistance,\n    HighSchooldistance)\n\nglimpse(houses_pdx)\n\n\nRows: 14,488\nColumns: 12\n$ longitude                <dbl> -122.4418, -122.4532, -122.4444, -122.4162, -…\n$ latitude                 <dbl> 45.54357, 45.54758, 45.48823, 45.48799, 45.49…\n$ price_category           <fct> below, below, below, below, below, below, bel…\n$ bathrooms                <dbl> 3.0, 3.0, 3.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, …\n$ yearBuilt                <dbl> 2007, 2001, 1982, 1967, 1978, 2018, 2006, 201…\n$ homeType                 <fct> TOWNHOUSE, SINGLE_FAMILY, SINGLE_FAMILY, SING…\n$ bedrooms                 <dbl> 3, 3, 4, 3, 3, 4, 3, 3, 4, 4, 3, 2, 4, 3, 4, …\n$ livingArea               <dbl> 1806, 1518, 2724, 1150, 2036, 1947, 1548, 220…\n$ lotSize                  <dbl> 1555, 3484, 9583, 7000, 6969, 4791, 5009, 522…\n$ MiddleSchooldistance     <dbl> 1.1, 1.0, 1.7, 0.4, 2.1, 2.5, 0.5, 1.8, 0.3, …\n$ ElementarySchooldistance <dbl> 0.4, 1.2, 0.8, 0.4, 1.0, 0.3, 0.5, 1.0, 0.1, …\n$ HighSchooldistance       <dbl> 2.6, 3.4, 1.4, 1.4, 1.4, 2.2, 1.5, 1.4, 0.9, …\n\n\n\n\nShow the code\npdx_long <- houses_pdx %>% \n  select(-longitude,-latitude, -homeType, -yearBuilt, -lotSize)%>%\n    pivot_longer(!price_category, names_to = \"features\", values_to = \"values\")\n\n\n# Print the first 10 rows\npdx_long %>% \n  slice_head(n = 10)\n\n\n# A tibble: 10 × 3\n   price_category features                 values\n   <fct>          <chr>                     <dbl>\n 1 below          bathrooms                   3  \n 2 below          bedrooms                    3  \n 3 below          livingArea               1806  \n 4 below          MiddleSchooldistance        1.1\n 5 below          ElementarySchooldistance    0.4\n 6 below          HighSchooldistance          2.6\n 7 below          bathrooms                   3  \n 8 below          bedrooms                    3  \n 9 below          livingArea               1518  \n10 below          MiddleSchooldistance        1  \n\n\n\n\nShow the code\ntheme_set(theme_light())\n\n# Make a box plot for each predictor feature\npdx_long %>% \n  ggplot(mapping = aes(x = price_category, y = values, fill = features)) +\n  geom_boxplot() + \n  facet_wrap(~ features, scales = \"free\", ncol = 4) +\n  scale_color_viridis_d(option = \"plasma\", end = .7) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nData Splitting\n\n\nShow the code\n# Fix the random numbers by setting the seed \n# This enables the analysis to be reproducible \nset.seed(504)\n\n# Put 3/4 of the data into the training set \ndata_split <- initial_split(houses_pdx, \n                           prop = 3/4)\n\n# Create dataframes for the two sets:\ntrain_data <- training(data_split) \ntest_data <- testing(data_split)\n\n\n\n\nValidaton Set\n\n\nShow the code\nhouse_folds <-\n vfold_cv(train_data, \n          v = 5, \n          strata = price_category) \n\n\n\n\nShow the code\npdx_rec <-\n  recipe(price_category ~ .,\n         data = train_data) %>%\n  update_role(longitude, latitude, \n              new_role = \"ID\") %>% \n  \n  step_naomit(everything(), skip = TRUE) %>% \n  \n  step_novel(all_nominal(), -all_outcomes()) %>% # converts all nominal variables to factors and takes care of other issues related to categorical variables.\n  \n  step_normalize(all_numeric(), -all_outcomes(), \n                 -longitude, -latitude) %>% # step_normalize() normalizes (center and scales) the numeric variables to have a standard deviation of one and a mean of zero\n  \n  step_dummy(all_nominal(), -all_outcomes()) %>% #converts our factor columns into numeric binary (0 and 1) variables.\n  \n  step_zv(all_numeric(), -all_outcomes()) %>% ## step_zv(): removes any numeric variables that have zero variance.\n  \n  step_corr(all_predictors(), threshold = 0.7, method = \"spearman\") # step_corr(): will remove predictor variables that have large correlations with other predictor variables.\n\n\n\n\nShow the code\nsummary(pdx_rec)\n\n\n# A tibble: 12 × 4\n   variable                 type      role      source  \n   <chr>                    <list>    <chr>     <chr>   \n 1 longitude                <chr [2]> ID        original\n 2 latitude                 <chr [2]> ID        original\n 3 bathrooms                <chr [2]> predictor original\n 4 yearBuilt                <chr [2]> predictor original\n 5 homeType                 <chr [3]> predictor original\n 6 bedrooms                 <chr [2]> predictor original\n 7 livingArea               <chr [2]> predictor original\n 8 lotSize                  <chr [2]> predictor original\n 9 MiddleSchooldistance     <chr [2]> predictor original\n10 ElementarySchooldistance <chr [2]> predictor original\n11 HighSchooldistance       <chr [2]> predictor original\n12 price_category           <chr [3]> outcome   original\n\n\n\n\nShow the code\nprep_data <- \n  pdx_rec %>% # use the recipe object\n  prep() %>% # perform the recipe on training data\n  juice() # extract only the preprocessed dataframe"
  },
  {
    "objectID": "posts/RegressionModel/RegressionModel.html#the-model--logistic-regression",
    "href": "posts/RegressionModel/RegressionModel.html#the-model--logistic-regression",
    "title": "House Prices in Portland, OR",
    "section": "The Model- Logistic regression",
    "text": "The Model- Logistic regression\n\n\nShow the code\nlog_spec <- # your model specification\n  logistic_reg() %>%  # model type\n  set_engine(engine = \"glm\") %>%  # model engine\n  set_mode(\"classification\") # model mode\n\n# Show your model specification\nlog_spec\n\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\nShow the code\npdx_wflow <- # new workflow object\n workflow() %>% # use workflow function\n add_recipe(pdx_rec) %>%   # use the new recipe\n add_model(log_spec)   # add your model spec\n\npdx_wflow\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_naomit()\n• step_novel()\n• step_normalize()\n• step_dummy()\n• step_zv()\n• step_corr()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\nShow the code\n# save model coefficients for a fitted model object from a workflow\n\nget_model <- function(x) {\n  pull_workflow_fit(x) %>% tidy()\n}\n\n# same as before with one exception\nlog_res_2 <- \n  pdx_wflow %>% \n  fit_resamples(\n    resamples = house_folds, \n    metrics = metric_set(\n      recall, precision, f_meas, \n      accuracy, kap,\n      roc_auc, sens, spec),\n      control = control_resamples(\n      save_pred = TRUE,\n      extract = get_model) # use extract and our new function\n    ) \n\n\n## All of the results can be flattened and collected using:\n\n\n\nall_coef <- map_dfr(log_res_2$.extracts, ~ .x[[1]][[1]])\nfilter(all_coef, term == \"bedrooms\")\n\n\n# A tibble: 4 × 5\n  term     estimate std.error statistic  p.value\n  <chr>       <dbl>     <dbl>     <dbl>    <dbl>\n1 bedrooms    0.129    0.0433      2.99 0.00284 \n2 bedrooms    0.116    0.0423      2.74 0.00623 \n3 bedrooms    0.118    0.0425      2.78 0.00550 \n4 bedrooms    0.147    0.0430      3.42 0.000618\n\n\n\nPerformance metrics\nShow performance for every single fold:\n\n\nShow the code\nlog_res_2 %>%  collect_metrics(summarize = FALSE)\n\n\n# A tibble: 32 × 5\n   id    .metric   .estimator .estimate .config             \n   <chr> <chr>     <chr>          <dbl> <chr>               \n 1 Fold2 recall    binary         0.783 Preprocessor1_Model1\n 2 Fold2 precision binary         0.824 Preprocessor1_Model1\n 3 Fold2 f_meas    binary         0.803 Preprocessor1_Model1\n 4 Fold2 accuracy  binary         0.808 Preprocessor1_Model1\n 5 Fold2 kap       binary         0.615 Preprocessor1_Model1\n 6 Fold2 sens      binary         0.783 Preprocessor1_Model1\n 7 Fold2 spec      binary         0.832 Preprocessor1_Model1\n 8 Fold2 roc_auc   binary         0.887 Preprocessor1_Model1\n 9 Fold3 recall    binary         0.806 Preprocessor1_Model1\n10 Fold3 precision binary         0.843 Preprocessor1_Model1\n# … with 22 more rows\n\n\n\n\nCollect predictions\nTo obtain the actual model predictions, we use the function collect_predictions and save the result as log_pred:\n\n\nShow the code\nlog_pred <- \n  log_res_2 %>%\n  collect_predictions()\n\n\nlog_pred %>% \n  conf_mat(price_category, .pred_class) \n\n\n          Truth\nPrediction above below\n     above  3487   686\n     below   862  3657\n\n\n\n\nShow the code\nlog_pred %>% \n  conf_mat(price_category, .pred_class) %>% \n  autoplot(type = \"heatmap\")+\n  theme_minimal()\n\n\n\n\n\n\n\nROC Curve\n\n\nShow the code\nlog_pred %>% \n  group_by(id) %>% # id contains our folds\n  roc_curve(price_category, .pred_above) %>% \n  autoplot()+\n  theme_minimal()"
  },
  {
    "objectID": "posts/RegressionModel/RegressionModel.html#use-the-workflow-to-train-our-model",
    "href": "posts/RegressionModel/RegressionModel.html#use-the-workflow-to-train-our-model",
    "title": "House Prices in Portland, OR",
    "section": "Use the workflow to train our model",
    "text": "Use the workflow to train our model\n\n\nShow the code\npdx_fit <- fit(pdx_wflow, train_data)\n\n\nThis allows us to use the model trained by this workflow to predict labels for our test set, and compare the performance metrics with the basic model we created previously.\n\n\nShow the code\npdx_fit %>% ## display results\npull_workflow_fit() %>%\ntidy()%>%\n  filter(p.value < 0.05)\n\n\n# A tibble: 8 × 5\n  term                 estimate std.error statistic  p.value\n  <chr>                   <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)           -0.426     0.0296    -14.4  5.43e-47\n2 yearBuilt              0.221     0.0482      4.59 4.49e- 6\n3 bedrooms               0.119     0.0381      3.12 1.82e- 3\n4 livingArea            -2.88      0.0689    -41.8  0       \n5 lotSize               -0.0448    0.0202     -2.21 2.68e- 2\n6 MiddleSchooldistance  -0.171     0.0310     -5.52 3.33e- 8\n7 HighSchooldistance     0.0805    0.0303      2.66 7.86e- 3\n8 homeType_TOWNHOUSE     0.998     0.179       5.59 2.24e- 8\n\n\n\n\nShow the code\nlibrary(vip)\n\npdx_fit %>%\nextract_fit_parsnip() %>%\n   vip(num_features = 5)+\n  theme_minimal()\n\n\n\n\n\nThe two most important predictors in whether the median house value is above or below 551,000 dollars were the Living Area and the home type: Townhouse\n\n\nShow the code\n# Make predictions on the test set\npred_results <- test_data %>% \n  select(price_category) %>% \n  bind_cols(pdx_fit %>% \n              predict(new_data = test_data)) %>% \n  bind_cols(pdx_fit %>% \n              predict(new_data = test_data, type = \"prob\"))\n\n# Print the results\npred_results %>% \n  slice_head(n = 10)\n\n\n   price_category .pred_class .pred_above .pred_below\n1           below       above  0.79554296   0.2044570\n2           below       below  0.26469350   0.7353065\n3           below       below  0.47600771   0.5239923\n4           below       below  0.02353063   0.9764694\n5           below       below  0.06272184   0.9372782\n6           below       below  0.10491967   0.8950803\n7           below       below  0.06923625   0.9307637\n8           below       below  0.03314398   0.9668560\n9           below       below  0.04580355   0.9541965\n10          below       above  0.83998089   0.1600191\n\n\nLet’s take a look at the confusion matrix:\n\n\nShow the code\npred_results%>% \n  conf_mat(price_category, .pred_class) %>% \n  autoplot(type = \"heatmap\")+\n  theme_minimal()"
  },
  {
    "objectID": "posts/RegressionModel/RegressionModel.html#reference",
    "href": "posts/RegressionModel/RegressionModel.html#reference",
    "title": "House Prices in Portland, OR",
    "section": "Reference",
    "text": "Reference\nTidymodels- https://www.tidymodels.org/"
  },
  {
    "objectID": "posts/TimeSeries/Time Series Project.html",
    "href": "posts/TimeSeries/Time Series Project.html",
    "title": "Time Series",
    "section": "",
    "text": "In this project, I will perform Time series analysis using the Zillow Home Value Index (ZHVI) dataset: A smoothed, seasonally adjusted measure of the typical home value and market changes across Portland, OR, four bedroom houses. It reflects the typical value for homes in the 35th to 65th percentile range.\nHere is the link: https://www.zillow.com/research/data/"
  },
  {
    "objectID": "posts/TimeSeries/Time Series Project.html#the-data",
    "href": "posts/TimeSeries/Time Series Project.html#the-data",
    "title": "Time Series",
    "section": "The Data",
    "text": "The Data\n\n\nShow the code\nmetrofour <- read.csv(\"https://raw.githubusercontent.com/karolo89/Raw_Data/main/City_zhvi_bdrmcnt_4_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")\n\nstr(metrofour[,c(1:11)])\n\n\n'data.frame':   13528 obs. of  11 variables:\n $ RegionID   : int  6181 12447 39051 17426 6915 40326 13271 18959 54296 38128 ...\n $ SizeRank   : int  0 1 2 3 4 5 6 7 8 9 ...\n $ RegionName : chr  \"New York\" \"Los Angeles\" \"Houston\" \"Chicago\" ...\n $ RegionType : chr  \"city\" \"city\" \"city\" \"city\" ...\n $ StateName  : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ State      : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ Metro      : chr  \"New York-Newark-Jersey City, NY-NJ-PA\" \"Los Angeles-Long Beach-Anaheim, CA\" \"Houston-The Woodlands-Sugar Land, TX\" \"Chicago-Naperville-Elgin, IL-IN-WI\" ...\n $ CountyName : chr  \"Queens County\" \"Los Angeles County\" \"Harris County\" \"Cook County\" ...\n $ X2000.01.31: num  284723 302701 146377 179997 154391 ...\n $ X2000.02.29: num  286885 303000 146270 180472 154691 ...\n $ X2000.03.31: num  288711 304356 145921 181368 154943 ...\n\n\nWe have to make this dataset tidy. Tidy Data is a way of structuring data so that it can be easily understood by people and analyzed by machines.\nI need to remove the X at the beginning of the dates (X2000.01.31,X2000.02.29,…)\n\n\nShow the code\nnames(metrofour) <- sub(\"^X\", \"\", names(metrofour))\n\nstr(metrofour[,c(1:11)])\n\n\n'data.frame':   13528 obs. of  11 variables:\n $ RegionID  : int  6181 12447 39051 17426 6915 40326 13271 18959 54296 38128 ...\n $ SizeRank  : int  0 1 2 3 4 5 6 7 8 9 ...\n $ RegionName: chr  \"New York\" \"Los Angeles\" \"Houston\" \"Chicago\" ...\n $ RegionType: chr  \"city\" \"city\" \"city\" \"city\" ...\n $ StateName : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ State     : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ Metro     : chr  \"New York-Newark-Jersey City, NY-NJ-PA\" \"Los Angeles-Long Beach-Anaheim, CA\" \"Houston-The Woodlands-Sugar Land, TX\" \"Chicago-Naperville-Elgin, IL-IN-WI\" ...\n $ CountyName: chr  \"Queens County\" \"Los Angeles County\" \"Harris County\" \"Cook County\" ...\n $ 2000.01.31: num  284723 302701 146377 179997 154391 ...\n $ 2000.02.29: num  286885 303000 146270 180472 154691 ...\n $ 2000.03.31: num  288711 304356 145921 181368 154943 ...\n\n\n\n\nShow the code\nhouse_price <- metrofour %>% \n  pivot_longer(-c(RegionID, SizeRank, RegionName, RegionType, StateName, State, Metro, CountyName),\n    names_to = \"Monthly\",\n    values_to = \"Price\"\n  ) \nstr(metrofour[,c(1:11)])\n\n\n'data.frame':   13528 obs. of  11 variables:\n $ RegionID  : int  6181 12447 39051 17426 6915 40326 13271 18959 54296 38128 ...\n $ SizeRank  : int  0 1 2 3 4 5 6 7 8 9 ...\n $ RegionName: chr  \"New York\" \"Los Angeles\" \"Houston\" \"Chicago\" ...\n $ RegionType: chr  \"city\" \"city\" \"city\" \"city\" ...\n $ StateName : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ State     : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ Metro     : chr  \"New York-Newark-Jersey City, NY-NJ-PA\" \"Los Angeles-Long Beach-Anaheim, CA\" \"Houston-The Woodlands-Sugar Land, TX\" \"Chicago-Naperville-Elgin, IL-IN-WI\" ...\n $ CountyName: chr  \"Queens County\" \"Los Angeles County\" \"Harris County\" \"Cook County\" ...\n $ 2000.01.31: num  284723 302701 146377 179997 154391 ...\n $ 2000.02.29: num  286885 303000 146270 180472 154691 ...\n $ 2000.03.31: num  288711 304356 145921 181368 154943 ...\n\n\n\n\nShow the code\n#Converting the Date from factor to character\n\nhouse_clean <- house_price %>%\n            mutate(Monthly_parsed = as.Date(Monthly,\"%Y.%m.%d\"))\n\n\nhouse_clean[[\"Monthly\"]]<- as.character(house_clean$Monthly)\n\nhouse_price[[\"Monthly\"]]<- as.character(house_price $Monthly)\nsummary(house_clean)\n\n\n    RegionID         SizeRank      RegionName         RegionType       \n Min.   :  3300   Min.   :    0   Length:3720200     Length:3720200    \n 1st Qu.: 17381   1st Qu.: 3511   Class :character   Class :character  \n Median : 31963   Median : 7196   Mode  :character   Mode  :character  \n Mean   : 51628   Mean   : 8235                                        \n 3rd Qu.: 46317   3rd Qu.:11713                                        \n Max.   :827230   Max.   :28439                                        \n                                                                       \n  StateName            State              Metro            CountyName       \n Length:3720200     Length:3720200     Length:3720200     Length:3720200    \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   Monthly              Price         Monthly_parsed      \n Length:3720200     Min.   :  17773   Min.   :2000-01-31  \n Class :character   1st Qu.: 172701   1st Qu.:2005-09-30  \n Mode  :character   Median : 244810   Median :2011-06-30  \n                    Mean   : 321841   Mean   :2011-07-01  \n                    3rd Qu.: 369075   3rd Qu.:2017-03-31  \n                    Max.   :8337561   Max.   :2022-11-30  \n                    NA's   :1201126                       \n\n\nWe see some missing values in the Price variable, but before I deal with those values, I will filter my data to the cities that I am interested the most\n\n\nShow the code\npdx_data <- house_clean %>%\n  dplyr:::filter(RegionID== 13373)  %>%\n  dplyr:::filter(Monthly_parsed >= \"2014-01-01\")\n\nsummary(pdx_data)\n\n\n    RegionID        SizeRank   RegionName         RegionType       \n Min.   :13373   Min.   :22   Length:107         Length:107        \n 1st Qu.:13373   1st Qu.:22   Class :character   Class :character  \n Median :13373   Median :22   Mode  :character   Mode  :character  \n Mean   :13373   Mean   :22                                        \n 3rd Qu.:13373   3rd Qu.:22                                        \n Max.   :13373   Max.   :22                                        \n  StateName            State              Metro            CountyName       \n Length:107         Length:107         Length:107         Length:107        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   Monthly              Price        Monthly_parsed      \n Length:107         Min.   :410582   Min.   :2014-01-31  \n Class :character   1st Qu.:506597   1st Qu.:2016-04-15  \n Mode  :character   Median :564473   Median :2018-06-30  \n                    Mean   :563372   Mean   :2018-06-30  \n                    3rd Qu.:588606   3rd Qu.:2020-09-15  \n                    Max.   :758797   Max.   :2022-11-30  \n\n\nAfter filtering the data, we don’t have any missing values\n\nCoerce to a tsibble with as_tsibble()\nA time series can be recorded as a tsibble object in R. tsibble objects extend tidy data frames (tibble objects) by introducing temporal structure, and to do it, we need to declare key and index. In this case, the Monthly_parsed containing the data-time is the index and the RegionID is the key. Other columns can be considered as measured variables.\n\n\nShow the code\ntsb_pdx <- pdx_data %>%\n                   select(RegionName,RegionID, Monthly_parsed, Price)\n\ntsb_pref_pdx <-tsb_pdx%>%\n  as_tsibble(key= RegionName, index= Monthly_parsed)%>%\n                   index_by(year_month = ~ yearmonth(.))\n\ntsibble_pdx <-tsb_pref_pdx%>%\n  select(-RegionID)%>%\n  as_tsibble(key= RegionName, index= year_month)%>%\n  mutate(Prices = Price/1000)"
  },
  {
    "objectID": "posts/TimeSeries/Time Series Project.html#data-visualization",
    "href": "posts/TimeSeries/Time Series Project.html#data-visualization",
    "title": "Time Series",
    "section": "Data Visualization",
    "text": "Data Visualization\nTo visualize the data, I could use the autoplot() command, but I rather to create my graph with ggplot.\n\n\nShow the code\nplot_pdx_house <- tsibble_pdx %>%\n  ggplot(aes(x= year_month, y= Prices)) +\n  geom_line(size=1, color= \"darkgreen\")+\n   \n    labs(y=\"Price in Thousands of Dollars \", \n       x= \" \",\n       title=\" Four Bedroom House Prices in Portland, OR, 2014-2022 \",\n       caption = \"data:https://www.zillow.com/research/data\")+\n  scale_y_continuous(labels=scales::dollar_format())+\n   theme_light()\n\n\nplot_pdx_house \n\n\n\n\n\nData is non- stationary, we can see a trend-cycle component in the graph above.\n\n\nShow the code\ntsibble_pdx %>%\ngg_subseries(Price/1000)+\n  labs(y= \"Price in Thousands of Dollars\",\n       x= \"Year\")+theme_minimal()+\n  scale_y_continuous(labels=scales::dollar_format())+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\nShow the code\ntsibble_pdx%>%\ngg_season(Price/1000, labels = \"both\")+\n  labs(x= \"\",\n       y= \"Price in Thousands of Dollars \", \n       title=\"Portland's Seasonal Plot\")+\n  \n  scale_y_continuous(labels=scales::dollar_format())+\n  theme_minimal()"
  },
  {
    "objectID": "posts/TimeSeries/Time Series Project.html#determining-stationarity",
    "href": "posts/TimeSeries/Time Series Project.html#determining-stationarity",
    "title": "Time Series",
    "section": "Determining Stationarity",
    "text": "Determining Stationarity\nIn our analysis, we use the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test (Kwiatkowski et al., 1992). In this test, the null hypothesis is that the data are stationary, and we look for evidence that the null hypothesis is false. Consequently, small p-values (e.g., less than 0.05) suggest that differencing is required. The test can be computed using the unitroot_kpss() function.\n\n\nShow the code\ntsibble_pdx%>%\n  features(Prices, unitroot_kpss)\n\n\n# A tibble: 1 × 3\n  RegionName kpss_stat kpss_pvalue\n  <chr>          <dbl>       <dbl>\n1 Portland        1.97        0.01\n\n\nThe p-value is reported as 0.01 if it is less than 0.01, and as 0.1 if it is greater than 0.1. In this case, the test statistic (1.946) is bigger than the 1% critical value, so the p-value is less than 0.01, indicating that the null hypothesis is rejected. That is, the data are not stationary.\n\n\nShow the code\ntsibble_pdx %>% \n  features(Prices ,unitroot_ndiffs)\n\n\n# A tibble: 1 × 2\n  RegionName ndiffs\n  <chr>       <int>\n1 Portland        1\n\n\nAs we saw from the KPSS tests above, one difference (d) is required to make the tsibble_pdx data stationary."
  },
  {
    "objectID": "posts/TimeSeries/Time Series Project.html#autocorrelation",
    "href": "posts/TimeSeries/Time Series Project.html#autocorrelation",
    "title": "Time Series",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\n\nShow the code\ntsibble_pdx %>%\n  gg_tsdisplay(Prices,\n                     plot_type='partial')+\n       labs(y=\"Thousands of Dollars \", \n       x= \" \")\n\n\n\n\n\nACF does not drop quickly to zero, moreover the value is large and positive (almost 1 in this case). All these are signs of a non-stationary time series. Therefore it should be differenced to obtain a stationary series.\nPACF value r1 is almost 1. All other values ri,i >1 are small. This is a sign of a non stationary process that should be differenced in order to obtain a stationary series.\nThe data are clearly non-stationary, so we will first take a seasonal difference. The seasonally differenced data are shown below:\n\n\nShow the code\ntsibble_pdx %>%\n  gg_tsdisplay(difference(Prices, 12),\n               plot_type='partial', lag=36) +\n  labs(title=\"Seasonally differenced\", y=\"\")\n\n\n\n\n\nOur aim now is to find an appropriate ARIMA model based on the ACF and PACF shown in the Double Differenced graph."
  },
  {
    "objectID": "posts/TimeSeries/Time Series Project.html#seasonal-arima-model",
    "href": "posts/TimeSeries/Time Series Project.html#seasonal-arima-model",
    "title": "Time Series",
    "section": "Seasonal Arima Model",
    "text": "Seasonal Arima Model\n\n\nShow the code\nall_fit <- tsibble_pdx%>%\n  model(\n    arima212012 = ARIMA(Prices ~ pdq(2,1,2)+ PDQ(0,1,2)),\n    arima210011 = ARIMA(Prices ~ pdq(2,1,0)+ PDQ(0,1,1)),\n    stepwise = ARIMA(Prices),\n    search = ARIMA(Prices,stepwise=FALSE))\n\n\n\n\nShow the code\nall_fit %>% pivot_longer(!RegionName,\n            names_to = \"Model name\", \n            values_to = \"Orders\")\n\n\n# A mable: 4 x 3\n# Key:     RegionName, Model name [4]\n  RegionName `Model name`                             Orders\n  <chr>      <chr>                                   <model>\n1 Portland   arima212012           <ARIMA(2,1,2)(0,1,2)[12]>\n2 Portland   arima210011           <ARIMA(2,1,0)(0,1,1)[12]>\n3 Portland   stepwise                <ARIMA(3,1,2) w/ drift>\n4 Portland   search       <ARIMA(2,1,3)(0,0,1)[12] w/ drift>\n\n\n\n\nShow the code\nglance(all_fit) %>% arrange(AICc) %>% select(.model:BIC)\n\n\n# A tibble: 4 × 6\n  .model      sigma2 log_lik   AIC  AICc   BIC\n  <chr>        <dbl>   <dbl> <dbl> <dbl> <dbl>\n1 arima212012   2.81   -190.  394.  395.  412.\n2 search        2.14   -191.  399.  400.  420.\n3 stepwise      2.23   -194.  402.  403.  420.\n4 arima210011   4.53   -207.  423.  423.  433.\n\n\nOf these models, the best is the ARIMA(2,1,2)(0,1,2)[12]model (i.e., it has the smallest AICc value).\n\n\nShow the code\narima212012 <- tsibble_pdx %>%\n  model(arima212012 = ARIMA(Prices ~ pdq(2,1,2)+ PDQ(0,1,2)))%>%\n  report()\n\n\nSeries: Prices \nModel: ARIMA(2,1,2)(0,1,2)[12] \n\nCoefficients:\n         ar1     ar2     ma1     ma2     sma1    sma2\n      0.5178  0.0341  1.0092  0.9999  -0.8285  0.1308\ns.e.  0.1130  0.1206  0.0573  0.0692   0.1461  0.1514\n\nsigma^2 estimated as 2.813:  log likelihood=-189.9\nAIC=393.8   AICc=395.1   BIC=411.6\n\n\n\n\nShow the code\nall_fit %>% select(arima212012) %>%\n  gg_tsresiduals()\n\n\n\n\n\n\n\nShow the code\nall_fit %>% select(\"search\") %>%\n  gg_tsresiduals()\n\n\n\n\n\n\n\nShow the code\naugment(all_fit) %>%\n  filter(.model=='arima212012') %>%\n  features(.innov, ljung_box, lag = 36, dof = 6)\n\n\n# A tibble: 1 × 4\n  RegionName .model      lb_stat lb_pvalue\n  <chr>      <chr>         <dbl>     <dbl>\n1 Portland   arima212012    36.7     0.186\n\n\n\n\nShow the code\ntsibble_pdx %>%\n  model(ARIMA(Prices ~ pdq(2,1,2) + PDQ(0,1,2))) %>%\n  forecast() %>%\n  autoplot(tsibble_pdx) +\n  labs(y=\" Thousands of $US \",\n       x =\" \",\n       title=\"Forecast from the ARIMA(2,1,2)(0,1,2)[12] model applied to the Portland House Prices data\")+\ntheme_minimal()\n\n\n\n\n\nShow the code\n##Price in Thousands of Dollars\ntsibble_pdx %>%\n  model(ARIMA(Prices ~ pdq(2,1,2) + PDQ(0,1,2))) %>%\n  forecast()\n\n\n# A fable: 24 x 5 [1M]\n# Key:     RegionName, .model [1]\n   RegionName .model                                  year_m…¹      Prices .mean\n   <chr>      <chr>                                      <mth>      <dist> <dbl>\n 1 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2022 Dec N(735, 2.9)  735.\n 2 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Jan  N(737, 21)  737.\n 3 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Feb  N(740, 75)  740.\n 4 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Mar N(745, 156)  745.\n 5 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Apr N(750, 255)  750.\n 6 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 May N(755, 365)  755.\n 7 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Jun N(758, 482)  758.\n 8 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Jul N(760, 604)  760.\n 9 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Aug N(762, 727)  762.\n10 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Sep N(764, 852)  764.\n# … with 14 more rows, and abbreviated variable name ¹​year_month"
  },
  {
    "objectID": "posts/TimeSeries/Time Series Project.html#ets",
    "href": "posts/TimeSeries/Time Series Project.html#ets",
    "title": "Time Series",
    "section": "ETS",
    "text": "ETS\n\n\nShow the code\nfit_ets <- tsibble_pdx %>%\n  model(ETS(Prices))\nreport(fit_ets)\n\n\nSeries: Prices \nModel: ETS(M,Ad,N) \n  Smoothing parameters:\n    alpha = 0.9997705 \n    beta  = 0.9997704 \n    phi   = 0.9088093 \n\n  Initial states:\n     l[0]    b[0]\n 406.6826 3.75712\n\n  sigma^2:  0\n\n     AIC     AICc      BIC \n659.0909 659.9309 675.1278 \n\n\nThe model selected is ETS(M,Ad,N)\n\n\nShow the code\ncomponents(fit_ets) %>%\n  autoplot() +\n  labs(title = \"ETS(M,Ad,N) components\")\n\n\n\n\n\nBecause this model has multiplicative errors, the innovation residuals are not equivalent to the regular residuals.\n\n\nShow the code\nfit_ets %>%\n    augment() %>%\n    select(.innov, .resid) %>%\n    pivot_longer(c(.innov, .resid)) %>%\n    autoplot()+\n   theme_fivethirtyeight()\n\n\n\n\n\n\n\nShow the code\nfit_ets%>%\n    gg_tsresiduals()\n\n\n\n\n\n\n\nShow the code\nfit_ets %>%\n  forecast(h = 24) %>%\n  autoplot(tsibble_pdx)+\n\n   theme_light()\n\n\n\n\n\n\n\nShow the code\nbind_rows(\n    arima212012 %>% accuracy(),\n    fit_ets %>% accuracy()) %>%\n  select(-ME, -MPE, -ACF1)\n\n\n# A tibble: 2 × 8\n  RegionName .model      .type     RMSE   MAE  MAPE   MASE  RMSSE\n  <chr>      <chr>       <chr>    <dbl> <dbl> <dbl>  <dbl>  <dbl>\n1 Portland   arima212012 Training  1.52  1.13 0.193 0.0284 0.0308\n2 Portland   ETS(Prices) Training  2.27  1.62 0.274 0.0408 0.0461\n\n\nIn this case the ARIMA model seems to be more accurate model based on the test set RMSE, MAPE and MASE."
  }
]