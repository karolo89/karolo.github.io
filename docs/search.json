[
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "Karol Orozco",
    "section": "",
    "text": "Background\nAs a highly motivated and results-oriented professional, I bring a unique blend of technical expertise, strategic thinking, and interpersonal skills to the table.\nI have always been fascinated by the power of data and its potential to inform business decisions and drive growth. As a result, I earned a master’s degree in data science and business administration and gained proficiency in programming languages like R, Tableau, and SQL. I have also completed projects that involved data cleaning, exploratory data analysis, and predictive modeling.\nI am committed to promoting diversity and inclusion in all aspects of my life and believe that a diverse and inclusive workplace leads to better results for employees, the company as a whole, and our community.\nMy experience has also taught me to collaborate with cross-functional teams to identify and translate business needs into data-driven solutions. I have an eye for detail, am an expert in data visualization tools and techniques, and have a proven track record of communicating complex technical concepts to non-technical stakeholders.\n\n\nEducation\nWillamette University | Portland, OR\nMaster in Data Science | August 2023\nPresidential Scholarship; selected based on professional achievements\n\nWillamette University | Portland, OR\nMaster of Business Administration | August 2022\nGeorge and Colleen Hoyt Not-For-Profit Scholarship; selected based on professional achievements\nAtkinson Honor\nBeta Gamma Sigma: The International Business Honor Society\n\nUniversidad Autonoma del Caribe | Barranquilla, Colombia\nBachelor’s Degree, Industrial Engineering | July 2014\n\n\n\nProfessional Skills\nData Analysis | Team Development | Partnership Management | Business Development | Diversity & Inclusion | Strategic Planning | Process Improvement | Project Management | People & Change\n\n\nDomain Knowledge\nR | Tableau | SQL | Community Affairs | Strategic Management | Leadership | Community Engagement | Latinx Community | Shared Value\n\n\nLanguages\n\nI have a deep understanding of the cultural nuances of both English and Spanish-speaking communities. I am able to navigate cultural differences with sensitivity and respect.\nI have basic verbal and written communication skills in Portuguese. I am able to introduce myself, ask and answer basic questions, and engage in simple conversations. I am also able to write simple sentences and paragraphs in Portuguese."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello! It’s nice to meet you",
    "section": "",
    "text": "About Me\nI help to create and sustain communities, programs, and solutions that enhance the use of responsible data science and business practices for equitable social good.\nMy name is Karol Orozco; I live in Portland, OR, with my husband, two kids, and my dog Lucy. I am originally from Barranquilla, Colombia, a city located in the northern region of Colombia on the Caribbean coast. It is the fourth-largest city in the country and an important economic and cultural center in the Caribbean region. Barranquilla is known for its vibrant culture and festive atmosphere, exemplified by its famous Carnival of Barranquilla.\n\n\nBehind the Scenes\nlibrary(leaflet)\nleaflet() %>%\n  addTiles() %>%  # Add default OpenStreetMap map tiles\n  addMarkers(lng=-74.77028, lat=10.98611, popup=\"The birthplace of R\")\n\n\n\n\n\n\nAs a graduated engineer, I have acquired extensive experience and problem–solving expertise in data wrangling, statistical analysis, and model development. Since effective communication of data is of paramount importance, I have subsequently developed strong skills in data visualization and business knowledge. I hold an MBA from Willamette University and currently pursuing a second master’s in Data Science.\nI have experience working in the public and private sectors helping organizations to embrace Shared Value by positively impacting society and the environment: The sweet spot where profit meets purpose."
  },
  {
    "objectID": "KarolOrozco.html#my-style",
    "href": "KarolOrozco.html#my-style",
    "title": "Measures that Matter",
    "section": "My style",
    "text": "My style\n𝑰 𝒆𝒏𝒔𝒖𝒓𝒆 𝒄𝒖𝒍𝒕𝒖𝒓𝒂𝒍𝒍𝒚 𝒓𝒆𝒍𝒆𝒗𝒂𝒏𝒕 𝒅𝒂𝒕𝒂 𝒂𝒏𝒂𝒍𝒚𝒔𝒊𝒔 𝒃𝒚:\n\n\nCraft intuitive data visualizations and clear narratives to help readers engage well with patterns in data\n\n\n\n\nTranslate data reports and business jargon into plain language in English and Spanish\n\n\n\n\nCreate bespoke solutions which automate data processes, and equipping others to use them\n\n\n\n\nLinkedIn: @karolorozcoe | GitHub: @karolo89"
  },
  {
    "objectID": "KarolOrozco.html#section-1",
    "href": "KarolOrozco.html#section-1",
    "title": "Measures that Matter",
    "section": "",
    "text": "Collect data by partnering with community-based, religious, or other organizations that work with members of the assessment’s target communities\n\n\n\nConduct key informant interviews with community leaders\n\n\n\n\nEngage community members as data collectors and meeting there where they are\n\n\n\n\nShare early results with community stakeholders to validate findings, obtain candid feedback, and adjust the interpretation of data\n\n\n\n\nLinkedIn: @karolorozcoe | GitHub: @karolo89"
  },
  {
    "objectID": "KarolOrozco.html#section-2",
    "href": "KarolOrozco.html#section-2",
    "title": "Measures that Matter",
    "section": "",
    "text": "If you like what you see, get in touch!\nContact\n\nLinkedIn: @karolorozcoe | GitHub: @karolo89"
  },
  {
    "objectID": "Models/TimeSeries/Time Series Project.html",
    "href": "Models/TimeSeries/Time Series Project.html",
    "title": "Time Series",
    "section": "",
    "text": "In this project, I will perform Time series analysis using the Zillow Home Value Index (ZHVI) dataset: A smoothed, seasonally adjusted measure of the typical home value and market changes across Portland, OR, four bedroom houses. It reflects the typical value for homes in the 35th to 65th percentile range.\nHere is the link: https://www.zillow.com/research/data/"
  },
  {
    "objectID": "Models/TimeSeries/Time Series Project.html#the-data",
    "href": "Models/TimeSeries/Time Series Project.html#the-data",
    "title": "Time Series",
    "section": "The Data",
    "text": "The Data\n\n\nBehind the Scenes\nmetrofour <- read.csv(\"https://raw.githubusercontent.com/karolo89/Raw_Data/main/City_zhvi_bdrmcnt_4_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")\n\nstr(metrofour[,c(1:11)])\n\n\n'data.frame':   13528 obs. of  11 variables:\n $ RegionID   : int  6181 12447 39051 17426 6915 40326 13271 18959 54296 38128 ...\n $ SizeRank   : int  0 1 2 3 4 5 6 7 8 9 ...\n $ RegionName : chr  \"New York\" \"Los Angeles\" \"Houston\" \"Chicago\" ...\n $ RegionType : chr  \"city\" \"city\" \"city\" \"city\" ...\n $ StateName  : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ State      : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ Metro      : chr  \"New York-Newark-Jersey City, NY-NJ-PA\" \"Los Angeles-Long Beach-Anaheim, CA\" \"Houston-The Woodlands-Sugar Land, TX\" \"Chicago-Naperville-Elgin, IL-IN-WI\" ...\n $ CountyName : chr  \"Queens County\" \"Los Angeles County\" \"Harris County\" \"Cook County\" ...\n $ X2000.01.31: num  284723 302701 146377 179997 154391 ...\n $ X2000.02.29: num  286885 303000 146270 180472 154691 ...\n $ X2000.03.31: num  288711 304356 145921 181368 154943 ...\n\n\nWe have to make this dataset tidy. Tidy Data is a way of structuring data so that it can be easily understood by people and analyzed by machines.\nI need to remove the X at the beginning of the dates (X2000.01.31,X2000.02.29,…)\n\n\nBehind the Scenes\nnames(metrofour) <- sub(\"^X\", \"\", names(metrofour))\n\nstr(metrofour[,c(1:11)])\n\n\n'data.frame':   13528 obs. of  11 variables:\n $ RegionID  : int  6181 12447 39051 17426 6915 40326 13271 18959 54296 38128 ...\n $ SizeRank  : int  0 1 2 3 4 5 6 7 8 9 ...\n $ RegionName: chr  \"New York\" \"Los Angeles\" \"Houston\" \"Chicago\" ...\n $ RegionType: chr  \"city\" \"city\" \"city\" \"city\" ...\n $ StateName : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ State     : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ Metro     : chr  \"New York-Newark-Jersey City, NY-NJ-PA\" \"Los Angeles-Long Beach-Anaheim, CA\" \"Houston-The Woodlands-Sugar Land, TX\" \"Chicago-Naperville-Elgin, IL-IN-WI\" ...\n $ CountyName: chr  \"Queens County\" \"Los Angeles County\" \"Harris County\" \"Cook County\" ...\n $ 2000.01.31: num  284723 302701 146377 179997 154391 ...\n $ 2000.02.29: num  286885 303000 146270 180472 154691 ...\n $ 2000.03.31: num  288711 304356 145921 181368 154943 ...\n\n\n\n\nBehind the Scenes\nhouse_price <- metrofour %>% \n  pivot_longer(-c(RegionID, SizeRank, RegionName, RegionType, StateName, State, Metro, CountyName),\n    names_to = \"Monthly\",\n    values_to = \"Price\"\n  ) \nstr(metrofour[,c(1:11)])\n\n\n'data.frame':   13528 obs. of  11 variables:\n $ RegionID  : int  6181 12447 39051 17426 6915 40326 13271 18959 54296 38128 ...\n $ SizeRank  : int  0 1 2 3 4 5 6 7 8 9 ...\n $ RegionName: chr  \"New York\" \"Los Angeles\" \"Houston\" \"Chicago\" ...\n $ RegionType: chr  \"city\" \"city\" \"city\" \"city\" ...\n $ StateName : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ State     : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ Metro     : chr  \"New York-Newark-Jersey City, NY-NJ-PA\" \"Los Angeles-Long Beach-Anaheim, CA\" \"Houston-The Woodlands-Sugar Land, TX\" \"Chicago-Naperville-Elgin, IL-IN-WI\" ...\n $ CountyName: chr  \"Queens County\" \"Los Angeles County\" \"Harris County\" \"Cook County\" ...\n $ 2000.01.31: num  284723 302701 146377 179997 154391 ...\n $ 2000.02.29: num  286885 303000 146270 180472 154691 ...\n $ 2000.03.31: num  288711 304356 145921 181368 154943 ...\n\n\n\n\nBehind the Scenes\n#Converting the Date from factor to character\n\nhouse_clean <- house_price %>%\n            mutate(Monthly_parsed = as.Date(Monthly,\"%Y.%m.%d\"))\n\n\nhouse_clean[[\"Monthly\"]]<- as.character(house_clean$Monthly)\n\nhouse_price[[\"Monthly\"]]<- as.character(house_price $Monthly)\nsummary(house_clean)\n\n\n    RegionID         SizeRank      RegionName         RegionType       \n Min.   :  3300   Min.   :    0   Length:3720200     Length:3720200    \n 1st Qu.: 17381   1st Qu.: 3511   Class :character   Class :character  \n Median : 31963   Median : 7196   Mode  :character   Mode  :character  \n Mean   : 51628   Mean   : 8235                                        \n 3rd Qu.: 46317   3rd Qu.:11713                                        \n Max.   :827230   Max.   :28439                                        \n                                                                       \n  StateName            State              Metro            CountyName       \n Length:3720200     Length:3720200     Length:3720200     Length:3720200    \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   Monthly              Price         Monthly_parsed      \n Length:3720200     Min.   :  17773   Min.   :2000-01-31  \n Class :character   1st Qu.: 172701   1st Qu.:2005-09-30  \n Mode  :character   Median : 244810   Median :2011-06-30  \n                    Mean   : 321841   Mean   :2011-07-01  \n                    3rd Qu.: 369075   3rd Qu.:2017-03-31  \n                    Max.   :8337561   Max.   :2022-11-30  \n                    NA's   :1201126                       \n\n\nWe see some missing values in the Price variable, but before I deal with those values, I will filter my data to the cities that I am interested the most\n\n\nBehind the Scenes\npdx_data <- house_clean %>%\n  dplyr:::filter(RegionID== 13373)  %>%\n  dplyr:::filter(Monthly_parsed >= \"2014-01-01\")\n\nsummary(pdx_data)\n\n\n    RegionID        SizeRank   RegionName         RegionType       \n Min.   :13373   Min.   :22   Length:107         Length:107        \n 1st Qu.:13373   1st Qu.:22   Class :character   Class :character  \n Median :13373   Median :22   Mode  :character   Mode  :character  \n Mean   :13373   Mean   :22                                        \n 3rd Qu.:13373   3rd Qu.:22                                        \n Max.   :13373   Max.   :22                                        \n  StateName            State              Metro            CountyName       \n Length:107         Length:107         Length:107         Length:107        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   Monthly              Price        Monthly_parsed      \n Length:107         Min.   :410582   Min.   :2014-01-31  \n Class :character   1st Qu.:506597   1st Qu.:2016-04-15  \n Mode  :character   Median :564473   Median :2018-06-30  \n                    Mean   :563372   Mean   :2018-06-30  \n                    3rd Qu.:588606   3rd Qu.:2020-09-15  \n                    Max.   :758797   Max.   :2022-11-30  \n\n\nAfter filtering the data, we don’t have any missing values\n\nCoerce to a tsibble with as_tsibble()\nA time series can be recorded as a tsibble object in R. tsibble objects extend tidy data frames (tibble objects) by introducing temporal structure, and to do it, we need to declare key and index. In this case, the Monthly_parsed containing the data-time is the index and the RegionID is the key. Other columns can be considered as measured variables.\n\n\nBehind the Scenes\ntsb_pdx <- pdx_data %>%\n                   select(RegionName,RegionID, Monthly_parsed, Price)\n\ntsb_pref_pdx <-tsb_pdx%>%\n  as_tsibble(key= RegionName, index= Monthly_parsed)%>%\n                   index_by(year_month = ~ yearmonth(.))\n\ntsibble_pdx <-tsb_pref_pdx%>%\n  select(-RegionID)%>%\n  as_tsibble(key= RegionName, index= year_month)%>%\n  mutate(Prices = Price/1000)"
  },
  {
    "objectID": "Models/TimeSeries/Time Series Project.html#data-visualization",
    "href": "Models/TimeSeries/Time Series Project.html#data-visualization",
    "title": "Time Series",
    "section": "Data Visualization",
    "text": "Data Visualization\nTo visualize the data, I could use the autoplot() command, but I rather to create my graph with ggplot.\n\n\nBehind the Scenes\nplot_pdx_house <- tsibble_pdx %>%\n  ggplot(aes(x= year_month, y= Prices)) +\n  geom_line(size=1, color= \"darkgreen\")+\n   \n    labs(y=\"Price in Thousands of Dollars \", \n       x= \" \",\n       title=\" Four Bedroom House Prices in Portland, OR, 2014-2022 \",\n       caption = \"data:https://www.zillow.com/research/data\")+\n  scale_y_continuous(labels=scales::dollar_format())+\n   theme_light()\n\n\nplot_pdx_house \n\n\n\n\n\nData is non- stationary, we can see a trend-cycle component in the graph above.\n\n\nBehind the Scenes\ntsibble_pdx %>%\ngg_subseries(Price/1000)+\n  labs(y= \"Price in Thousands of Dollars\",\n       x= \"Year\")+theme_minimal()+\n  scale_y_continuous(labels=scales::dollar_format())+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\nBehind the Scenes\ntsibble_pdx%>%\ngg_season(Price/1000, labels = \"both\")+\n  labs(x= \"\",\n       y= \"Price in Thousands of Dollars \", \n       title=\"Portland's Seasonal Plot\")+\n  \n  scale_y_continuous(labels=scales::dollar_format())+\n  theme_minimal()"
  },
  {
    "objectID": "Models/TimeSeries/Time Series Project.html#determining-stationarity",
    "href": "Models/TimeSeries/Time Series Project.html#determining-stationarity",
    "title": "Time Series",
    "section": "Determining Stationarity",
    "text": "Determining Stationarity\nIn our analysis, we use the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test (Kwiatkowski et al., 1992). In this test, the null hypothesis is that the data are stationary, and we look for evidence that the null hypothesis is false. Consequently, small p-values (e.g., less than 0.05) suggest that differencing is required. The test can be computed using the unitroot_kpss() function.\n\n\nBehind the Scenes\ntsibble_pdx%>%\n  features(Prices, unitroot_kpss)\n\n\n# A tibble: 1 × 3\n  RegionName kpss_stat kpss_pvalue\n  <chr>          <dbl>       <dbl>\n1 Portland        1.97        0.01\n\n\nThe p-value is reported as 0.01 if it is less than 0.01, and as 0.1 if it is greater than 0.1. In this case, the test statistic (1.946) is bigger than the 1% critical value, so the p-value is less than 0.01, indicating that the null hypothesis is rejected. That is, the data are not stationary.\n\n\nBehind the Scenes\ntsibble_pdx %>% \n  features(Prices ,unitroot_ndiffs)\n\n\n# A tibble: 1 × 2\n  RegionName ndiffs\n  <chr>       <int>\n1 Portland        1\n\n\nAs we saw from the KPSS tests above, one difference (d) is required to make the tsibble_pdx data stationary."
  },
  {
    "objectID": "Models/TimeSeries/Time Series Project.html#autocorrelation",
    "href": "Models/TimeSeries/Time Series Project.html#autocorrelation",
    "title": "Time Series",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\n\nBehind the Scenes\ntsibble_pdx %>%\n  gg_tsdisplay(Prices,\n                     plot_type='partial')+\n       labs(y=\"Thousands of Dollars \", \n       x= \" \")\n\n\n\n\n\nACF does not drop quickly to zero, moreover the value is large and positive (almost 1 in this case). All these are signs of a non-stationary time series. Therefore it should be differenced to obtain a stationary series.\nPACF value r1 is almost 1. All other values ri,i >1 are small. This is a sign of a non stationary process that should be differenced in order to obtain a stationary series.\nThe data are clearly non-stationary, so we will first take a seasonal difference. The seasonally differenced data are shown below:\n\n\nBehind the Scenes\ntsibble_pdx %>%\n  gg_tsdisplay(difference(Prices, 12),\n               plot_type='partial', lag=36) +\n  labs(title=\"Seasonally differenced\", y=\"\")\n\n\n\n\n\nOur aim now is to find an appropriate ARIMA model based on the ACF and PACF shown in the Double Differenced graph."
  },
  {
    "objectID": "Models/TimeSeries/Time Series Project.html#seasonal-arima-model",
    "href": "Models/TimeSeries/Time Series Project.html#seasonal-arima-model",
    "title": "Time Series",
    "section": "Seasonal Arima Model",
    "text": "Seasonal Arima Model\n\n\nBehind the Scenes\nall_fit <- tsibble_pdx%>%\n  model(\n    arima212012 = ARIMA(Prices ~ pdq(2,1,2)+ PDQ(0,1,2)),\n    arima210011 = ARIMA(Prices ~ pdq(2,1,0)+ PDQ(0,1,1)),\n    stepwise = ARIMA(Prices),\n    search = ARIMA(Prices,stepwise=FALSE))\n\n\n\n\nBehind the Scenes\nall_fit %>% pivot_longer(!RegionName,\n            names_to = \"Model name\", \n            values_to = \"Orders\")\n\n\n# A mable: 4 x 3\n# Key:     RegionName, Model name [4]\n  RegionName `Model name`                             Orders\n  <chr>      <chr>                                   <model>\n1 Portland   arima212012           <ARIMA(2,1,2)(0,1,2)[12]>\n2 Portland   arima210011           <ARIMA(2,1,0)(0,1,1)[12]>\n3 Portland   stepwise                <ARIMA(3,1,2) w/ drift>\n4 Portland   search       <ARIMA(2,1,3)(0,0,1)[12] w/ drift>\n\n\n\n\nBehind the Scenes\nglance(all_fit) %>% arrange(AICc) %>% select(.model:BIC)\n\n\n# A tibble: 4 × 6\n  .model      sigma2 log_lik   AIC  AICc   BIC\n  <chr>        <dbl>   <dbl> <dbl> <dbl> <dbl>\n1 arima212012   2.81   -190.  394.  395.  412.\n2 search        2.14   -191.  399.  400.  420.\n3 stepwise      2.23   -194.  402.  403.  420.\n4 arima210011   4.53   -207.  423.  423.  433.\n\n\nOf these models, the best is the ARIMA(2,1,2)(0,1,2)[12]model (i.e., it has the smallest AICc value).\n\n\nBehind the Scenes\narima212012 <- tsibble_pdx %>%\n  model(arima212012 = ARIMA(Prices ~ pdq(2,1,2)+ PDQ(0,1,2)))%>%\n  report()\n\n\nSeries: Prices \nModel: ARIMA(2,1,2)(0,1,2)[12] \n\nCoefficients:\n         ar1     ar2     ma1     ma2     sma1    sma2\n      0.5178  0.0341  1.0092  0.9999  -0.8285  0.1308\ns.e.  0.1130  0.1206  0.0573  0.0692   0.1461  0.1514\n\nsigma^2 estimated as 2.813:  log likelihood=-189.9\nAIC=393.8   AICc=395.1   BIC=411.6\n\n\n\n\nBehind the Scenes\nall_fit %>% select(arima212012) %>%\n  gg_tsresiduals()\n\n\n\n\n\n\n\nBehind the Scenes\nall_fit %>% select(\"search\") %>%\n  gg_tsresiduals()\n\n\n\n\n\n\n\nBehind the Scenes\naugment(all_fit) %>%\n  filter(.model=='arima212012') %>%\n  features(.innov, ljung_box, lag = 36, dof = 6)\n\n\n# A tibble: 1 × 4\n  RegionName .model      lb_stat lb_pvalue\n  <chr>      <chr>         <dbl>     <dbl>\n1 Portland   arima212012    36.7     0.186\n\n\n\n\nBehind the Scenes\ntsibble_pdx %>%\n  model(ARIMA(Prices ~ pdq(2,1,2) + PDQ(0,1,2))) %>%\n  forecast() %>%\n  autoplot(tsibble_pdx) +\n  labs(y=\" Thousands of $US \",\n       x =\" \",\n       title=\"Forecast from the ARIMA(2,1,2)(0,1,2)[12] model applied to the Portland House Prices data\")+\ntheme_minimal()\n\n\n\n\n\nBehind the Scenes\n##Price in Thousands of Dollars\ntsibble_pdx %>%\n  model(ARIMA(Prices ~ pdq(2,1,2) + PDQ(0,1,2))) %>%\n  forecast()\n\n\n# A fable: 24 x 5 [1M]\n# Key:     RegionName, .model [1]\n   RegionName .model                                  year_m…¹      Prices .mean\n   <chr>      <chr>                                      <mth>      <dist> <dbl>\n 1 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2022 Dec N(735, 2.9)  735.\n 2 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Jan  N(737, 21)  737.\n 3 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Feb  N(740, 75)  740.\n 4 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Mar N(745, 156)  745.\n 5 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Apr N(750, 255)  750.\n 6 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 May N(755, 365)  755.\n 7 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Jun N(758, 482)  758.\n 8 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Jul N(760, 604)  760.\n 9 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Aug N(762, 727)  762.\n10 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Sep N(764, 852)  764.\n# … with 14 more rows, and abbreviated variable name ¹​year_month"
  },
  {
    "objectID": "Models/TimeSeries/Time Series Project.html#ets",
    "href": "Models/TimeSeries/Time Series Project.html#ets",
    "title": "Time Series",
    "section": "ETS",
    "text": "ETS\n\n\nBehind the Scenes\nfit_ets <- tsibble_pdx %>%\n  model(ETS(Prices))\nreport(fit_ets)\n\n\nSeries: Prices \nModel: ETS(M,Ad,N) \n  Smoothing parameters:\n    alpha = 0.9997705 \n    beta  = 0.9997704 \n    phi   = 0.9088093 \n\n  Initial states:\n     l[0]    b[0]\n 406.6826 3.75712\n\n  sigma^2:  0\n\n     AIC     AICc      BIC \n659.0909 659.9309 675.1278 \n\n\nThe model selected is ETS(M,Ad,N)\n\n\nBehind the Scenes\ncomponents(fit_ets) %>%\n  autoplot() +\n  labs(title = \"ETS(M,Ad,N) components\")\n\n\n\n\n\nBecause this model has multiplicative errors, the innovation residuals are not equivalent to the regular residuals.\n\n\nBehind the Scenes\nfit_ets %>%\n    augment() %>%\n    select(.innov, .resid) %>%\n    pivot_longer(c(.innov, .resid)) %>%\n    autoplot()+\n   theme_fivethirtyeight()\n\n\n\n\n\n\n\nBehind the Scenes\nfit_ets%>%\n    gg_tsresiduals()\n\n\n\n\n\n\n\nBehind the Scenes\nfit_ets %>%\n  forecast(h = 24) %>%\n  autoplot(tsibble_pdx)+\n\n   theme_light()\n\n\n\n\n\n\n\nBehind the Scenes\nbind_rows(\n    arima212012 %>% accuracy(),\n    fit_ets %>% accuracy()) %>%\n  select(-ME, -MPE, -ACF1)\n\n\n# A tibble: 2 × 8\n  RegionName .model      .type     RMSE   MAE  MAPE   MASE  RMSSE\n  <chr>      <chr>       <chr>    <dbl> <dbl> <dbl>  <dbl>  <dbl>\n1 Portland   arima212012 Training  1.52  1.13 0.193 0.0284 0.0308\n2 Portland   ETS(Prices) Training  2.27  1.62 0.274 0.0408 0.0461\n\n\nIn this case the ARIMA model seems to be more accurate model based on the test set RMSE, MAPE and MASE."
  },
  {
    "objectID": "Mortality.html",
    "href": "Mortality.html",
    "title": "Child Mortality",
    "section": "",
    "text": "Behind the Scenes\nlibrary(emojifont)\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.0\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n\nBehind the Scenes\nlibrary(usefunc)\n\n\n\nAttaching package: 'usefunc'\n\nThe following object is masked from 'package:dplyr':\n\n    last\n\n\nBehind the Scenes\nlibrary(showtext)\n\n\nLoading required package: sysfonts\nLoading required package: showtextdb\n\n\nBehind the Scenes\nlibrary(ggplot2)\n\n\nThe rate of child mortality is at its lowest point ever. From 12.5 million in 1990 to 5.2 million in 2019, child mortality has more than halved in less than three decades. It is essential to recognize this significant accomplishment.\nNations committed to achieving Sustainable Development Goals (SDGs). By 2030, all countries must achieve at least a 2.5% child mortality rate. This would imply that regardless of where a baby is born, more than 97.5% survive the first five years of their existence. Currently, 3.9% of children worldwide die before turning five, equating to an average of 15,000 deaths per day.\n\n\nBehind the Scenes\ndf <-read.csv(\"https://raw.githubusercontent.com/karolo89/Raw_Data/main/child-deaths-igme-data.csv\")\n\n\n\n\nBehind the Scenes\ndf <- tibble(year = c(1800, 1900, 2020),\n             percent_die = c(43.3, 36.2, 3.9))\n\nfont_add_google(\"Roboto Slab\", \"slab\")\nfont_add_google(\"Roboto\", \"roboto\")\nshowtext_auto()\n\n# prep data\ndf_data <- df %>%\n  mutate(per_hundred = round(percent_die),\n         survive = (100 - per_hundred),\n         facet_label = paste0(year, \" (\", survive, \"%)\")) %>%\n  pivot_longer(cols = 3:4, values_to = \"perc\", names_to = \"type\") %>%\n  select(-percent_die)\n\nplot_data <- rep_df(expand.grid(x = rep(1:10), y = rep(1:10)), length(unique(df_data$facet_label))) %>%\n  mutate(year = rep(unique(df_data$facet_label), each = 100),\n         label = fontawesome('fa-user'),\n         type = rep(df_data$type, times = df_data$perc))\n\n# plot\nggplot() +\n  geom_text(data = plot_data,\n            mapping = aes(x = x,\n                          y = y,\n                          label = label,\n                          colour = type),\n            family='fontawesome-webfont', size = 6) +\n  facet_wrap(~year) +\n  \n  scale_colour_manual(\"\", values = c(\"#d3d3d3\", \"#293241\")) +\n  \n  labs(title = \"Child mortality rates continue to fall\",\n       subtitle = str_wrap_break(\"What percentage of children live beyond their fifth birthday?\\n\\n\", 70),\n       caption = \"Karol O. | Data: ourworldindata.org\",\n       x = \"\",\n       y = \"\") +\n  \n  theme_minimal() +\n  \n  theme(panel.spacing = unit(2, \"lines\"),\n        \n        plot.background = element_rect(fill = \"#EEE4E1\", colour=\"#EEE4E1\"),\n        panel.background = element_rect(fill = \"#EEE4E1\", colour= \"#EEE4E1\"),\n        legend.position=\"none\",\n        strip.background =element_rect(fill=\"#EEE4E1\", colour =\"#EEE4E1\"),\n        strip.text = element_text(colour = '#3D5A80', family=\"slab\", size=24),\n        plot.title = element_text(colour = \"#3D5A80\", size=26, hjust = 0.5, family=\"slab\"),\n        plot.subtitle = element_text(colour = \"#404040\", size=14, hjust = 0.5, family=\"slab\"),\n        plot.caption = element_text(colour = \"#404040\", size=12, hjust = 0.01, family=\"slab\"),\n        \n        \n        \n        plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), \"cm\"),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.text = element_blank())"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Dispatched Calls for Service\n          \n        \n      \n      \n        \n      \n      \n        2023-04-23\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Worst Drivers\n          \n        \n      \n      \n        \n      \n      \n        2023-03-18\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Child Mortality\n          \n        \n      \n      \n        \n      \n      \n        2023-03-17\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Freedom in the World\n          \n        \n      \n      \n        \n      \n      \n        2023-03-06\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Transit Costs Project\n          \n        \n      \n      \n        \n      \n      \n        2023-03-06\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Census Data\n          \n        \n      \n      \n        \n      \n      \n        2023-02-28\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Hurricanes\n          \n        \n      \n      \n        \n      \n      \n        2023-02-05\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          House Prices in Portland, OR\n          \n        \n      \n      \n        \n      \n      \n        2022-12-01\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Reported Bias Crimes in Portland\n          \n        \n      \n      \n        \n      \n      \n        2022-02-28\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Voter Registrations Are Way, Way Down During The Pandemic\n          \n        \n      \n      \n        \n      \n      \n        2020-03-06\n      \n    \n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "revealjs/presentationkoe.html#about-me",
    "href": "revealjs/presentationkoe.html#about-me",
    "title": "Karol Orozco",
    "section": "ABOUT ME",
    "text": "ABOUT ME\n\nI help to create and sustain communities, programs, and solutions that enhance the use of responsible data science and business practices for equitable social good.\n\n\n\n\n\n\nLinkedIn: @karolorozcoe | GitHub: @karolo89"
  },
  {
    "objectID": "revealjs/presentationkoe.html#skills",
    "href": "revealjs/presentationkoe.html#skills",
    "title": "Karol Orozco",
    "section": "SKILLS",
    "text": "SKILLS\n\n𝑷𝒓𝒐𝒇𝒆𝒔𝒔𝒊𝒐𝒏𝒂𝒍 𝑺𝒌𝒊𝒍𝒍𝒔\nData Analysis | Partnership Management | Business Development | Diversity & Inclusion | Strategic Planning | Process Improvement | Project Management\n\n\n𝑫𝒐𝒎𝒂𝒊𝒏 𝑲𝒏𝒐𝒘𝒍𝒆𝒅𝒈𝒆\nR | Tableau | SQL | Community Affairs | Leadership | Community Engagement | Latinx Community | Shared Value\n\n\nLinkedIn: @karolorozcoe | GitHub: @karolo89"
  },
  {
    "objectID": "revealjs/presentationkoe.html#education",
    "href": "revealjs/presentationkoe.html#education",
    "title": "Karol Orozco",
    "section": "EDUCATION",
    "text": "EDUCATION\n\n\nWillamette University | Portland, OR\nMaster in Data Science | August 2023\nPresidential Scholarship\nWillamette University | Portland, OR\nMaster of Business Administration | August 2022\nGeorge and Colleen Hoyt Not-For-Profit Scholarship Atkinson Honor\nBeta Gamma Sigma\nUniversidad Autonoma del Caribe | Barranquilla, Colombia\nBachelor’s Degree, Industrial Engineering | July 2014\n\n\n\n\nLinkedIn: @karolorozcoe | GitHub: @karolo89"
  },
  {
    "objectID": "revealjs/presentationkoe.html#portfolio",
    "href": "revealjs/presentationkoe.html#portfolio",
    "title": "Karol Orozco",
    "section": "PORTFOLIO",
    "text": "PORTFOLIO\n\n\n\n\n\n\n\n\nLinkedIn: @karolorozcoe | GitHub: @karolo89"
  },
  {
    "objectID": "revealjs/presentationkoe.html#portfolio-1",
    "href": "revealjs/presentationkoe.html#portfolio-1",
    "title": "Karol Orozco",
    "section": "PORTFOLIO",
    "text": "PORTFOLIO\n\n\n\n\n\n\n\n\nLinkedIn: @karolorozcoe | GitHub: @karolo89"
  },
  {
    "objectID": "revealjs/presentationkoe.html#portfolio-2",
    "href": "revealjs/presentationkoe.html#portfolio-2",
    "title": "Karol Orozco",
    "section": "PORTFOLIO",
    "text": "PORTFOLIO\n\n\n\n\n\n\n\n\nLinkedIn: @karolorozcoe | GitHub: @karolo89"
  },
  {
    "objectID": "revealjs/presentationkoe.html#portfolio-3",
    "href": "revealjs/presentationkoe.html#portfolio-3",
    "title": "Karol Orozco",
    "section": "PORTFOLIO",
    "text": "PORTFOLIO\n\n\n\n\n\n\n\n\nLinkedIn: @karolorozcoe | GitHub: @karolo89"
  },
  {
    "objectID": "revealjs/presentationkoe.html#contact-me",
    "href": "revealjs/presentationkoe.html#contact-me",
    "title": "Karol Orozco",
    "section": "CONTACT ME",
    "text": "CONTACT ME\n\n\n\nLinkedIn:@karolorozcoe\nGitHub: @karolo89\n\n\n\n\n\n Back to My Page"
  },
  {
    "objectID": "shiny/shiny.html",
    "href": "shiny/shiny.html",
    "title": "Hispanic",
    "section": "",
    "text": "Warning: package 'thematic' was built under R version 4.2.3\n\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\n\n\nDefine UI for application\n\n\nWarning: Navigation containers expect a collection of\n`bslib::nav()`/`shiny::tabPanel()`s and/or\n`bslib::nav_menu()`/`shiny::navbarMenu()`s. Consider using `header` or `footer`\nif you wish to place content above (or below) every panel's contents.\n\n\nPhantomJS not found. You can install it with webshot::install_phantomjs(). If it is installed, please make sure the phantomjs executable can be found via the PATH variable.\n\n\nShiny applications not supported in static R Markdown documents"
  },
  {
    "objectID": "social/biascrimes/biascrime.html",
    "href": "social/biascrimes/biascrime.html",
    "title": "Reported Bias Crimes in Portland",
    "section": "",
    "text": "This data was obtained from the PortlandMaps- Open Data website- Reported Bias Crime Statistics, Portland Police Bureau.\nA hate crime is a criminal offense committed against persons, property, or society that is motivated, in whole or in part, by an offender’s bias against an individual’s or a group’s race, religion, ethnic/national origin, gender, age, disability or sexual orientation. (Definition developed at the 1998 IACP Summit on Hate Crime in America.) This data exploration aims to learn about the most common locations where these crimes occurred and their types."
  },
  {
    "objectID": "social/biascrimes/biascrime.html#insights",
    "href": "social/biascrimes/biascrime.html#insights",
    "title": "Reported Bias Crimes in Portland",
    "section": "Insights:",
    "text": "Insights:\n\nThe victims are from 2 to 83 years old, with a median of 32 (when the information was available)\nThe ages of the suspects are from 11 to over 80 years, with a median of 40 (when the information was available)\nMost hate crimes were reported in 2021, with a decline in 2022, but bias incidents have increased over the past two years.\nOver the past five years, the most common motivations for bias have been race, sexual orientation, and ethnicity.\nThe most common cases of bias reported have been against Black or African-American people, followed by Anti-Gay (Male) and Anti-White.\nThe places where these incidents occurred the most are Public Spaces (Streets/Roads/Sidewalks), residences, and Parking Lots.\n\n\n\nBehind the Scenes\npp1 <- crime %>%\n filter(!(Bias.Category %in% \"\")) %>%\n filter(!(Bias.Type %in% \"\")) %>%\n filter(!(Location.Type %in% \n \"\")) %>%\n filter(!(Primary.Offense.Type %in% c(\"\", \"Hit & Run-Property\"))) %>%\n filter(!(Suspect.Gender %in% \n \"\")) %>%\n filter(!(Suspect.Race %in% \"\")) %>%\n filter(!(Victim.Gender %in% \"\")) %>%\n filter(!(Victim.Race %in% \n \"\")) %>%\n filter(!(Victim.Suspect.Relationship %in% \"\")) %>%\n ggplot() +\n aes(x = Victim.Age, fill = Victim.Gender) +\n geom_histogram(bins = 30L) +\n scale_fill_viridis_d(option = \"viridis\") +\n labs(title = \"Number of Victims by Gender and Age Group \",  x=\"Age\", y=\"\") +\n theme_minimal() +\n theme(\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        legend.position = \"none\",\n        plot.title.position = \"plot\", \n        plot.title = element_text(lineheight = 0.5, face = \"bold\", size = 12, hjust = 0.5),\n        axis.text = element_text(size = 7),\n        axis.title = element_text(size= 8),\n        plot.background = element_rect(fill = \"#fafafa\", colour = \"#fafafa\"),\n        panel.background = element_rect(fill = \"#fafafa\", colour = \"#fafafa\"))\n\npp2 <-crime %>%\n filter(!(Bias.Category %in% \"\")) %>%\n filter(!(Bias.Type %in% \"\")) %>%\n filter(!(Location.Type %in% \n \"\")) %>%\n filter(!(Primary.Offense.Type %in% c(\"\", \"Hit & Run-Property\"))) %>%\n filter(!(Suspect.Gender %in% \n \"\")) %>%\n filter(!(Suspect.Race %in% \"\")) %>%\n filter(!(Victim.Gender %in% \"\")) %>%\n filter(!(Victim.Race %in% \n \"\")) %>%\n filter(!(Victim.Suspect.Relationship %in% \"\")) %>%\n ggplot() +\n aes(x = Suspect.Age, y = Suspect.Gender, fill = Suspect.Gender) +\n geom_boxplot() +\n  labs(x= \"Age\",\n       y= \"\")+\n scale_fill_viridis_d(option = \"viridis\", \n direction = 1) +\n labs(title = \"Number of Suspects by Gender and Age Group \",\n      x= \"Age\") +\n  theme_minimal()+\n theme(\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        legend.position = \"none\",\n        plot.title.position = \"plot\", \n        plot.subtitle = element_text(hjust = 0.5, lineheight = 0.4, size = 10),\n        plot.title = element_text(lineheight = 0.5, face = \"bold\", size = 12, hjust = 0.5),\n        axis.text = element_text(size = 7),\n        axis.title = element_text(size= 8),\n        plot.background = element_rect(fill = \"#fafafa\", colour = \"#fafafa\"),\n        panel.background = element_rect(fill = \"#fafafa\", colour = \"#fafafa\"))\n\n\n\npp1+pp2\n\n\n\n\n\n\n\nBehind the Scenes\nlibrary(directlabels)\nlibrary(gganimate)\n\nplot <- ggplot(crime_type) +\n aes(x = year, y = prov_freq, colour = Case.Type) +\n geom_line() +\n  \n    geom_point(aes(x = year, y = prov_freq, shape=Case.Type, color=Case.Type))+\n  \n scale_color_viridis(discrete = TRUE, option = \"D\")+\n  scale_fill_viridis(discrete = TRUE) +\n  \n labs(y = \"Number of Cases\", \n      x= \"Year\",\n title = \"Report of Bias by Year\", \n subtitle = \"Data current through September 2022\", \n \n caption = \"Police Bureau: The graph visualizes the current case status for all reported bias/hate incidents that have been reviewed by the assigned detective.\\nBias Incidents (instances in which bias occurred, but no crime occurred) began to be recorded in March 2020\") +\n    geom_dl(aes(label = Case.Type), method = list(dl.trans(x = x + 0.2), \"last.points\", cex = 0.8)) +\n  \n theme_minimal() +\n theme(\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        legend.position = \"none\",\n        plot.title.position = \"plot\", \n        plot.subtitle = element_text(hjust = 0.5, lineheight = 0.4, size = 10),\n        plot.title = element_text(lineheight = 0.5, face = \"bold\", size = 12, hjust = 0.5),\n        axis.text = element_text(size = 7),\n        axis.title = element_text(size= 8),\n        plot.background = element_rect(fill = \"#fafafa\", colour = \"#fafafa\"),\n        panel.background = element_rect(fill = \"#fafafa\", colour = \"#fafafa\"))\n\nplot +transition_reveal(year)\n\n\n\n\n\n\n\nBehind the Scenes\n## Bias Category per year\n\n## Bias Category & Type\nbc_type <- crime%>%\n  group_by(Bias.Type, Bias.Category, year)%>%\n  summarise(frequency = n())%>%\nfilter(!(Bias.Category == \"\" | Bias.Category ==\"None\"))\n\nbc_type %>%\n  filter(year>=2020)%>%\nggplot(aes(x = reorder(Bias.Category, -frequency, sum), y = frequency, fill = Bias.Category, colour = Bias.Category)) +\n geom_col() +\n  scale_color_viridis(discrete = TRUE, option = \"D\")+\n  scale_fill_viridis(discrete = TRUE) +\n theme_minimal() +\n    scale_y_continuous(limits = c(0, 70)) +\n\n\n  labs(x=\" \", y=\" \", title = \"Bias Motivation Categories for Victims of Single Bias Incidents since 2020\")+\n  coord_flip()+\n  theme(legend.position = \"none\", \n        panel.spacing = unit(1, \"lines\"),\n        plot.title.position = \"plot\", \n        plot.caption.position = \"plot\", \n        plot.caption = element_text(hjust = 0, lineheight = 0.4, size = 8),\n        plot.title = element_text(lineheight = 0.4, face = \"bold\", size = 12, hjust= 0.5),\n        strip.text = element_text(size = 10),\n        axis.text = element_text(size = 6.5),\n        plot.background = element_rect(fill = \"#fafafa\", colour = \"#fafafa\"),\n        panel.background = element_rect(fill = \"#fafafa\", colour = \"#fafafa\"))+\n  \n facet_grid(vars(year), scale= \"free_y\")\n\n\n\n\n\n\n\nBehind the Scenes\nbias_t <- bc_type %>%\nfilter(year>= 2020)\n\n\np <- ggplot(bias_t) +\n  aes(x = reorder(Bias.Type,-frequency,sum), y = frequency, colour= factor(Bias.Type), fill= Bias.Type) +\n  geom_col() +\n   scale_color_viridis(discrete = TRUE, option = \"D\")+\n  scale_fill_viridis(discrete = TRUE) +\n  labs(x = \"\", y= \"\", title = \"Frequency of Bias Type Since 2020\") +\n  coord_flip() +\n  theme_minimal() +\ntheme(legend.position = \"none\", \n        panel.background = element_rect(fill = \"#FAFAFA\", colour = \"#FAFAFA\"), \n        plot.background = element_rect(fill = \"#FAFAFA\", colour = \"#FAFAFA\"), \n        legend.background = element_rect(fill = \"transparent\", colour = \"transparent\"), \n        legend.key = element_rect(fill = \"transparent\", colour = \"transparent\"),\n        axis.text = element_text(family = \"dosis\", size= 8),\n        panel.grid.major = element_line(colour = \"#DEDEDE\"), \n        panel.grid.minor = element_blank())\n\np\n\n\n\n\n\n\n\nBehind the Scenes\n## Top Five Bias Incidents and Crimes Locations\n\np_loc <- crime %>%\n  filter(year >= 2020)%>%\n  filter(Case.Type== c(\"BIAS CRIME\", \"BIAS INCIDENT\"))%>%\n  group_by(Location.Type)%>%\n  summarise(frequency = n())%>%\nfilter(frequency >= 7)\n\n\n## Graph\n\nloc <- ggplot(p_loc) +\n aes(x = reorder(Location.Type, -frequency, sum), y = frequency, fill= factor(Location.Type)) +\n geom_col(width = 0.5) +\n  \n scale_color_viridis(discrete = TRUE, option = \"D\")+\n  scale_fill_viridis(discrete = TRUE) +\n  \n labs(y = \"Number of Cases\", x= \"\",\n title = \"Top Five Bias Incidents and Crimes Locations\", \n subtitle = \"Year 2020-22\")+\n  \n theme_minimal() +\n  \n theme(\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        legend.position = \"none\",\n        plot.title.position = \"plot\", \n        plot.subtitle = element_text(hjust = 0.5, lineheight = 0.4, size = 10),\n        plot.title = element_text(lineheight = 0.5, face = \"bold\", size = 12, hjust = 0.5),\n        axis.text = element_text(size = 7),\n        axis.title = element_text(size= 8, hjust = 0.5),\n        plot.background = element_rect(fill = \"#fafafa\", colour = \"#fafafa\"),\n        panel.background = element_rect(fill = \"#fafafa\", colour = \"#fafafa\"))\n\n\n\n  loc+transition_states(Location.Type, wrap = FALSE) + shadow_mark() + enter_grow() +  enter_fade()"
  },
  {
    "objectID": "social/Calls/calls.html#purpose",
    "href": "social/Calls/calls.html#purpose",
    "title": "Dispatched Calls for Service",
    "section": "Purpose",
    "text": "Purpose\nThis project aims to create a basic Shiny App with calls for services statistics from the Portland Police Bureau.\nLaw enforcement agencies use calls for service statistics to track and report the level of police activity in a specific area during a given time frame. These statistics are divided into dispatched calls, generated by the community through a call or text to 911 or the non-emergency line, and self-initiated/directed calls, which are officer-initiated calls for service.\nThe Portland Police Bureau counts all calls for service where at least one officer was dispatched or self-initiated a call, and these statistics focus on total police response. The dispatcher assigns each call for service a call type based on the preliminary nature of the call, and call types are categorized into one of eight call groups for reporting purposes. These call types may not be descriptive of the potential kind of crime but rather a description of the situation.\n\nColor Theming\n\ncustom_theme <- bs_theme(\n  version = 5,\n  bg = \"#FFFFFF\",\n  fg = \"#000000\",\n  primary = \"#0199F8\",\n  secondary = \"#FF374B\",\n  base_font = \"Maven Pro\"\n)\n\nthematic::thematic_shiny()"
  },
  {
    "objectID": "social/Calls/calls.html#load-libraries",
    "href": "social/Calls/calls.html#load-libraries",
    "title": "Dispatched Calls for Service",
    "section": "Load Libraries",
    "text": "Load Libraries\n\nlibrary(shiny)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(lubridate)\nlibrary(lubridate)\nlibrary(viridis) \nlibrary(zoo)\nlibrary(DT)\nlibrary(shinythemes)\nlibrary(thematic)\nlibrary(bslib)"
  },
  {
    "objectID": "social/Calls/calls.html#data-wrangling-multiple-datasets",
    "href": "social/Calls/calls.html#data-wrangling-multiple-datasets",
    "title": "Dispatched Calls for Service",
    "section": "Data Wrangling Multiple Datasets",
    "text": "Data Wrangling Multiple Datasets\n\ndispatch_calls2023 <- read.csv(\"https://raw.githubusercontent.com/karolo89/Raw_Data/main/dispatchedcalls_opendata_2023_1.csv\")|>\n  select(-ReportDateTime)|>\n  mutate(Priority = as.factor(Priority))|>\n  mutate(FinalCallCategory = as.factor(FinalCallCategory)) |>\n  mutate(FinalCallGroup = as.factor(FinalCallGroup))|>\n  mutate(Neighborhood = as.factor(Neighborhood))|>\n  mutate(ReportMonthYear = mdy(ReportMonthYear))|>\n  mutate_if(is.character, as.double)|>\n  mutate(ResponseTime= ResponseTime_sec/60)|>\n  separate(\"ReportMonthYear\", c(\"Year\", \"Month\", \"Day\"), sep = \"-\")|>\n  select(-Day)|>\n  mutate(date= as.yearmon(paste(Year, Month), \"%Y %m\"))\n\ndispatch_calls2022 <-read.csv(\"https://raw.githubusercontent.com/karolo89/Raw_Data/main/dispatchedcalls_opendata_2022_0.csv\")|>\n  mutate(Priority = as.factor(Priority))|>\n  mutate(FinalCallCategory = as.factor(FinalCallCategory)) |>\n  mutate(FinalCallGroup = as.factor(FinalCallGroup))|>\n  mutate(Neighborhood = as.factor(Neighborhood))|>\n  mutate(ReportMonthYear = mdy(ReportMonthYear))|>\n  mutate_if(is.character, as.double)|>\n  mutate(ResponseTime= ResponseTime_sec/60)|>\n  separate(\"ReportMonthYear\", c(\"Year\", \"Month\", \"Day\"), sep = \"-\")|>\n  select(-Day)|>\n  mutate(date= as.yearmon(paste(Year, Month), \"%Y %m\"))\n\ncalls <- rbind(dispatch_calls2022, dispatch_calls2023)\n\nmonth_call <- calls |>\n  group_by(date, Year, FinalCallGroup)|>\n  filter(FinalCallGroup!= \"NULL\") |>\n  summarise(count = n()) |>\n  arrange(desc(count))|>\n  na.omit() |> \n  ungroup()\n\ngroup_month <- calls |>\n  group_by(date, FinalCallGroup)|>\n  summarise(num= n())|>\n  mutate(date= as.Date(date))|>\n  arrange(desc(num))|>\n  top_n(5) |>\n  ungroup()\n\n\nColor Theming\n\ncustom_theme <- bs_theme(\n  version = 5,\n  bg = \"#FFFFFF\",\n  fg = \"#000000\",\n  primary = \"#0199F8\",\n  secondary = \"#FF374B\",\n  base_font = \"Maven Pro\"\n)\n\nthematic::thematic_shiny()"
  },
  {
    "objectID": "social/Calls/calls.html#shiny-ui",
    "href": "social/Calls/calls.html#shiny-ui",
    "title": "Dispatched Calls for Service",
    "section": "Shiny UI",
    "text": "Shiny UI\n\n## library(shiny)\n## library(shinythemes)\n## library(thematic)\n## library(bslib)\n\n# Define UI for application \nui <- fluidPage(theme = custom_theme,\n                # Application title\n                \n                titlePanel(\"Dispatched Calls for Service- Portland Police Bureau\"),\n                # sidebar\n                sidebarLayout(\n                  sidebarPanel(\n                    dateRangeInput(inputId = \"date\",\n                                   label   = \"Enter date\",\n                                   start   = \"2022-01-01\",\n                                   end     = \"2023-03-01\",\n                                   min     = \"2022-01-01\",\n                                   max     = \"2023-03-01\"),\n                    \n                    selectInput(inputId  = \"FinalCallGroup\",\n                                label    = \"Select Call Type\",\n                                choices  = group_month$FinalCallGroup,\n                                multiple = TRUE,\n                                selected = \"Disorder\"),\n                    plotOutput(\"density\")),\n                  # main panel\n                  mainPanel(\n                    plotOutput(outputId = \"plot\"),\n                    dataTableOutput(outputId = \"table\")\n                  )\n                )\n)"
  },
  {
    "objectID": "social/Calls/calls.html#shiny-server",
    "href": "social/Calls/calls.html#shiny-server",
    "title": "Dispatched Calls for Service",
    "section": "Shiny Server",
    "text": "Shiny Server\n\n# Create server function\n\nserver <- function(input, output) {\n  GM <- reactive({ group_month |> \n      filter(date >= input$date[1], \n             date <= input$date[2]) |> \n      filter(FinalCallGroup == input$FinalCallGroup)\n  })\n  TC <- reactive({  calls |> \n      filter(FinalCallGroup == input$FinalCallGroup) |> \n      arrange(date)})\n  \n  output$plot <- renderPlot({\n    GM() |> \n      ggplot(aes(x= fct_rev(fct_reorder(FinalCallGroup, num)), y= num))+\n      geom_bar(stat = 'identity', aes(color=FinalCallGroup, fill= FinalCallGroup))+\n      \n      scale_y_continuous(labels = scales::number_format(scale = .001, suffix = \"K\"))+\n      \n      scale_color_viridis_d()+\n      scale_fill_viridis_d()+\n      \n      labs(title= \"Total Calls by Group, 2022-23\",\n           x=\"\",\n           y= \"\") +\n      theme_minimal()+\n      theme(\n            legend.position = \"none\",\n            panel.background = element_rect(colour = \"#fdf8ec\", fill = \"#fdf8ec\"),\n            plot.background = element_rect(colour = \"#fdf8ec\", fill = \"#fdf8ec\"),\n            legend.background = element_rect(colour = \"#fdf8ec\", fill = \"#fdf8ec\"))\n  })\n  \n  output$table <- DT::renderDataTable({\n    TC() |> \n      datatable(options = list(scrollX = T))\n  })\n  output$density <- renderPlot({\n    TC() |> ggplot() + aes(y=ResponseTime, x=FinalCallGroup, color=FinalCallGroup) +\n      geom_boxplot()+\n      labs(title= \"Response Time by Call Type\",\n           x=\"\",\n           y= \"min\")+\n      theme(legend.position = \"none\")+\n      theme_minimal()+\n      theme(\n            legend.position = \"none\",\n            panel.background = element_rect(colour = \"#fdf8ec\", fill = \"#fdf8ec\"),\n            plot.background = element_rect(colour = \"#fdf8ec\", fill = \"#fdf8ec\"),\n            legend.background = element_rect(colour = \"#fdf8ec\", fill = \"#fdf8ec\"))\n  })\n  \n}\n\n# Build and run the application\nrun_with_themer(shinyApp(ui = ui, server = server))"
  },
  {
    "objectID": "social/Census/census.html",
    "href": "social/Census/census.html",
    "title": "Census Data",
    "section": "",
    "text": "Behind the Scenes\n# Set a year first\nthis.year = 2021\n\n### BASE PLOT EXAMPLE\n### Washington\nor_tracts <- tracts(state = 'OR', \n                    cb = T, year = this.year)\n\n# This is the structure of spatial data \nhead(or_tracts)\n\n\nSimple feature collection with 6 features and 13 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -122.9903 ymin: 44.88572 xmax: -118.7871 ymax: 45.7816\nGeodetic CRS:  NAD83\n  STATEFP COUNTYFP TRACTCE             AFFGEOID       GEOID   NAME\n1      41      047  001607 1400000US41047001607 41047001607  16.07\n2      41      005  980000 1400000US41005980000 41005980000   9800\n3      41      051  001301 1400000US41051001301 41051001301  13.01\n4      41      067  031516 1400000US41067031516 41067031516 315.16\n5      41      059  950500 1400000US41059950500 41059950500   9505\n6      41      051  005103 1400000US41051005103 41051005103  51.03\n             NAMELSAD STUSPS        NAMELSADCO STATE_NAME LSAD      ALAND\n1  Census Tract 16.07     OR     Marion County     Oregon   CT    1814790\n2   Census Tract 9800     OR  Clackamas County     Oregon   CT 1634590508\n3  Census Tract 13.01     OR  Multnomah County     Oregon   CT     744911\n4 Census Tract 315.16     OR Washington County     Oregon   CT    1879977\n5   Census Tract 9505     OR   Umatilla County     Oregon   CT 1073211610\n6  Census Tract 51.03     OR  Multnomah County     Oregon   CT     433965\n   AWATER                       geometry\n1       0 MULTIPOLYGON (((-122.9903 4...\n2 6684470 MULTIPOLYGON (((-122.266 45...\n3       0 MULTIPOLYGON (((-122.6348 4...\n4       0 MULTIPOLYGON (((-122.8068 4...\n5       0 MULTIPOLYGON (((-119.4346 4...\n6  155315 MULTIPOLYGON (((-122.6776 4...\n\n\nBehind the Scenes\nplot(or_tracts)\n\n\n\n\n\nBehind the Scenes\n# GGPLOT\nggplot(or_tracts) + \n  geom_sf() + \n  coord_sf()\n\n\n\n\n\nBehind the Scenes\n### GET CENSUS DATA\n### B25077_001E: MEDIAN HOME VALUE\nor <- get_acs(geography = \"tract\", year=this.year,\n              state = \"OR\", \n              variables = \"B25077_001E\")%>%\n  mutate(GEO_ID=paste0(\"1400000US\", GEOID))\n\nhead(or)\n\n\n# A tibble: 6 × 6\n  GEOID       NAME                                  varia…¹ estim…²   moe GEO_ID\n  <chr>       <chr>                                 <chr>     <dbl> <dbl> <chr> \n1 41001950100 Census Tract 9501, Baker County, Ore… B25077…  265000 46736 14000…\n2 41001950200 Census Tract 9502, Baker County, Ore… B25077…  175200 40020 14000…\n3 41001950300 Census Tract 9503, Baker County, Ore… B25077…  140600 22155 14000…\n4 41001950400 Census Tract 9504, Baker County, Ore… B25077…  190500 36472 14000…\n5 41001950500 Census Tract 9505, Baker County, Ore… B25077…  191100 11383 14000…\n6 41001950600 Census Tract 9506, Baker County, Ore… B25077…  237500 46978 14000…\n# … with abbreviated variable names ¹​variable, ²​estimate\n\n\n\n\nBehind the Scenes\njoinrace<- geo_join(or_tracts, orPct, \n                 by_sp=\"GEOID\", by_df=\"GEOID\")\n\n## SET GEOMETRY\noregon <- get_acs(geography = \"tract\", year=this.year,\n               state = \"OR\",\n               variables = \"B25077_001E\",\n               geometry = TRUE)\n\n#Plotting with MAPVIEW \nmapview(oregon, zcol = \"estimate\", legend = TRUE, \n        lwd=.25)\n\n\n\n\n\n\n\n\n\nBehind the Scenes\n## USE GEO_JOIN TO COMBINE SPATIAL DATA AND OTHER DATA FRAMES\n\njoinOR<- geo_join(or_tracts, or, \n                 by_sp=\"GEOID\", by_df=\"GEOID\")\n\n## USE TMAP PACKAGE\ntm_shape(joinOR)+\n  tm_fill(\"estimate\", style = \"quantile\", n=7, palette = \"Greens\")+\n  tm_legend(bg.color=\"white\", bg.alpha=0.6)+\n  tm_style(\"gray\")\n\n\n\n\n\nBehind the Scenes\n## SET GEOMETRY\noregon <- get_acs(geography = \"tract\", year=this.year,\n               state = \"OR\",\n               variables = \"B25077_001E\",\n               geometry = TRUE)\n\n#Plotting with MAPVIEW \nmapview(oregon, zcol = \"estimate\", legend = TRUE, \n        lwd=.25)\n\n\n\n\n\n\n\n\n\nBehind the Scenes\n## Plotting with LEAFLET\n\n## Leaflet with reactive\npal<-colorNumeric(\"Greens\", domain=0:ceiling(max(oregon$estimate, na.rm=TRUE)))\npopup<-paste(\"Tract: \", as.character(substring(oregon$GEOID, 6, 11)), \"<br>\",\n             \"Median Home Value: \", as.character(oregon$estimate))\nleaflet()%>%\n  addProviderTiles(\"CartoDB.Positron\")%>%\n  addPolygons(data=oregon,\n              fillColor= ~pal(oregon$estimate),\n              fillOpacity = 0.7,\n              weight = 0.4,\n              smoothFactor = 0.2,\n              popup = popup)\n\n\n\n\n\n\n\n\nBehind the Scenes\n## Total Hispanic/Latino\nor <- get_acs(geography = \"county\", variables = \"B03001_001\", state = 'OR', year = 2021)\n\nor %>%\n ggplot(aes(x = estimate, y = reorder(NAME, estimate))) +\n geom_point(color = \"red\", size = 1) +\n  labs(title = \"Latino Population by county in Oregon\",\n       subtitle = \"2018-2021 American Community Survey\",\n       y = \"\",\n       x = \"ACS estimate\")\n\n\n\n\n\n\n\nBehind the Scenes\noregon <- get_acs(geography = \"tract\", year=2021,\n               state = \"OR\",\n               variables = \"B03001_001\",\n               geometry = TRUE)\n\n#Plotting with MAPVIEW \nmapview(oregon, zcol = \"estimate\", legend = TRUE, \n        lwd=.25)"
  },
  {
    "objectID": "social/ChildMortality/childMortality.html",
    "href": "social/ChildMortality/childMortality.html",
    "title": "Child Mortality",
    "section": "",
    "text": "The rate of child mortality is at its lowest point ever. From 12.5 million in 1990 to 5.2 million in 2019, child mortality has more than halved in less than three decades. It is essential to recognize this significant accomplishment.\nNations committed to achieving Sustainable Development Goals (SDGs). By 2030, all countries must achieve at least a 2.5% child mortality rate. This would imply that regardless of where a baby is born, more than 97.5% survive the first five years of their existence. Currently, 3.9% of children worldwide die before turning five, equating to an average of 15,000 deaths per day.\n\n\nBehind the Scenes\ndf <-read.csv(\"https://raw.githubusercontent.com/karolo89/Raw_Data/main/child-deaths-igme-data.csv\")\n\ndf <- tibble(year = c(1800, 1900, 2020),\n             percent_die = c(43.3, 36.2, 3.9))\n\nfont_add_google(\"Roboto Slab\", \"slab\")\nfont_add_google(\"Roboto\", \"roboto\")\nshowtext_auto()\n\n# prep data\ndf_data <- df %>%\n  mutate(per_hundred = round(percent_die),\n         survive = (100 - per_hundred),\n         facet_label = paste0(year, \" (\", survive, \"%)\")) %>%\n  pivot_longer(cols = 3:4, values_to = \"perc\", names_to = \"type\") %>%\n  select(-percent_die)\n\nplot_data <- rep_df(expand.grid(x = rep(1:10), y = rep(1:10)), length(unique(df_data$facet_label))) %>%\n  mutate(year = rep(unique(df_data$facet_label), each = 100),\n         label = fontawesome('fa-user'),\n         type = rep(df_data$type, times = df_data$perc))\n\n# plot\nggplot() +\n  geom_text(data = plot_data,\n            mapping = aes(x = x,\n                          y = y,\n                          label = label,\n                          colour = type),\n            family='fontawesome-webfont', size = 6) +\n  facet_wrap(~year) +\n  \n  scale_colour_manual(\"\", values = c(\"#d3d3d3\", \"#293241\")) +\n  \n  labs(title = \"Child mortality rates continue to fall\",\n       subtitle = str_wrap_break(\"What percentage of children live beyond their fifth birthday?\\n\\n\", 70),\n       caption = \"Karol O. | Data: ourworldindata.org\",\n       x = \"\",\n       y = \"\") +\n  \n  theme_minimal() +\n  \n  theme(panel.spacing = unit(2, \"lines\"),\n        \n        plot.background = element_rect(fill = \"#EEE4E1\", colour=\"#EEE4E1\"),\n        panel.background = element_rect(fill = \"#EEE4E1\", colour= \"#EEE4E1\"),\n        legend.position=\"none\",\n        strip.background =element_rect(fill=\"#EEE4E1\", colour =\"#EEE4E1\"),\n        strip.text = element_text(colour = '#3D5A80', family=\"slab\", size=24),\n        plot.title = element_text(colour = \"#3D5A80\", size=26, hjust = 0.5, family=\"slab\"),\n        plot.subtitle = element_text(colour = \"#404040\", size=14, hjust = 0.5, family=\"slab\"),\n        plot.caption = element_text(colour = \"#404040\", size=12, hjust = 0.01, family=\"slab\"),\n        \n        \n        \n        plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), \"cm\"),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.text = element_blank())"
  },
  {
    "objectID": "social/drivers/drivers.html",
    "href": "social/drivers/drivers.html",
    "title": "Worst Drivers",
    "section": "",
    "text": "States with the Most drivers Involved in Fatal Collisions per 100m Vehicle Miles\n\n\nBehind the Scenes\nworst_drivers <- bad_drivers %>%\n  arrange(desc(fatalities)) %>%\n  mutate(Rank = row_number())\n\nstate_row <- worst_drivers %>%\n  filter(state == params$state) %>%\n  pull(Rank)\n\nworst_drivers %>%  \n  filter(Rank %in% 1:5 | state == params$state) %>%\n  mutate_at(.vars = vars(insuredDrivers.), .funs = ~scales::percent(. / 100)) %>%  \n  select(Rank, State = \"state\",'% Insured Drivers'= insuredDrivers.,`DUI Arrests per 1,000 Drivers` = DIUArrests, `Fatalities /100m Vehicle Miles` =fatalities) %>%\n\n  kable(format = \"html\", align = \"c\") %>%\n  kable_styling(full_width = FALSE, position = \"left\") %>%\n  row_spec(min(state_row, 6), color = \"lightgray\", background = \"darkblue\", bold = TRUE)\n\n\n\n\n \n  \n    Rank \n    State \n    % Insured Drivers \n    DUI Arrests per 1,000 Drivers \n    Fatalities /100m Vehicle Miles \n  \n \n\n  \n    1 \n    South Carolina \n    90.6% \n    3.95 \n    1.83 \n  \n  \n    2 \n    Mississippi \n    76.3% \n    2.61 \n    1.63 \n  \n  \n    3 \n    Arizona \n    88.0% \n    3.63 \n    1.53 \n  \n  \n    4 \n    Louisiana \n    78.0% \n    1.65 \n    1.53 \n  \n  \n    5 \n    West Virginia \n    89.9% \n    2.76 \n    1.51 \n  \n  \n    13 \n    Oregon \n    87.3% \n    4.68 \n    1.37 \n  \n\n\n\n\n\n\n\nBehind the Scenes\ntitler <- function(title) {\n  textGrob(title, \n           x = unit(0, \"npc\"), \n           hjust = 0, \n           vjust = 0.5,\n           gp = gpar(fontsize = 40, fontfamily = \"josefin-slab\"))\n}\n\nsourcer <- function(source) {\n  grobTree(\n    textGrob(\"Source: \", \n             name = \"source1\",\n             x = unit(0, \"npc\"), \n             hjust = 0, \n             vjust = 0,\n             gp = gpar(fontsize = 15, fontfamily = \"josefin-slab\", fontface = \"bold\")),\n    textGrob(source, \n             x = unit(0, \"npc\") + grobWidth(\"source1\"), \n             hjust = 0, \n             vjust = 0,\n             gp = gpar(fontsize = 15, fontfamily = \"josefin-slab\"))\n  )\n}\n\ncaption <- grobTree(\n  gp = gpar(fontsize = 15), \n  textGrob(label = \"Karol Orozco\", \n           name = \"caption1\",\n           x = unit(0, \"npc\"),  \n           y = unit(0, \"npc\"),\n           hjust = 0, \n           vjust = 1)\n)\n\nplotr <- function(plot, title = NULL, subtitle = NULL, \n                  source = \"World Population Report\",\n                  endnote = \"\") {\n  grid.arrange(titler(title), \n              plot, \n              caption, \n              sourcer(source),\n              heights = c(1.5, 4, 1, 1)) \n\n}\n\n\nplot <- worst_drivers%>%\n select(state, `DUI Arrests per 1,000 Drivers` = DIUArrests, `Fatalities /100m Vehicle Miles` =fatalities)%>%\n  gather(key = \"key\", value = \"value\", -state) %>%\n  filter(state == params$state) %>%\n  ggplot(aes(key, value)) +\n  geom_bar(stat = \"identity\", fill= \"darkblue\")+\n  scale_y_continuous(expand = c(0, 0), limits = c(0, 5)) +\n  labs(x= \"\", y =\" \")\n\nplotr(plot= plot,\n      title = paste0(\"Driver Behaviors in \", params$state),\n      source = \"World Population Review\")"
  },
  {
    "objectID": "social/FreedomintheWorld/Freedom.html",
    "href": "social/FreedomintheWorld/Freedom.html",
    "title": "Freedom in the World",
    "section": "",
    "text": "The flagship publication of Freedom House, Freedom in the World, is the benchmark for comparing assessments of civil liberties and political rights worldwide. Policymakers, the media, multinational businesses, community activists, and human rights advocates use the survey ratings and narrative reports on 195 countries and 15 connected and disputed territories published annually since 1972.\nThis data visualization explores the Political rights of African countries, where the scores range from 1 to 7, where 7 means more violations of political rights. Note that in 1972, South Africa was rated as “White” (2,3 Free) and “Black” (5,6 Not Free).\n\n\nBehind the Scenes\n# plot \nggplot(plot_data, aes(x = year, y = PR.x, fill = as.character(PR.y))) +\n  \n  geom_area() +\n  \n  facet_geo(~ country, grid = africa_countries_grid1, label = \"code\") +\n  \n  scale_x_continuous(limits = c(1995, 2020), breaks = c(2000, 2020)) +\n  \n  scale_y_continuous(limits = c(0, 10), breaks = c(0, 5, 10)) +\n  \n  coord_cartesian(expand = F) +\n  \n  labs(title = \"Freedom in the World: Political Rights, African Countries\", \n       subtitle = str_wrap_break(\"\", 70), \n       x = \"\", \n       y = \"\") +\n  \n   scale_fill_viridis_d(\"2020 Political\\nRights Index\") +\n  \n  guides(fill=guide_legend(ncol=4)) +\n  \n  theme_light() +\n   theme(\n     plot.background = element_rect(fill = \"darkgray\", colour = \"darkgray\"),\n        panel.background = element_rect(fill = \"darkgray\", colour = \"darkgray\"), \n     \n        plot.title = element_text(colour = \"black\", size=25, face = \"bold\", hjust = 0.5, family=\"josefin-slab\"), \n  \n        plot.caption = element_text(colour = \"#fafafa\", size=10, hjust = 0.5, family=\"josefin-slab\", \n                                     margin = margin(5, 0, 5, 0)), \n        strip.text = element_text(colour = \"black\", size=12, face= \"bold\",hjust = 0.5, family=\"josefin-slab\"), \n     \n        strip.background = element_rect(fill = \"darkgray\", colour =\"darkgray\"), \n        plot.margin = unit(c(0.3, 0.9, 0.3, 0.3), \"cm\"), \n        legend.background = element_rect(fill = \"darkgray\", colour = \"darkgray\"), \n        legend.key = element_rect(fill = \"darkgray\", colour = \"darkgray\"),\n        legend.position = c(0.85, 0.1),\n     \n        legend.text = element_text(colour = \"black\", size=9, hjust = 0.5, family=\"josefin-slab\"), \n        legend.title = element_text(colour = \"black\", size=10, hjust = 0.5, family=\"josefin-slab\"))"
  },
  {
    "objectID": "social/Hurricanes Graph/hurricanes.html",
    "href": "social/Hurricanes Graph/hurricanes.html",
    "title": "Hurricanes",
    "section": "",
    "text": "Hello there,\nToday, I will be recreating the graph from the article “Why Past Hurricane Seasons Don’t Tell Us Much About The Future” by Anna Wiederkehr.\nYou can find the raw data in the National Oceanic and Atmospheric Administration’s HURDAT2 database, which has records of all Atlantic basin tropical cyclones dating back to 1851.\nThis exercise focuses on practicing our data visualization skills with ggplot2, so I’ll skip the data manipulation steps. You can find the dataset for the graph in my GitHub repository.\n\n\nBehind the Scenes\n## library(ggplot2)\n## library(geomtextpath)\n## library(RCurl)\n## library(grid)\n## library(gridExtra)\n\nx <- getURL(\"https://raw.githubusercontent.com/karolo89/Raw_Data/main/Hurricane.csv\")\nhurricane <- read.csv(text = x)\n\ngraph1 <- ggplot(hurricane, aes(year,average, color = type)) +\n  geom_step(size=.7) +\n  \n## Adding the text\n  geom_textline(aes(label = ifelse(type == \"avg_h_15year\", \n                    \"All Hurricanes\",\n                    \"Major Hurricanes\"), \n                    y = average + .8), \n                text_smoothing = 50, \n                fontface = 2,\n                hjust = 0,  \n                linetype = 0, \n                size = 3) +\n  annotate(\"text\", x = 1886, y = 2.2, label = \"Category 3 - 5\", hjust = 0,\n           color = \"#3b2d74\", size= 2.5) +\n\n## Scales\n  \n  scale_x_continuous(breaks = seq(1860,2020,20)) +\n  scale_y_continuous(limits = c(0,8.5),\n                     breaks = seq(2,8,2),\n                     expand = c(0,0)) +\n  scale_color_manual(values = c(\"#735ad2\", \"#3b2d74\")) +\n\n## Labs\n  \n  labs(\n    y = \" \",\n    x = \" \", \n    title = \"15-year average recorded Atlantic basin hurricanes, 1851-2019\") +\n  \n## Theme \n  \n  theme_minimal() +\n  \n  theme(\n    \n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    \n    ## Customize x axis\n    axis.ticks.x = element_line(colour = \"#e1e1e1\"),\n    axis.ticks.length.x =unit(0.3,\"cm\"),\n    axis.line.x.bottom = element_line(colour = \"#313131\"),\n    \n    ## axis text\n\n    axis.text = element_text(size= 6, color= \"#8f8f8f\", \n                             face = \"bold\"),\n    \n    ## We don't want a legend\n    \n    legend.position = \"none\",\n    \n    ## title\n    plot.title = element_text(size =8, face = \"bold\", colour= \"#545454\",\n                              hjust = 0.50, vjust = 1)\n  )\ngraph1\n\n\n\n\n\nI used the Image Color Picker tool to get the hex colors from the original graph.\nWe are still missing the footer, so let’s add it.\n\n\nBehind the Scenes\nfooter<- grobTree( \n                  textGrob(\"      FiveThirtyEight\", x=unit(.05, \"npc\"), \n                      gp=gpar(col=\"#868686\", \n                              family=\"sans\", \n                              fontsize= 5.5,\n                              fontface= \"bold\"),\n                      hjust=0,\n                      vjust = -2),\n                  \n                  textGrob(\"SOURCES: HURDAT2, VECCHI ET AL      \",\n                        x=unit(1, \"npc\"), \n                        gp=gpar(col=\"#868686\", \n                                family= \"sans\",\n                                fontsize=5.5,\n                              fontface= \"bold\"),\n                        hjust=1,\n                      vjust = -2))\n                  \n# Plot All Together\n\n plt.final <- grid.arrange(graph1, footer, heights=unit(c(0.72, 0.03), \n                                                    c(\"npc\", \"npc\")))"
  },
  {
    "objectID": "social/RegressionModel/RegressionModel.html",
    "href": "social/RegressionModel/RegressionModel.html",
    "title": "House Prices in Portland, OR",
    "section": "",
    "text": "The goal is to build a classification model to predict the type of median housing prices in Portland, OR and its metropolitan area.\n\n\nBehind the Scenes\nclean_data %>% \n  count(price_category, \n        name =\"total\") %>%\n  mutate(percent = total/sum(total)*100,\n         percent = round(percent, 2)) %>%\n gt() %>%\n  tab_header(\n    title = \"Portland, OR and its Metropolitan Area Median House Prices\",\n    subtitle = \"Above and below 551,000$\"\n  ) %>%\n  cols_label(\n    price_category = \"Price\",\n    total = \"Total\",\n    percent = \"Percent\"\n  ) %>% \n  fmt_number(\n    columns = vars(total),\n    suffixing = TRUE\n  )  \n\n\n\n\n\n\n  \n    \n      Portland, OR and its Metropolitan Area Median House Prices\n    \n    \n      Above and below 551,000$\n    \n  \n  \n    \n      Price\n      Total\n      Percent\n    \n  \n  \n    above\n7.25K\n50.02\n    below\n7.24K\n49.98\n  \n  \n  \n\n\n\n\n\n\nBehind the Scenes\nqmplot(x = longitude, \n       y = latitude, \n       data = clean_data, \n       geom = \"point\", \n       color = price_category, \n       alpha = 0.4) +\n  scale_alpha(guide = 'none')\n\n\n\n\n\nData Splitting\n\n\nBehind the Scenes\n# Fix the random numbers by setting the seed \n# This enables the analysis to be reproducible \nset.seed(504)\n\n# Put 3/4 of the data into the training set \ndata_split <- initial_split(houses_pdx, \n                           prop = 3/4)\n\n# Create dataframes for the two sets:\ntrain_data <- training(data_split) \ntest_data <- testing(data_split)\n\n\nValidaton Set\n\n\nBehind the Scenes\nhouse_folds <-\n vfold_cv(train_data, \n          v = 5, \n          strata = price_category) \n\n\n\n\nBehind the Scenes\npdx_rec <-\n  recipe(price_category ~ .,\n         data = train_data) %>%\n  update_role(longitude, latitude, \n              new_role = \"ID\") %>% \n  \n  step_naomit(everything(), skip = TRUE) %>% \n  \n  step_novel(all_nominal(), -all_outcomes()) %>% # converts all nominal variables to factors and takes care of other issues related to categorical variables.\n  \n  step_normalize(all_numeric(), -all_outcomes(), \n                 -longitude, -latitude) %>% # step_normalize() normalizes (center and scales) the numeric variables to have a standard deviation of one and a mean of zero\n  \n  step_dummy(all_nominal(), -all_outcomes()) %>% #converts our factor columns into numeric binary (0 and 1) variables.\n  \n  step_zv(all_numeric(), -all_outcomes()) %>% ## step_zv(): removes any numeric variables that have zero variance.\n  \n  step_corr(all_predictors(), threshold = 0.7, method = \"spearman\") # step_corr(): will remove predictor variables that have large correlations with other predictor variables.\n\n\n\n\nBehind the Scenes\nprep_data <- \n  pdx_rec %>% # use the recipe object\n  prep() %>% # perform the recipe on training data\n  juice() # extract only the preprocessed dataframe"
  },
  {
    "objectID": "social/RegressionModel/RegressionModel.html#the-model--logistic-regression",
    "href": "social/RegressionModel/RegressionModel.html#the-model--logistic-regression",
    "title": "House Prices in Portland, OR",
    "section": "The Model- Logistic regression",
    "text": "The Model- Logistic regression\n\n\nBehind the Scenes\nlog_spec <- # your model specification\n  logistic_reg() %>%  # model type\n  set_engine(engine = \"glm\") %>%  # model engine\n  set_mode(\"classification\") # model mode\n\n# Show your model specification\nlog_spec\n\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\nBehind the Scenes\npdx_wflow <- # new workflow object\n workflow() %>% # use workflow function\n add_recipe(pdx_rec) %>%   # use the new recipe\n add_model(log_spec)   # add your model spec\n\npdx_wflow\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_naomit()\n• step_novel()\n• step_normalize()\n• step_dummy()\n• step_zv()\n• step_corr()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\nBehind the Scenes\n# save model coefficients for a fitted model object from a workflow\n\nget_model <- function(x) {\n  pull_workflow_fit(x) %>% tidy()\n}\n\n# same as before with one exception\nlog_res_2 <- \n  pdx_wflow %>% \n  fit_resamples(\n    resamples = house_folds, \n    metrics = metric_set(\n      recall, precision, f_meas, \n      accuracy, kap,\n      roc_auc, sens, spec),\n      control = control_resamples(\n      save_pred = TRUE,\n      extract = get_model) # use extract and our new function\n    ) \n\n\n## All of the results can be flattened and collected using:\n\n\n\nall_coef <- map_dfr(log_res_2$.extracts, ~ .x[[1]][[1]])\nfilter(all_coef, term == \"bedrooms\")\n\n\n# A tibble: 5 × 5\n  term     estimate std.error statistic  p.value\n  <chr>       <dbl>     <dbl>     <dbl>    <dbl>\n1 bedrooms   0.0748    0.0424      1.76 0.0777  \n2 bedrooms   0.129     0.0433      2.99 0.00284 \n3 bedrooms   0.116     0.0423      2.74 0.00623 \n4 bedrooms   0.118     0.0425      2.78 0.00550 \n5 bedrooms   0.147     0.0430      3.42 0.000618\n\n\n\nPerformance metrics\nShow performance for every single fold:\n\n\nBehind the Scenes\nlog_res_2 %>%  collect_metrics(summarize = FALSE)\n\n\n# A tibble: 40 × 5\n   id    .metric   .estimator .estimate .config             \n   <chr> <chr>     <chr>          <dbl> <chr>               \n 1 Fold1 recall    binary         0.788 Preprocessor1_Model1\n 2 Fold1 precision binary         0.834 Preprocessor1_Model1\n 3 Fold1 f_meas    binary         0.810 Preprocessor1_Model1\n 4 Fold1 accuracy  binary         0.816 Preprocessor1_Model1\n 5 Fold1 kap       binary         0.631 Preprocessor1_Model1\n 6 Fold1 sens      binary         0.788 Preprocessor1_Model1\n 7 Fold1 spec      binary         0.843 Preprocessor1_Model1\n 8 Fold1 roc_auc   binary         0.902 Preprocessor1_Model1\n 9 Fold2 recall    binary         0.783 Preprocessor1_Model1\n10 Fold2 precision binary         0.824 Preprocessor1_Model1\n# … with 30 more rows\n\n\n\n\nCollect predictions\nTo obtain the actual model predictions, we use the function collect_predictions and save the result as log_pred:\n\n\nBehind the Scenes\nlog_pred <- \n  log_res_2 %>%\n  collect_predictions()\n\n\nlog_pred %>% \n  conf_mat(price_category, .pred_class) \n\n\n          Truth\nPrediction above below\n     above  4344   856\n     below  1093  4573\n\n\n\n\nBehind the Scenes\nlog_pred %>% \n  conf_mat(price_category, .pred_class) %>% \n  autoplot(type = \"heatmap\")+\n  theme_minimal()\n\n\n\n\n\n\n\nROC Curve\n\n\nBehind the Scenes\nlog_pred %>% \n  group_by(id) %>% # id contains our folds\n  roc_curve(price_category, .pred_above) %>% \n  autoplot()+\n  theme_minimal()"
  },
  {
    "objectID": "social/RegressionModel/RegressionModel.html#use-the-workflow-to-train-our-model",
    "href": "social/RegressionModel/RegressionModel.html#use-the-workflow-to-train-our-model",
    "title": "House Prices in Portland, OR",
    "section": "Use the workflow to train our model",
    "text": "Use the workflow to train our model\n\n\nBehind the Scenes\npdx_fit <- fit(pdx_wflow, train_data)\n\n\nThis allows us to use the model trained by this workflow to predict labels for our test set, and compare the performance metrics with the basic model we created previously.\n\n\nBehind the Scenes\npdx_fit %>% ## display results\npull_workflow_fit() %>%\ntidy()%>%\n  filter(p.value < 0.05)\n\n\n# A tibble: 8 × 5\n  term                 estimate std.error statistic  p.value\n  <chr>                   <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)           -0.426     0.0296    -14.4  5.43e-47\n2 yearBuilt              0.221     0.0482      4.59 4.49e- 6\n3 bedrooms               0.119     0.0381      3.12 1.82e- 3\n4 livingArea            -2.88      0.0689    -41.8  0       \n5 lotSize               -0.0448    0.0202     -2.21 2.68e- 2\n6 MiddleSchooldistance  -0.171     0.0310     -5.52 3.33e- 8\n7 HighSchooldistance     0.0805    0.0303      2.66 7.86e- 3\n8 homeType_TOWNHOUSE     0.998     0.179       5.59 2.24e- 8\n\n\n\n\nBehind the Scenes\nlibrary(vip)\n\npdx_fit %>%\nextract_fit_parsnip() %>%\n   vip(num_features = 5)+\n  theme_minimal()\n\n\n\n\n\nThe two most important predictors in whether the median house value is above or below 551,000 dollars were the Living Area and the home type: Townhouse\n\n\nBehind the Scenes\n# Make predictions on the test set\npred_results <- test_data %>% \n  select(price_category) %>% \n  bind_cols(pdx_fit %>% \n              predict(new_data = test_data)) %>% \n  bind_cols(pdx_fit %>% \n              predict(new_data = test_data, type = \"prob\"))\n\n# Print the results\npred_results %>% \n  slice_head(n = 10)\n\n\n   price_category .pred_class .pred_above .pred_below\n1           below       above  0.79554296   0.2044570\n2           below       below  0.26469350   0.7353065\n3           below       below  0.47600771   0.5239923\n4           below       below  0.02353063   0.9764694\n5           below       below  0.06272184   0.9372782\n6           below       below  0.10491967   0.8950803\n7           below       below  0.06923625   0.9307637\n8           below       below  0.03314398   0.9668560\n9           below       below  0.04580355   0.9541965\n10          below       above  0.83998089   0.1600191\n\n\nLet’s take a look at the confusion matrix:\n\n\nBehind the Scenes\npred_results%>% \n  conf_mat(price_category, .pred_class) %>% \n  autoplot(type = \"heatmap\")+\n  theme_minimal()"
  },
  {
    "objectID": "social/RegressionModel/RegressionModel.html#reference",
    "href": "social/RegressionModel/RegressionModel.html#reference",
    "title": "House Prices in Portland, OR",
    "section": "Reference",
    "text": "Reference\nTidymodels- https://www.tidymodels.org/"
  },
  {
    "objectID": "social/transit/transit.html",
    "href": "social/transit/transit.html",
    "title": "Transit Costs Project",
    "section": "",
    "text": "Politicians in the twenty-first century frequently advocate for establishing a massive public works program in the United States to create the necessary infrastructure. The Biden administration’s Bipartisan Infrastructure Bill (BIL) turned this excitement for infrastructure into law after a few tentative steps in the Obama administration and the Trump Administration’s persistent attempts at Infrastructure Week. BIL proposes spending nearly $1 trillion between fiscal years 2022 and 2026, of which more than $500 billion will be allocated to transportation, including $66 billion for mainline rail and $39 billion for other public transit (National Association of Counties, 2022); when the customary five-year appropriations are taken into account, non-mainline public transit is expected to receive between $91 and $108 billion (FTA n.d.).\nSpending money wisely is essential when such significant quantities are at stake. Governments invest in infrastructure, transportation, and public transit because they see them as opportunities to improve connectivity, spur economic activity, renovate aging infrastructure, cut emissions, produce high-paying jobs, and spark innovation rather than just handing out money to people as welfare (The White House 2022). Regrettably, as we will demonstrate in the graph below, the US transit infrastructure cost is among the highest in the world.\n\n\nBehind the Scenes\n#maximum cost per km for each country\ndf <- raw_df %>% group_by(country) %>% top_n(1, cost_km_millions) \n\n#obtain country names\ndf$end_year <- as.numeric(df$end_year) \ndf$country_name <- countrycode(df$country, \"ecb\", \"country.name\")\ndf$country_name[which(df$country == \"UK\")] <- \"United Kingdom\"\n\n#remove NA and get distinct names\ndf <- distinct(df)\ndf <- df %>% drop_na(c(\"country\", \"start_year\", \"end_year\", \"cost_km_millions\"))\n\n#plot\np <- ggplot(df, aes(colour=cost_km_millions)) +\n  #plot project duration\n  geom_segment(aes(x=as.numeric(start_year), xend=as.numeric(end_year), y=country_name, yend=country_name), size=4.5) +\n  #colour by cost\n  scale_colour_gradient2(low = \"lightgray\",\n                        mid = \"#00008B\",\n    high = \"#800080\",\n    midpoint = 2000,\n    space = \"Lab\",\n    guide = \"colourbar\",\n    aesthetics = \"colour\"\n  ) + \n  #order countries \n  scale_y_discrete(limits = factor(df$country_name[order(df$start_year)], levels=df$country_name[order(df$start_year)])) +\n  scale_x_continuous(breaks=seq(min(df$start_year), max(df$end_year), by = 3)) +\n  #note current year\n  geom_segment(aes(x = 2021, y = df$country_name[which.min(df$start_year)], xend = 2021, yend = df$country_name[which.max(df$start_year)]), colour = \"#70284a\", linetype=\"dashed\", size=1.5) +\n  \n  #visualisation\n  theme_classic() +\n  \n  theme(plot.background = element_rect(fill = \"#e6e6fa\"),\n        panel.background = element_rect(fill = \"#e6e6fa\"),\n        legend.background = element_rect(fill =\"#e6e6fa\"),\n        plot.title = element_text(colour = \"black\", size=18, face=\"bold\", hjust = 0.5),\n        plot.subtitle = element_text(colour = \"black\", size=12, hjust = 0.5),\n        axis.text.x= element_text(colour=\"black\", size=10),\n        axis.text.y= element_text(colour=\"black\", size=10)) +\n  xlab(\"\") + ylab(\"\") +  labs(title = \"Transit-infrastructure Costs\", \n                              subtitle = \"Construction timelines for the most expensive transit-infrastructure \\nprojects in each country\",\n                              colour = \"US Dollars \\n(millions)/km\") \n\np\n\n\n\n\n\nThe first completed Case Study can be found on Boston’s Green Line, although there is data from around the world! The raw data is available here."
  },
  {
    "objectID": "social/vote/vote.html",
    "href": "social/vote/vote.html",
    "title": "Voter Registrations Are Way, Way Down During The Pandemic",
    "section": "",
    "text": "Let’s practice our visualization skills with ggplot2 and recreate the graph from the Voter Registrations Are Way, Way Down During The Pandemic article.\n\n\nBehind the Scenes\nggplot(vreg, aes(x= Month, y= change, fill = Color))+\ngeom_col()+ \ngeom_hline( yintercept = 0, color= \"black\")+\n\ngeom_rect(data = data.frame(Jurisdiction = \"Arizona\"), \n              aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n              color = \"lightgrey\", \n              fill = \"white\", \n              alpha = 0, \n              linetype = \"dotted\", \n              inherit.aes = FALSE)+\n  geom_rect(data = data.frame(Jurisdiction = \"California\"), \n            aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n            color = \"lightgrey\", \n            fill = \"white\", \n            alpha = 0, \n            linetype = \"dotted\", \n            inherit.aes = FALSE)+\n   geom_rect(data = data.frame(Jurisdiction = \"Colorado\"), \n             aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n             color = \"lightgrey\", \n             fill = \"white\", \n             alpha = 0, \n             linetype = \"dotted\", \n             inherit.aes = FALSE)+\n  geom_rect(data = data.frame(Jurisdiction = \"Delaware\"), \n            aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n            color = \"lightgrey\", fill = \"white\", \n            alpha = 0, \n            linetype = \"dotted\", \n            inherit.aes = FALSE)+\n  geom_rect(data = data.frame(Jurisdiction = \"Florida\"), \n            aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n            color = \"lightgrey\", \n            fill = \"white\", \n            alpha = 0, \n            linetype = \"dotted\", inherit.aes = FALSE)+\n  geom_rect(data = data.frame(Jurisdiction = \"Georgia\"), \n            aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n            color = \"lightgrey\", \n            fill = \"white\", \n            alpha = 0, \n            linetype = \"dotted\", \n            inherit.aes = FALSE)+\n  geom_rect(data = data.frame(Jurisdiction = \"Illinois\"), \n            aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n            color = \"lightgrey\", \n            fill = \"white\", \n            alpha = 0, \n            linetype = \"dotted\", \n            inherit.aes = FALSE)+\n  \n  facet_wrap(~Jurisdiction, scales = \"free_y\")+\n\n  scale_fill_identity(guide= FALSE)+\n  scale_x_discrete(limits=c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\"), \n                   breaks=c(\"Jan\",\"May\"))+\n  scale_y_continuous(labels = label_number_si(a =! 0), n.breaks = 4)+\n\n  labs(\n        x=\"\",\n        y=\"\",\n      title = \"Voter registration dropped dramatically during the pandemic\",\n      subtitle = \"Difference in the number of newly registered voters for each month in 2020 compared to the same month in 2016\",\n      caption = \"Some states treat voters who move between counties within a state as new registrants because they're unregistered from their old county and nearly registered in the new ones.          \",\n      \n      tag= \"FiveThirtyEight\") +\n\n    theme_classic()+\n    theme(\n          axis.line.y=element_blank(),\n          axis.line.x = element_blank(),\n          axis.ticks = element_blank(),\n          axis.text.y = element_text(size = 6.5, color = \"gray\"), \n          axis.text.x = element_text(size= 6.5, color = \"gray\"),\n          \n          \n          plot.title = element_text(size =9, face = \"bold\", hjust = 0.55),\n          plot.title.position = \"plot\",\n          plot.subtitle= element_text(size = 8, hjust = 0.55),\n          \n          plot.caption = element_text(hjust = 0, size = 6, color = \"grey50\",\n                                      margin = margin(r=5)),\n          plot.background = element_rect(fill= \"white\"),\n          plot.tag.position = \"bottom\",\n          plot.tag = element_text(size= 5, color = \"gray\", hjust =0.1, \n                                  margin=margin(t=1, \n                                                r=5, \n                                                b=1, \n                                                l=20, \n                                                unit=\"pt\")),\n          panel.grid.minor.y = element_blank(),\n          panel.grid.major.y = element_line(size= 0.1, color= \"lightgrey\",\n                                            linetype= \"solid\"),\n          panel.background = element_rect(fill = \"white\"),\n          panel.border = element_blank(),\n          panel.spacing = unit(1, \"lines\"),\n          \n          strip.background= element_rect(fill= \"white\", linetype = \"blank\"),\n          strip.text = element_text(color= \"black\", face= \"bold\"),\n          strip.text.x = element_text(face = \"bold\", size= 7),\n\n          \n          legend.title = element_blank(),\n          legend.position = \"none\"\n    \n)"
  },
  {
    "objectID": "Spanish_index.html",
    "href": "Spanish_index.html",
    "title": "Karol Orozco E.",
    "section": "",
    "text": "Willamette University | Portland, OR\nMaestría en Ciencia de Datos | Agosto 2023\nBeca Presidencial; seleccionado en base a los logros profesionales\n\nWillamette University | Portland, OR\nMaestría en Administración de Empresas | Agosto 2022\nBeca sin fines de lucro George y Colleen Hoyt; seleccionado en base a los logros profesionales\nHonores Atkinson\nBeta Gamma Sigma: La Sociedad de Honor de Negocios Internacionales\n\nUniversidad Autonóma del Caribe | Barranquilla, Colombia\nLicenciatura, Ingeniería Industrial | Julio 2014\n\n\n\n  \n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Github\n  \n  \n    \n     Email"
  },
  {
    "objectID": "viz/Hollywood Age Gaps/AgeGaps.html",
    "href": "viz/Hollywood Age Gaps/AgeGaps.html",
    "title": "Hollywood Age Gaps",
    "section": "",
    "text": "Behind the Scenes\np <- age_gaps %>% \n  select(release_year, age_difference) %>%\n  ggplot(aes(x = release_year,  y = age_difference)) +\n  geom_point(colour = alpha(\"#FF9A00\", 0.5)) +\n  geom_smooth(colour = \"white\", se = FALSE) +\n  labs(y = \"Age gap (years)\",\n       x= \"Release Year\",\n       title =  \"Age Gap in Hollywood Movies from 1935 to 2022\",\n       caption=\"Data from TidyTuesday | Chart by @Karol_Orozco\") +\n  theme_minimal()+\n  theme(text=element_text(family = \"chivo\", color=\"white\"), \n        panel.grid.minor = element_blank(),\n        panel.grid.major = element_line(color=\"black\", size=0.1),\n        axis.title=element_text(face=\"bold\", size= 15),\n        axis.text =element_text(color=\"white\"),\n        axis.title.y=element_text(margin=margin(r=10)),\n        plot.title = element_text(size = 20, face = \"bold\", \n                                  hjust = 0.5, vjust = 0.5),\n        plot.background = element_rect(fill=\"black\"),\n        plot.margin  = margin(20,20,10,10)) \n\n\np"
  },
  {
    "objectID": "viz/Hollywood Age Gaps/AgeGaps.html#reference",
    "href": "viz/Hollywood Age Gaps/AgeGaps.html#reference",
    "title": "Hollywood Age Gaps",
    "section": "Reference",
    "text": "Reference\nThe data this week comes from Hollywood Age Gap via Data Is Plural.\nThomas Mock (2022). Tidy Tuesday: A weekly data project aimed at the R ecosystem.GitHub"
  },
  {
    "objectID": "viz/Interactive Viz/Interactive Instacart.html",
    "href": "viz/Interactive Viz/Interactive Instacart.html",
    "title": "Interactive Graphics- Instacart",
    "section": "",
    "text": "Behind the Scenes\nlibrary(gganimate) \n\nday_week <- orders %>%\n    mutate(day = as.factor(order_dow)) %>%\n    mutate(hour = as.factor(order_hour_of_day)) %>%\n    group_by(day,hour) %>%\n    dplyr::summarise(count = n()) %>%\n    arrange(desc(count))\n\n\nday_weekp <-day_week %>%\n    ggplot(aes(x=day, y=hour))+\n    geom_tile(aes(fill=count), colour = \"white\") + \n  \n    scale_fill_gradient(name= \"Number of\\nOrders\", low = \"#fff1e6\",high = \"#00835C\")+\n  \n    scale_x_discrete( position = \"top\",\n                    breaks = c(\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"),\n                    label = c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\",\"Thursday\", \"Friday\", \"Saturday\"),\n                    expand=c(0,0))+\n  \n   scale_y_discrete( \n                    breaks = c(\"0\", \"6\", \"12\", \"18\", \"23\"),\n                    label = c(\"12am\", \"6am\", \"12pm\", \"6pm\", \"11pm\"),\n                    expand=c(0,0))+\n  \n      labs(title=\"Which Day and What Time\\nDo Customers Order the Most?\",\n         x=\"\", \n         y=\"\",\n         caption = \"Maximum number of orders are placed between 9:00am and 4:00pm on Sunday and Monday. There is also a big number of orders during Firday\\nand Saturday.\")+\n  \n    theme_classic()+\n  \n    theme(\n      \n    axis.line=element_blank(),                                               \n    axis.ticks=element_line(size=0.4),\n    axis.text = element_text(size= 10, color= \"#00835C\"),\n    axis.line.x = element_line(color= \"#00835C\" ),\n    \n    plot.background=element_blank(),         \n    plot.title = element_text(size =10, face = \"bold\", hjust = 0.50, vjust = 1, colour = \"darkgreen\"),\n    plot.caption = element_text(hjust = 0, size = 7, margin = unit(c(0.5, 0.5, 0.5, 0.5), \"cm\"), color = \"#718c9e\"),\n\n    \n    panel.grid = element_blank(),\n    \n    legend.position = \"bottom\",\n    legend.title = element_text(size= 8),\n    legend.margin=margin(grid::unit(0,\"cm\")),\n    legend.key.width=grid::unit(2,\"cm\"),\n    legend.key.height=grid::unit(0.2,\"cm\")\n)\n\nday_weekp+ transition_manual(day, cumulative = TRUE)"
  },
  {
    "objectID": "viz/Interactive Viz/Interactive Instacart.html#plotly",
    "href": "viz/Interactive Viz/Interactive Instacart.html#plotly",
    "title": "Interactive Graphics- Instacart",
    "section": "Plotly",
    "text": "Plotly\n\n\nBehind the Scenes\nlibrary(plotly)\n\ntype <- data   %>%\n    group_by(product_id)%>% \n    dplyr::summarize(count = n()) %>% \n    top_n(20, wt = count) %>%\n    left_join(select(products, product_id, product_name, category), by=\"product_id\") %>%\n    arrange(desc(count))\n\n\nbest <- type %>% \n\nggplot(aes(x=reorder(product_name,count), y=count, color= category, text= paste0(product_name, \", Total Orders:\", count)))+    \n  \n  geom_point(size= 2)+\n    geom_segment(aes(x=reorder(product_name,count), \n                     xend=reorder(product_name,count), \n                     y=0, \n                     yend=count), size=0.8)+\n  \n    scale_y_continuous(labels = scales::comma) +\n  \n      labs(title=\"Bestsellers Products\",\n           subtitle = \"Organic vs Non-Organic\",\n           y=\"\", \n           x=\"\", \n           legend = \"\")+\n  \n    scale_color_manual(\"\", values = c(\"#FF8200\", \"#0AAD0A\"))+\n\n  \n    theme_minimal()+\n  \n    theme(\n       axis.text.x= element_text( size= 7),\n       axis.text.y= element_text( size= 8),\n\n       plot.title = element_text(hjust=0.5, size= 12, face = \"bold\", vjust = 0.5),\n       plot.subtitle = element_text(hjust=0.5, size= 9, vjust = 0.5),\n       \n       panel.grid.major.x = element_blank(),\n       panel.grid.major.y = element_blank(),\n       panel.grid.minor.y = element_blank(),\n       panel.grid = element_line(color = \"#e5e5e5\"))+\n  \n  \n  \n     coord_flip()\n\nggplotly(best, tooltip = \"text\")"
  },
  {
    "objectID": "viz/Interactive Viz/Interactive Instacart.html#ggiraph",
    "href": "viz/Interactive Viz/Interactive Instacart.html#ggiraph",
    "title": "Interactive Graphics- Instacart",
    "section": "ggiraph",
    "text": "ggiraph\n\n\nBehind the Scenes\nlibrary(ggiraph)\nlibrary(glue)\n\n\ntooltip_css <- \"background-color:#d8118c;color:white;padding:5px;border-radius:3px;\"\nbg_color <- \"#D7E0DA\"\nfont_color <- \"#1f3225\"\n\n\ndf3 <- data%>%\n  select(product_id, reordered, product_name)%>%\n  group_by(product_id, product_name)%>% \n  dplyr::summarize(proportion_reordered = mean(reordered), n=n())\n\n\ngg_scatter <- ggplot(\n  data = df3,\n  mapping = aes(x=n, y=proportion_reordered,\n                \n    # here we add iteractive aesthetics\n    tooltip = paste0(toupper(product_name), \":\", \n                   n))) +\n  geom_jitter_interactive(\n    size = 3, hover_nearest = TRUE)+\n\ngeom_smooth(aes(tooltip=\"smoothed line\", data_id=\"smooth\"), se= FALSE)+  \n  \n  labs(y = \"Proportion of reorders\",\n       title= \"Association between number of orders and probability of reordering\",\n       caption = \"Products with a high number of orders are naturally more likely to be reordered.\\nHowever, there seems to be a ceiling effect.\")+\n  theme_minimal()+\n  theme(text=element_text(family = \"chivo\", color=\"#1f3225\"), \n        panel.grid.minor = element_blank(),\n        panel.grid.major = element_line(color=\"#1f3225\", size=0.1),\n        axis.title=element_text(face=\"bold\", size= 10),\n        axis.text =element_text(color=\"#1f3225\"),\n        axis.title.y=element_text(margin=margin(r=10)),\n        plot.title = element_text(size = 13, face = \"bold\", \n                                  hjust = 0.5, vjust = 0.5),\n        plot.margin  = margin(20,20,20,20))\n\n## Customizing girafe animations\n\ngirafe(\n  ggobj = gg_scatter,\n  bg = bg_color,\n  options = list(\n    opts_tooltip(css = tooltip_css, opacity = 1),\n    opts_sizing(width = .7),\n    opts_zoom(max = 1),\n    opts_hover(\n      css = girafe_css(\n        css = glue(\"fill:{font_color};\"),\n        text = glue(\"stroke:none;fill:{font_color};fill-opacity:1;\")))))"
  },
  {
    "objectID": "viz/Interactive Viz/Interactive Instacart.html#reference",
    "href": "viz/Interactive Viz/Interactive Instacart.html#reference",
    "title": "Interactive Graphics- Instacart",
    "section": "Reference",
    "text": "Reference\nInstacart-market-basket-analysis by Jeremy Staley, Meg Risdal, sharathrao, Will Cukierski publisher by Kaggle, 2017. https://kaggle.com/competitions/instacart-market-basket-analysis"
  },
  {
    "objectID": "viz/Table/tables.html",
    "href": "viz/Table/tables.html",
    "title": "Tables",
    "section": "",
    "text": "The Palmer Archipelago penguins. Artwork by @allison_horst.\n\n\n\n\nBehind the Scenes\nlibrary(DT)\n\ndatatable(table,\n\nextensions = c('Select', 'Buttons'), options = list(\n    select = list(style = 'os', items = 'row'),\n    dom = 'Blfrtip',\n    rowId = 0,\n    buttons = c('selectAll', 'selectNone', 'selectRows', 'selectColumns', 'selectCells')\n  ),\n  selection = 'none',\n    \n          caption = htmltools::tags$caption(\n    style = 'caption-side: bottom; text-align: center;',\n    'Table 1: ', htmltools::em('Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network.')\n  ))\n\n\n\n\n\n\n\n\n\n\nGreat Resource: Modify the table output options\n\n\nBehind the Scenes\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(downloadthis)\nlibrary(emo)\n\nattach_excel <- table %>%\n  download_this(\n    output_name = \"Penguins\",\n    output_extension = \".xlsx\", # Excel file type\n    button_label = \"Download Excel\",\n    button_type = \"primary\", # change button type\n  )\n\ntable%>%\n  group_by(Penguin)%>%\n  gt(rowname_col = \"Sex\") %>%\n\n  tab_header(\n    title = md(paste0(\n      emo::ji(\"penguin\"), emo::ji(\"penguin\"), emo::ji(\"penguin\"),\n      \"Penguins are fun to summarize!\",\n      emo::ji(\"penguin\"), emo::ji(\"penguin\"), emo::ji(\"penguin\"))),\n  # Use markdown syntax with md()\n    subtitle = md(\"The palmerpenguins: Palmer Archipelago (Antarctica) penguin data\"))%>%\n  \n  \ntab_footnote(\n    footnote = \"Measurements in millimeters (mm)\", \n    locations = cells_column_labels(\n      columns = 4:6 # note\n      ))%>% \n  \ntab_footnote(\n    footnote = \"Measurements in gram (g)\", \n    locations = cells_column_labels(\n      columns = 7:7 # note\n      ))%>% \n  tab_source_note(source_note = \"Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. | Table: Karol Orozco\")%>% \n  \n tab_options(\n    row_group.border.top.width = px(3),\n    row_group.border.top.color = \"gray\",\n    row_group.border.bottom.color = \"gray\",\n    \n    table_body.hlines.color = \"white\",\n    table.border.top.color = \"white\",\n    table.border.top.width = px(3),\n    table.border.bottom.color = \"white\",\n    table.border.bottom.width = px(3),\n    \n    column_labels.border.bottom.color = \"gray\",\n    column_labels.border.bottom.width = px(2),\n    column_labels.background.color = \"black\",\n    \n    heading.background.color = \"#eebf00\",\n    heading.border.bottom.color = \"#eebd00\",\n    heading.border.bottom.width = \"5px\")%>% \n  \n  tab_style(\n    style = cell_text(color = \"white\"),\n    locations = list(\n      cells_row_groups(),\n      cells_column_labels(everything())))%>%\n  \n   tab_style(\n    locations = cells_row_groups(groups = \"Adelie\"),\n    style = cell_fill(color = \"darkorange\"))%>%\n  \n   tab_style(\n    locations = cells_row_groups(groups = \"Chinstrap\"),\n    style = cell_fill(color = \"darkorchid\"))%>%\n  \n   tab_style(\n    locations = cells_row_groups(groups = \"Gentoo\"),\n    style = cell_fill(color = \"cyan4\"))%>%\n\n  \n  tab_source_note(attach_excel)\n\n\n\n\n\n\n  \n    \n      🐧🐧🐧Penguins are fun to summarize!🐧🐧🐧\n    \n    \n      The palmerpenguins: Palmer Archipelago (Antarctica) penguin data\n    \n  \n  \n    \n      \n      Count\n      Culmen_Length1\n      Culmen_Depth1\n      Flipper_Length1\n      Body_Mass2\n    \n  \n  \n    \n      Adelie\n    \n    female\n73\n37.26\n17.62\n187.79\n3368.84\n    male\n73\n40.39\n19.07\n192.41\n4043.49\n    \n      Chinstrap\n    \n    female\n34\n46.57\n17.59\n191.74\n3527.21\n    male\n34\n51.09\n19.25\n199.91\n3938.97\n    \n      Gentoo\n    \n    female\n58\n45.56\n14.24\n212.71\n4679.74\n    male\n61\n49.47\n15.72\n221.54\n5484.84\n  \n  \n    \n      Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. | Table: Karol Orozco\n    \n    \n      \n   Download Excel\n\n    \n  \n  \n    \n      1 Measurements in millimeters (mm)\n    \n    \n      2 Measurements in gram (g)\n    \n  \n\n\n\n\n\n\n\n\nBehind the Scenes\ndf3 <- penguins%>%\n  group_by(species) %>% \n  summarize(across(where(is.numeric), mean, na.rm = TRUE))%>% \n  dplyr::mutate_if(is.numeric, round, 2)  %>%\n  rename(Culmen_Length = bill_length_mm , \n         Culmen_Depth = bill_depth_mm,  \n         Flipper_Length= flipper_length_mm, \n         Body_Mass= body_mass_g,\n         Penguin= species)%>%\n  mutate(Photo = case_when(\n    str_detect(Penguin,'Adelie') ~ \"https://github.com/karolo89/karolo_website/blob/main/viz/Table/adellie.png?raw=true\", \n    str_detect(Penguin,'Chinstrap') ~ \"https://github.com/karolo89/karolo_website/blob/main/viz/Table/chinstrap.png?raw=true\",\n    str_detect(Penguin,'Gentoo') ~ \"https://github.com/karolo89/karolo_website/blob/main/viz/Table/gentoo.png?raw=true\"))%>%\n  \n  select(\"Photo\", \"Penguin\", \"Culmen_Length\", \"Culmen_Depth\", \"Flipper_Length\", \"Body_Mass\")\n\ndf3\n\n\n# A tibble: 3 × 6\n  Photo                                  Penguin Culme…¹ Culme…² Flipp…³ Body_…⁴\n  <chr>                                  <fct>     <dbl>   <dbl>   <dbl>   <dbl>\n1 https://github.com/karolo89/karolo_we… Adelie     38.8    18.4    190.   3706.\n2 https://github.com/karolo89/karolo_we… Chinst…    48.8    18.4    196.   3733.\n3 https://github.com/karolo89/karolo_we… Gentoo     47.6    15      217.   5092.\n# … with abbreviated variable names ¹​Culmen_Length, ²​Culmen_Depth,\n#   ³​Flipper_Length, ⁴​Body_Mass\n\n\n\n\nBehind the Scenes\ndf3 %>%\n  gt()%>%\n  tab_header(\n    title =  \"Penguins are fun to summarize!\")%>%\n  gtExtras::gt_theme_nytimes()%>%\n  # Add flag images\n  gtExtras::gt_img_rows(columns = Photo, height = 20)%>%\n  opt_align_table_header(align = \"center\")%>% \n  tab_source_note(source_note = \"Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. | Table: Karol Orozco\")%>% \n  \n tab_options(\n\n    table_body.hlines.color = \"white\",\n    table.border.top.color = \"white\",\n    table.border.top.width = px(3),\n    table.border.bottom.color = \"white\",\n    table.border.bottom.width = px(3),\n    \n    column_labels.border.bottom.color = \"gray\",\n    column_labels.border.bottom.width = px(2),\n    column_labels.background.color = \"black\",\n    \n    heading.background.color = \"cyan4\",\n    heading.border.bottom.color = \"cyan4\",\n    heading.border.bottom.width = \"5px\")\n\n\n\n\n\n\n  \n    \n      Penguins are fun to summarize!\n    \n    \n  \n  \n    \n      Photo\n      Penguin\n      Culmen_Length\n      Culmen_Depth\n      Flipper_Length\n      Body_Mass\n    \n  \n  \n    \nAdelie\n38.82\n18.35\n190.10\n3706.16\n    \nChinstrap\n48.83\n18.42\n195.82\n3733.09\n    \nGentoo\n47.57\n15.00\n217.24\n5092.44\n  \n  \n    \n      Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. | Table: Karol Orozco\n    \n  \n  \n\n\n\n\n\n\n\n\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/. doi: 10.5281/zenodo.3960218."
  },
  {
    "objectID": "viz/TidyTuesday/PetCats.html",
    "href": "viz/TidyTuesday/PetCats.html",
    "title": "Pet Cats UK",
    "section": "",
    "text": "This dataset is full of exciting columns that provide insight into the secret lives of cats. I must admit that dogs are my favorite pets, but many of my friends love cats, and I always hear stories about how independent their pets are, which made me want to learn more about them.\nWell, I was born in Colombia, and during my time there, I always remember the cats running and fighting on my roof, which would keep me up at night for a few hours. Then, a few weeks later, I heard tiny meows and noticed the presence of some kittens, which made me suspect that it was another kind of fun that was going on there.\nAccording to the American Society for the Prevention of Cruelty to Animals ASPCA, a female cat can have an average of four to six kittens per litter and may have one to two litters per year. Some cat owners prefer to spay or neuter their cats.\nThe TidyTuesday, weekly data project, posted on January 31st, 2023, a dataset from the Movebank for Animal Tracking Data, which includes cats’ characteristics (such as age, sex, neuter status, hunting habits) and time-stamped GPS pings. I’ll take a look at the reproductive status of the cats and improve the graph offered by the Twitter user @OluwafemOyedele.\nYou can find the data on the TidyTuesday Github site- “Pet Cats UK”.\nHere is the list of cat names, in case you need some inspiration."
  },
  {
    "objectID": "viz/TidyTuesday/PetCats.html#reference",
    "href": "viz/TidyTuesday/PetCats.html#reference",
    "title": "Pet Cats UK",
    "section": "Reference",
    "text": "Reference\nKays R, Dunn RR, Parsons AW, Mcdonald B, Perkins T, Powers S, Shell L, McDonald JL, Cole H, Kikillus H, Woods L, Tindle H, Roetman P (2020) The small home ranges and large local ecological impacts of pet cats. Animal Conservation. doi:10.1111/acv.12563\nMcDonald JL, Cole H (2020) Data from: The small home ranges and large local ecological impacts of pet cats [United Kingdom]. Movebank Data Repository. doi:10.5441/001/1.pf315732\nThomas Mock (2022). Tidy Tuesday: A weekly data project aimed at the R ecosystem.GitHub"
  }
]