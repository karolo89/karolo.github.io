[
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "Karol Orozco",
    "section": "",
    "text": "Background\nAs a highly motivated and results-oriented professional, I bring a unique blend of technical expertise, strategic thinking, and interpersonal skills to the table.\nI have always been fascinated by the power of data and its potential to inform business decisions and drive growth. As a result, I earned a masterâ€™s degree in data science and business administration and gained proficiency in programming languages like R, Tableau, and SQL. I have also completed projects that involved data cleaning, exploratory data analysis, and predictive modeling.\nI am committed to promoting diversity and inclusion in all aspects of my life and believe that a diverse and inclusive workplace leads to better results for employees, the company as a whole, and our community.\nMy experience has also taught me to collaborate with cross-functional teams to identify and translate business needs into data-driven solutions. I have an eye for detail, am an expert in data visualization tools and techniques, and have a proven track record of communicating complex technical concepts to non-technical stakeholders.\n\n\nEducation\nWillamette University | Portland, OR\nMaster in Data Science | August 2023\nPresidential Scholarship; selected based on professional achievements\n\nWillamette University | Portland, OR\nMaster of Business Administration | August 2022\nGeorge and Colleen Hoyt Not-For-Profit Scholarship; selected based on professional achievements\nAtkinson Honor\nBeta Gamma Sigma: The International Business Honor Society\n\nUniversidad Autonoma del Caribe | Barranquilla, Colombia\nBachelorâ€™s Degree, Industrial Engineering | July 2014\n\n\n\nProfessional Skills\nData Analysis | Team Development | Partnership Management | Business Development | Diversity & Inclusion | Strategic Planning | Process Improvement | Project Management | People & Change\n\n\nDomain Knowledge\nR | Tableau | SQL | Community Affairs | Strategic Management | Leadership | Community Engagement | Latinx Community | Shared Value\n\n\nLanguages\n\nI have a deep understanding of the cultural nuances of both English and Spanish-speaking communities. I am able to navigate cultural differences with sensitivity and respect.\nI have basic verbal and written communication skills in Portuguese. I am able to introduce myself, ask and answer basic questions, and engage in simple conversations. I am also able to write simple sentences and paragraphs in Portuguese."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello! Itâ€™s nice to meet you",
    "section": "",
    "text": "About Me\n\n\n English EspaÃ±ol\n\n\nI help to create and sustain communities, programs, and solutions that enhance the use of responsible data science and business practices for equitable social good.\nMy name is Karol Orozco; I live in Portland, OR, with my husband, two kids, and my dog Lucy. I am originally from Barranquilla, Colombia, a city located in the northern region of Colombia on the Caribbean coast. It is the fourth-largest city in the country and an important economic and cultural center in the Caribbean region. Barranquilla is known for its vibrant culture and festive atmosphere, exemplified by its famous Carnival of Barranquilla.\n\n\nShow the code\nlibrary(leaflet)\nleaflet() %>%\n  addTiles() %>%  # Add default OpenStreetMap map tiles\n  addMarkers(lng=-74.77028, lat=10.98611, popup=\"The birthplace of R\")\n\n\n\n\n\n\nAs a graduated engineer, I have acquired extensive experience and problemâ€“solving expertise in data wrangling, statistical analysis, and model development. Since effective communication of data is of paramount importance, I have subsequently developed strong skills in data visualization and business knowledge. I hold an MBA from Willamette University and currently pursuing a second masterâ€™s in Data Science.\nI have experience working in the public and private sectors helping organizations to embrace Shared Value by positively impacting society and the environment: The sweet spot where profit meets purpose."
  },
  {
    "objectID": "karolo89/karol_website-karolo89/karol_website-karolo89.html#introduction",
    "href": "karolo89/karol_website-karolo89/karol_website-karolo89.html#introduction",
    "title": "A Title",
    "section": "Introduction",
    "text": "Introduction\nSubtitle\nsome code and some text.\nA URL www.good.cnrs.fr.\n\n\n[1] 2\n\n\n\n\n[1] 2"
  },
  {
    "objectID": "karolo89/karol_website-karolo89/karol_website-karolo89.html#ggplot2-with-code",
    "href": "karolo89/karol_website-karolo89/karol_website-karolo89.html#ggplot2-with-code",
    "title": "A Title",
    "section": "GGPLOT2 With code",
    "text": "GGPLOT2 With code"
  },
  {
    "objectID": "karolo89/karol_website-karolo89/karol_website-karolo89.html#new-slide",
    "href": "karolo89/karol_website-karolo89/karol_website-karolo89.html#new-slide",
    "title": "A Title",
    "section": "New slide",
    "text": "New slide\nWith only text"
  },
  {
    "objectID": "KarolOrozco.html#my-style",
    "href": "KarolOrozco.html#my-style",
    "title": "Measures that Matter",
    "section": "My style",
    "text": "My style\nğ‘° ğ’†ğ’ğ’”ğ’–ğ’“ğ’† ğ’„ğ’–ğ’ğ’•ğ’–ğ’“ğ’‚ğ’ğ’ğ’š ğ’“ğ’†ğ’ğ’†ğ’—ğ’‚ğ’ğ’• ğ’…ğ’‚ğ’•ğ’‚ ğ’‚ğ’ğ’‚ğ’ğ’šğ’”ğ’Šğ’” ğ’ƒğ’š:\n\n\nCraft intuitive data visualizations and clear narratives to help readers engage well with patterns in data\n\n\n\n\nTranslate data reports and business jargon into plain language in English and Spanish\n\n\n\n\nCreate bespoke solutions which automate data processes, and equipping others to use them\n\n\n\n\nLinkedIn: @karolorozcoe | GitHub: @karolo89"
  },
  {
    "objectID": "KarolOrozco.html#section-1",
    "href": "KarolOrozco.html#section-1",
    "title": "Measures that Matter",
    "section": "",
    "text": "Collect data by partnering with community-based, religious, or other organizations that work with members of the assessmentâ€™s target communities\n\n\n\nConduct key informant interviews with community leaders\n\n\n\n\nEngage community members as data collectors and meeting there where they are\n\n\n\n\nShare early results with community stakeholders to validate findings, obtain candid feedback, and adjust the interpretation of data\n\n\n\n\nLinkedIn: @karolorozcoe | GitHub: @karolo89"
  },
  {
    "objectID": "KarolOrozco.html#section-2",
    "href": "KarolOrozco.html#section-2",
    "title": "Measures that Matter",
    "section": "",
    "text": "If you like what you see, get in touch!\nContact\n\nLinkedIn: @karolorozcoe | GitHub: @karolo89"
  },
  {
    "objectID": "Models/RegressionModel/RegressionModel.html",
    "href": "Models/RegressionModel/RegressionModel.html",
    "title": "House Prices in Portland, OR",
    "section": "",
    "text": "The goal is to build a classification model to predict the type of median housing prices in Portland, OR and its metropolitan area."
  },
  {
    "objectID": "Models/RegressionModel/RegressionModel.html#get-the-data",
    "href": "Models/RegressionModel/RegressionModel.html#get-the-data",
    "title": "House Prices in Portland, OR",
    "section": "Get the Data",
    "text": "Get the Data\n\n\nShow the code\nraw_pdx <- read.csv(\"https://raw.githubusercontent.com/karolo89/Raw_Data/main/PORTLAND%20HOUSE.csv\", stringsAsFactors=TRUE)"
  },
  {
    "objectID": "Models/RegressionModel/RegressionModel.html#prepare-the-data",
    "href": "Models/RegressionModel/RegressionModel.html#prepare-the-data",
    "title": "House Prices in Portland, OR",
    "section": "Prepare the Data",
    "text": "Prepare the Data\nThis data has 25731 obs. of 32 variables\n\n\nShow the code\n## raw_pdx <- raw_pdx%>%select(-id)\n\nhead(raw_pdx)\n\n\n  id yearBuilt     City latitude longitude zipcode bathrooms bedrooms\n1  1      2007 Fairview 45.54357 -122.4418   97024         3        3\n2  2      2001 Fairview 45.54758 -122.4532   97024         3        3\n3  3      1982  Gresham 45.48823 -122.4444   97080         3        4\n4  4      1953 Portland 45.52663 -122.4641   97230         1        3\n5  5      1967  Gresham 45.51124 -122.4315   97030         3        6\n6  6      1967  Gresham 45.48799 -122.4162   97080         2        3\n  DateListed  DateSold daysOnZillow      homeType lastSoldPrice livingArea\n1  4/26/2021 5/21/2021           25     TOWNHOUSE        315400       1806\n2   3/1/2021 4/23/2021           53 SINGLE_FAMILY        400000       1518\n3  5/24/2021  6/4/2021           11 SINGLE_FAMILY        512000       2724\n4  5/24/2021  6/4/2021           11 SINGLE_FAMILY        348000       1217\n5  5/18/2021  6/1/2021           14     APARTMENT        510000       2400\n6  5/18/2021  6/1/2021           14 SINGLE_FAMILY        404200       1150\n  lotSize  price priceHistory.1.price propertyTaxRate hasCooling hasFireplace\n1    1555 315400               212000            1.12      FALSE         TRUE\n2    3484 400000               375000            1.12       TRUE         TRUE\n3    9583 512000               479000            1.12       TRUE         TRUE\n4   13939 348000               339500            1.12         NA         TRUE\n5    8545 510000               252450            1.12       TRUE           NA\n6    7000 404200               204500            1.12      FALSE         TRUE\n  hasGarage hasHeating hasView ElementarySchooldistance ElementarySchools\n1     FALSE       TRUE   FALSE                      0.4        Elementary\n2     FALSE       TRUE    TRUE                      1.2        Elementary\n3     FALSE       TRUE    TRUE                      0.8        Elementary\n4     FALSE       TRUE   FALSE                      0.8        Elementary\n5     FALSE       TRUE   FALSE                      0.3        Elementary\n6     FALSE       TRUE   FALSE                      0.4        Elementary\n  ElementarySchoolrating MiddleSchooldistance schoolsMiddlelevel\n1                      5                  1.1             Middle\n2                      5                  1.0             Middle\n3                      5                  1.7             Middle\n4                      2                  0.7             Middle\n5                      2                  0.9             Middle\n6                      2                  0.4             Middle\n  MiddleSchoolsrating HighSchooldistance HighSchoollevel HighSchoolRating\n1                   2                2.6            High                3\n2                   2                3.4            High                3\n3                   6                1.4            High                3\n4                   2                3.8            High                3\n5                   6                0.3            High                3\n6                   6                1.4            High                3\n\n\n\nMissing data\n\n\nShow the code\nclean_data <- raw_pdx %>%\n  filter(!is.na(yearBuilt))%>%\n  filter(!is.na(longitude))%>%\n  filter(!is.na(bedrooms))%>%\n  filter(!is.na(daysOnZillow))%>%\n  filter(!is.na(livingArea))%>%\n  filter(!is.na(priceHistory.1.price))%>%\n  filter(!is.na(hasFireplace))%>%\n  filter(!is.na(latitude))%>%\n  filter(!is.na(hasHeating))%>%\n  filter(!is.na(hasCooling))%>%\n  filter(!is.na(bathrooms))%>%\n  filter(!is.na(lotSize))%>%\n  filter(!is.na(propertyTaxRate))%>%\n  filter(!is.na(ElementarySchooldistance))%>%\n  filter(!is.na(MiddleSchooldistance))%>%\n  filter(!is.na(HighSchooldistance))%>%\n  filter(!is.na(ElementarySchoolrating))%>%\n  filter(!is.na(MiddleSchoolsrating))%>%\n  filter(!is.na(HighSchoolRating))\n\n\n\n\nShow the code\nsummary(clean_data)\n\n\n       id          yearBuilt             City         latitude    \n Min.   :    1   Min.   :   0   Portland   :5232   Min.   :45.26  \n 1st Qu.: 6207   1st Qu.:1965   Beaverton  :1456   1st Qu.:45.42  \n Median :15156   Median :1989   Hillsboro  :1194   Median :45.47  \n Mean   :13622   Mean   :1981   Lake Oswego: 942   Mean   :45.47  \n 3rd Qu.:20440   3rd Qu.:2003   Tigard     : 913   3rd Qu.:45.52  \n Max.   :25730   Max.   :2021   Gresham    : 845   Max.   :45.62  \n                                (Other)    :3906                  \n   longitude         zipcode        bathrooms        bedrooms     \n Min.   :-123.1   97229  :  834   Min.   : 0.00   Min.   : 0.000  \n 1st Qu.:-122.8   97045  :  713   1st Qu.: 2.00   1st Qu.: 3.000  \n Median :-122.7   97007  :  706   Median : 3.00   Median : 3.000  \n Mean   :-122.7   97086  :  632   Mean   : 2.78   Mean   : 3.568  \n 3rd Qu.:-122.6   97123  :  599   3rd Qu.: 3.00   3rd Qu.: 4.000  \n Max.   :-122.3   97068  :  573   Max.   :10.00   Max.   :10.000  \n                  (Other):10431                                   \n     DateListed          DateSold      daysOnZillow              homeType    \n 8/9/2019 :   71   5/28/2021 :  159   Min.   :  1   APARTMENT        :   38  \n 6/4/2021 :   66   6/30/2021 :  142   1st Qu.: 80   CONDO            :  136  \n 3/30/2021:   65   10/30/2020:  137   Median :192   HOME_TYPE_UNKNOWN:    2  \n 8/6/2019 :   64   7/31/2020 :  128   Mean   :187   SINGLE_FAMILY    :13659  \n 8/19/2019:   62   4/30/2021 :  127   3rd Qu.:294   TOWNHOUSE        :  653  \n 8/23/2019:   60   9/30/2020 :  122   Max.   :422                            \n (Other)  :14100   (Other)   :13673                                          \n lastSoldPrice       livingArea       lotSize             price        \n Min.   :    443   Min.   :  416   Min.   :       0   Min.   :    500  \n 1st Qu.: 450000   1st Qu.: 1664   1st Qu.:    4791   1st Qu.: 450000  \n Median : 551000   Median : 2206   Median :    7405   Median : 551000  \n Mean   : 634458   Mean   : 2399   Mean   :   17065   Mean   : 634827  \n 3rd Qu.: 710000   3rd Qu.: 2892   3rd Qu.:   10018   3rd Qu.: 710000  \n Max.   :6300000   Max.   :14014   Max.   :18992160   Max.   :6300000  \n                                                                       \n priceHistory.1.price propertyTaxRate hasCooling      hasFireplace   \n Min.   :    895      Min.   :1.010   Mode :logical   Mode :logical  \n 1st Qu.: 415000      1st Qu.:1.080   FALSE:807       FALSE:645      \n Median : 525000      Median :1.120   TRUE :13681     TRUE :13843    \n Mean   : 594080      Mean   :1.113                                  \n 3rd Qu.: 679992      3rd Qu.:1.130                                  \n Max.   :6888000      Max.   :1.130                                  \n                                                                     \n hasGarage       hasHeating       hasView        ElementarySchooldistance\n Mode :logical   Mode :logical   Mode :logical   Min.   :0.0000          \n FALSE:14150     FALSE:24        FALSE:8574      1st Qu.:0.4000          \n TRUE :338       TRUE :14464     TRUE :5914      Median :0.6000          \n                                                 Mean   :0.8057          \n                                                 3rd Qu.:1.0000          \n                                                 Max.   :9.4000          \n                                                                         \n  ElementarySchools ElementarySchoolrating MiddleSchooldistance\n           :    0   7      :2862           Min.   : 0.000      \n Elementary:13077   5      :2359           1st Qu.: 0.800      \n Primary   : 1411   6      :2323           Median : 1.300      \n                    8      :1606           Mean   : 1.546      \n                    3      :1368           3rd Qu.: 2.100      \n                    4      :1332           Max.   :11.900      \n                    (Other):2638                               \n  schoolsMiddlelevel MiddleSchoolsrating HighSchooldistance HighSchoollevel\n           :    0    5      :2347        Min.   : 0.100         :    0     \n Elementary:    3    8      :2262        1st Qu.: 1.000     High:14488     \n High      :    0    3      :2238        Median : 1.700                    \n Middle    :14485    6      :2104        Mean   : 1.911                    \n                     7      :1655        3rd Qu.: 2.500                    \n                     4      :1595        Max.   :10.800                    \n                     (Other):2287                                          \n HighSchoolRating\n 5      :3804    \n 8      :2365    \n 3      :1962    \n 6      :1787    \n 9      :1746    \n 4      :1230    \n (Other):1594"
  },
  {
    "objectID": "Models/RegressionModel/RegressionModel.html#take-a-look-at-the-data",
    "href": "Models/RegressionModel/RegressionModel.html#take-a-look-at-the-data",
    "title": "House Prices in Portland, OR",
    "section": "Take a look at the Data",
    "text": "Take a look at the Data\n\n\nShow the code\nclean_data %>% \n  count(price_category, \n        name =\"total\") %>%\n  mutate(percent = total/sum(total)*100,\n         percent = round(percent, 2)) %>%\n gt() %>%\n  tab_header(\n    title = \"Portland, OR and its Metropolitan Area Median House Prices\",\n    subtitle = \"Above and below 551,000$\"\n  ) %>%\n  cols_label(\n    price_category = \"Price\",\n    total = \"Total\",\n    percent = \"Percent\"\n  ) %>% \n  fmt_number(\n    columns = vars(total),\n    suffixing = TRUE\n  )  \n\n\nWarning: Since gt v0.3.0, `columns = vars(...)` has been deprecated.\nâ€¢ Please use `columns = c(...)` instead.\nSince gt v0.3.0, `columns = vars(...)` has been deprecated.\nâ€¢ Please use `columns = c(...)` instead.\n\n\n\n\n\n\n  \n    \n      Portland, OR and its Metropolitan Area Median House Prices\n    \n    \n      Above and below 551,000$\n    \n  \n  \n    \n      Price\n      Total\n      Percent\n    \n  \n  \n    above\n7.25K\n50.02\n    below\n7.24K\n49.98\n  \n  \n  \n\n\n\n\n\n\nShow the code\nqmplot(x = longitude, \n       y = latitude, \n       data = clean_data, \n       geom = \"point\", \n       color = price_category, \n       alpha = 0.4) +\n  scale_alpha(guide = 'none')\n\n\nâ„¹ Using `zoom = 10`\n\n\nâ„¹ Map tiles by Stamen Design, under CC BY 3.0. Data by OpenStreetMap, under ODbL.\n\n\n\n\n\n\n\nShow the code\nhouses_pdx <-\n  clean_data %>% \n  select( # select our predictors\n    longitude, \n    latitude, \n    price_category,\n    bathrooms, \n    yearBuilt, \n    homeType,\n    bedrooms, \n    livingArea, \n    lotSize,\n    MiddleSchooldistance,\n    ElementarySchooldistance,\n    HighSchooldistance)\n\nglimpse(houses_pdx)\n\n\nRows: 14,488\nColumns: 12\n$ longitude                <dbl> -122.4418, -122.4532, -122.4444, -122.4162, -â€¦\n$ latitude                 <dbl> 45.54357, 45.54758, 45.48823, 45.48799, 45.49â€¦\n$ price_category           <fct> below, below, below, below, below, below, belâ€¦\n$ bathrooms                <dbl> 3.0, 3.0, 3.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, â€¦\n$ yearBuilt                <dbl> 2007, 2001, 1982, 1967, 1978, 2018, 2006, 201â€¦\n$ homeType                 <fct> TOWNHOUSE, SINGLE_FAMILY, SINGLE_FAMILY, SINGâ€¦\n$ bedrooms                 <dbl> 3, 3, 4, 3, 3, 4, 3, 3, 4, 4, 3, 2, 4, 3, 4, â€¦\n$ livingArea               <dbl> 1806, 1518, 2724, 1150, 2036, 1947, 1548, 220â€¦\n$ lotSize                  <dbl> 1555, 3484, 9583, 7000, 6969, 4791, 5009, 522â€¦\n$ MiddleSchooldistance     <dbl> 1.1, 1.0, 1.7, 0.4, 2.1, 2.5, 0.5, 1.8, 0.3, â€¦\n$ ElementarySchooldistance <dbl> 0.4, 1.2, 0.8, 0.4, 1.0, 0.3, 0.5, 1.0, 0.1, â€¦\n$ HighSchooldistance       <dbl> 2.6, 3.4, 1.4, 1.4, 1.4, 2.2, 1.5, 1.4, 0.9, â€¦\n\n\n\n\nShow the code\npdx_long <- houses_pdx %>% \n  select(-longitude,-latitude, -homeType, -yearBuilt, -lotSize)%>%\n    pivot_longer(!price_category, names_to = \"features\", values_to = \"values\")\n\n\n# Print the first 10 rows\npdx_long %>% \n  slice_head(n = 10)\n\n\n# A tibble: 10 Ã— 3\n   price_category features                 values\n   <fct>          <chr>                     <dbl>\n 1 below          bathrooms                   3  \n 2 below          bedrooms                    3  \n 3 below          livingArea               1806  \n 4 below          MiddleSchooldistance        1.1\n 5 below          ElementarySchooldistance    0.4\n 6 below          HighSchooldistance          2.6\n 7 below          bathrooms                   3  \n 8 below          bedrooms                    3  \n 9 below          livingArea               1518  \n10 below          MiddleSchooldistance        1  \n\n\n\n\nShow the code\ntheme_set(theme_light())\n\n# Make a box plot for each predictor feature\npdx_long %>% \n  ggplot(mapping = aes(x = price_category, y = values, fill = features)) +\n  geom_boxplot() + \n  facet_wrap(~ features, scales = \"free\", ncol = 4) +\n  scale_color_viridis_d(option = \"plasma\", end = .7) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nData Splitting\n\n\nShow the code\n# Fix the random numbers by setting the seed \n# This enables the analysis to be reproducible \nset.seed(504)\n\n# Put 3/4 of the data into the training set \ndata_split <- initial_split(houses_pdx, \n                           prop = 3/4)\n\n# Create dataframes for the two sets:\ntrain_data <- training(data_split) \ntest_data <- testing(data_split)\n\n\n\n\nValidaton Set\n\n\nShow the code\nhouse_folds <-\n vfold_cv(train_data, \n          v = 5, \n          strata = price_category) \n\n\n\n\nShow the code\npdx_rec <-\n  recipe(price_category ~ .,\n         data = train_data) %>%\n  update_role(longitude, latitude, \n              new_role = \"ID\") %>% \n  \n  step_naomit(everything(), skip = TRUE) %>% \n  \n  step_novel(all_nominal(), -all_outcomes()) %>% # converts all nominal variables to factors and takes care of other issues related to categorical variables.\n  \n  step_normalize(all_numeric(), -all_outcomes(), \n                 -longitude, -latitude) %>% # step_normalize() normalizes (center and scales) the numeric variables to have a standard deviation of one and a mean of zero\n  \n  step_dummy(all_nominal(), -all_outcomes()) %>% #converts our factor columns into numeric binary (0 and 1) variables.\n  \n  step_zv(all_numeric(), -all_outcomes()) %>% ## step_zv(): removes any numeric variables that have zero variance.\n  \n  step_corr(all_predictors(), threshold = 0.7, method = \"spearman\") # step_corr(): will remove predictor variables that have large correlations with other predictor variables.\n\n\n\n\nShow the code\nsummary(pdx_rec)\n\n\n# A tibble: 12 Ã— 4\n   variable                 type      role      source  \n   <chr>                    <list>    <chr>     <chr>   \n 1 longitude                <chr [2]> ID        original\n 2 latitude                 <chr [2]> ID        original\n 3 bathrooms                <chr [2]> predictor original\n 4 yearBuilt                <chr [2]> predictor original\n 5 homeType                 <chr [3]> predictor original\n 6 bedrooms                 <chr [2]> predictor original\n 7 livingArea               <chr [2]> predictor original\n 8 lotSize                  <chr [2]> predictor original\n 9 MiddleSchooldistance     <chr [2]> predictor original\n10 ElementarySchooldistance <chr [2]> predictor original\n11 HighSchooldistance       <chr [2]> predictor original\n12 price_category           <chr [3]> outcome   original\n\n\n\n\nShow the code\nprep_data <- \n  pdx_rec %>% # use the recipe object\n  prep() %>% # perform the recipe on training data\n  juice() # extract only the preprocessed dataframe"
  },
  {
    "objectID": "Models/RegressionModel/RegressionModel.html#the-model--logistic-regression",
    "href": "Models/RegressionModel/RegressionModel.html#the-model--logistic-regression",
    "title": "House Prices in Portland, OR",
    "section": "The Model- Logistic regression",
    "text": "The Model- Logistic regression\n\n\nShow the code\nlog_spec <- # your model specification\n  logistic_reg() %>%  # model type\n  set_engine(engine = \"glm\") %>%  # model engine\n  set_mode(\"classification\") # model mode\n\n# Show your model specification\nlog_spec\n\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\nShow the code\npdx_wflow <- # new workflow object\n workflow() %>% # use workflow function\n add_recipe(pdx_rec) %>%   # use the new recipe\n add_model(log_spec)   # add your model spec\n\npdx_wflow\n\n\nâ•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPreprocessor: Recipe\nModel: logistic_reg()\n\nâ”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n6 Recipe Steps\n\nâ€¢ step_naomit()\nâ€¢ step_novel()\nâ€¢ step_normalize()\nâ€¢ step_dummy()\nâ€¢ step_zv()\nâ€¢ step_corr()\n\nâ”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\nShow the code\n# save model coefficients for a fitted model object from a workflow\n\nget_model <- function(x) {\n  pull_workflow_fit(x) %>% tidy()\n}\n\n# same as before with one exception\nlog_res_2 <- \n  pdx_wflow %>% \n  fit_resamples(\n    resamples = house_folds, \n    metrics = metric_set(\n      recall, precision, f_meas, \n      accuracy, kap,\n      roc_auc, sens, spec),\n      control = control_resamples(\n      save_pred = TRUE,\n      extract = get_model) # use extract and our new function\n    ) \n\n\n## All of the results can be flattened and collected using:\n\n\n\nall_coef <- map_dfr(log_res_2$.extracts, ~ .x[[1]][[1]])\nfilter(all_coef, term == \"bedrooms\")\n\n\n# A tibble: 5 Ã— 5\n  term     estimate std.error statistic  p.value\n  <chr>       <dbl>     <dbl>     <dbl>    <dbl>\n1 bedrooms   0.0748    0.0424      1.76 0.0777  \n2 bedrooms   0.129     0.0433      2.99 0.00284 \n3 bedrooms   0.116     0.0423      2.74 0.00623 \n4 bedrooms   0.118     0.0425      2.78 0.00550 \n5 bedrooms   0.147     0.0430      3.42 0.000618\n\n\n\nPerformance metrics\nShow performance for every single fold:\n\n\nShow the code\nlog_res_2 %>%  collect_metrics(summarize = FALSE)\n\n\n# A tibble: 40 Ã— 5\n   id    .metric   .estimator .estimate .config             \n   <chr> <chr>     <chr>          <dbl> <chr>               \n 1 Fold1 recall    binary         0.788 Preprocessor1_Model1\n 2 Fold1 precision binary         0.834 Preprocessor1_Model1\n 3 Fold1 f_meas    binary         0.810 Preprocessor1_Model1\n 4 Fold1 accuracy  binary         0.816 Preprocessor1_Model1\n 5 Fold1 kap       binary         0.631 Preprocessor1_Model1\n 6 Fold1 sens      binary         0.788 Preprocessor1_Model1\n 7 Fold1 spec      binary         0.843 Preprocessor1_Model1\n 8 Fold1 roc_auc   binary         0.902 Preprocessor1_Model1\n 9 Fold2 recall    binary         0.783 Preprocessor1_Model1\n10 Fold2 precision binary         0.824 Preprocessor1_Model1\n# â€¦ with 30 more rows\n\n\n\n\nCollect predictions\nTo obtain the actual model predictions, we use the function collect_predictions and save the result as log_pred:\n\n\nShow the code\nlog_pred <- \n  log_res_2 %>%\n  collect_predictions()\n\n\nlog_pred %>% \n  conf_mat(price_category, .pred_class) \n\n\n          Truth\nPrediction above below\n     above  4344   856\n     below  1093  4573\n\n\n\n\nShow the code\nlog_pred %>% \n  conf_mat(price_category, .pred_class) %>% \n  autoplot(type = \"heatmap\")+\n  theme_minimal()\n\n\n\n\n\n\n\nROC Curve\n\n\nShow the code\nlog_pred %>% \n  group_by(id) %>% # id contains our folds\n  roc_curve(price_category, .pred_above) %>% \n  autoplot()+\n  theme_minimal()\n\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nâ„¹ Please use `reframe()` instead.\nâ„¹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\nâ„¹ The deprecated feature was likely used in the yardstick package.\n  Please report the issue at <https://github.com/tidymodels/yardstick/issues>."
  },
  {
    "objectID": "Models/RegressionModel/RegressionModel.html#use-the-workflow-to-train-our-model",
    "href": "Models/RegressionModel/RegressionModel.html#use-the-workflow-to-train-our-model",
    "title": "House Prices in Portland, OR",
    "section": "Use the workflow to train our model",
    "text": "Use the workflow to train our model\n\n\nShow the code\npdx_fit <- fit(pdx_wflow, train_data)\n\n\nThis allows us to use the model trained by this workflow to predict labels for our test set, and compare the performance metrics with the basic model we created previously.\n\n\nShow the code\npdx_fit %>% ## display results\npull_workflow_fit() %>%\ntidy()%>%\n  filter(p.value < 0.05)\n\n\n# A tibble: 8 Ã— 5\n  term                 estimate std.error statistic  p.value\n  <chr>                   <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)           -0.426     0.0296    -14.4  5.43e-47\n2 yearBuilt              0.221     0.0482      4.59 4.49e- 6\n3 bedrooms               0.119     0.0381      3.12 1.82e- 3\n4 livingArea            -2.88      0.0689    -41.8  0       \n5 lotSize               -0.0448    0.0202     -2.21 2.68e- 2\n6 MiddleSchooldistance  -0.171     0.0310     -5.52 3.33e- 8\n7 HighSchooldistance     0.0805    0.0303      2.66 7.86e- 3\n8 homeType_TOWNHOUSE     0.998     0.179       5.59 2.24e- 8\n\n\n\n\nShow the code\nlibrary(vip)\n\npdx_fit %>%\nextract_fit_parsnip() %>%\n   vip(num_features = 5)+\n  theme_minimal()\n\n\n\n\n\nThe two most important predictors in whether the median house value is above or below 551,000 dollars were the Living Area and the home type: Townhouse\n\n\nShow the code\n# Make predictions on the test set\npred_results <- test_data %>% \n  select(price_category) %>% \n  bind_cols(pdx_fit %>% \n              predict(new_data = test_data)) %>% \n  bind_cols(pdx_fit %>% \n              predict(new_data = test_data, type = \"prob\"))\n\n# Print the results\npred_results %>% \n  slice_head(n = 10)\n\n\n   price_category .pred_class .pred_above .pred_below\n1           below       above  0.79554296   0.2044570\n2           below       below  0.26469350   0.7353065\n3           below       below  0.47600771   0.5239923\n4           below       below  0.02353063   0.9764694\n5           below       below  0.06272184   0.9372782\n6           below       below  0.10491967   0.8950803\n7           below       below  0.06923625   0.9307637\n8           below       below  0.03314398   0.9668560\n9           below       below  0.04580355   0.9541965\n10          below       above  0.83998089   0.1600191\n\n\nLetâ€™s take a look at the confusion matrix:\n\n\nShow the code\npred_results%>% \n  conf_mat(price_category, .pred_class) %>% \n  autoplot(type = \"heatmap\")+\n  theme_minimal()"
  },
  {
    "objectID": "Models/RegressionModel/RegressionModel.html#reference",
    "href": "Models/RegressionModel/RegressionModel.html#reference",
    "title": "House Prices in Portland, OR",
    "section": "Reference",
    "text": "Reference\nTidymodels- https://www.tidymodels.org/"
  },
  {
    "objectID": "Models/TimeSeries/Time Series Project.html",
    "href": "Models/TimeSeries/Time Series Project.html",
    "title": "Time Series",
    "section": "",
    "text": "In this project, I will perform Time series analysis using the Zillow Home Value Index (ZHVI) dataset: A smoothed, seasonally adjusted measure of the typical home value and market changes across Portland, OR, four bedroom houses. It reflects the typical value for homes in the 35th to 65th percentile range.\nHere is the link: https://www.zillow.com/research/data/"
  },
  {
    "objectID": "Models/TimeSeries/Time Series Project.html#the-data",
    "href": "Models/TimeSeries/Time Series Project.html#the-data",
    "title": "Time Series",
    "section": "The Data",
    "text": "The Data\n\n\nShow the code\nmetrofour <- read.csv(\"https://raw.githubusercontent.com/karolo89/Raw_Data/main/City_zhvi_bdrmcnt_4_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")\n\nstr(metrofour[,c(1:11)])\n\n\n'data.frame':   13528 obs. of  11 variables:\n $ RegionID   : int  6181 12447 39051 17426 6915 40326 13271 18959 54296 38128 ...\n $ SizeRank   : int  0 1 2 3 4 5 6 7 8 9 ...\n $ RegionName : chr  \"New York\" \"Los Angeles\" \"Houston\" \"Chicago\" ...\n $ RegionType : chr  \"city\" \"city\" \"city\" \"city\" ...\n $ StateName  : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ State      : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ Metro      : chr  \"New York-Newark-Jersey City, NY-NJ-PA\" \"Los Angeles-Long Beach-Anaheim, CA\" \"Houston-The Woodlands-Sugar Land, TX\" \"Chicago-Naperville-Elgin, IL-IN-WI\" ...\n $ CountyName : chr  \"Queens County\" \"Los Angeles County\" \"Harris County\" \"Cook County\" ...\n $ X2000.01.31: num  284723 302701 146377 179997 154391 ...\n $ X2000.02.29: num  286885 303000 146270 180472 154691 ...\n $ X2000.03.31: num  288711 304356 145921 181368 154943 ...\n\n\nWe have to make this dataset tidy. Tidy Data is a way of structuring data so that it can be easily understood by people and analyzed by machines.\nI need to remove the X at the beginning of the dates (X2000.01.31,X2000.02.29,â€¦)\n\n\nShow the code\nnames(metrofour) <- sub(\"^X\", \"\", names(metrofour))\n\nstr(metrofour[,c(1:11)])\n\n\n'data.frame':   13528 obs. of  11 variables:\n $ RegionID  : int  6181 12447 39051 17426 6915 40326 13271 18959 54296 38128 ...\n $ SizeRank  : int  0 1 2 3 4 5 6 7 8 9 ...\n $ RegionName: chr  \"New York\" \"Los Angeles\" \"Houston\" \"Chicago\" ...\n $ RegionType: chr  \"city\" \"city\" \"city\" \"city\" ...\n $ StateName : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ State     : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ Metro     : chr  \"New York-Newark-Jersey City, NY-NJ-PA\" \"Los Angeles-Long Beach-Anaheim, CA\" \"Houston-The Woodlands-Sugar Land, TX\" \"Chicago-Naperville-Elgin, IL-IN-WI\" ...\n $ CountyName: chr  \"Queens County\" \"Los Angeles County\" \"Harris County\" \"Cook County\" ...\n $ 2000.01.31: num  284723 302701 146377 179997 154391 ...\n $ 2000.02.29: num  286885 303000 146270 180472 154691 ...\n $ 2000.03.31: num  288711 304356 145921 181368 154943 ...\n\n\n\n\nShow the code\nhouse_price <- metrofour %>% \n  pivot_longer(-c(RegionID, SizeRank, RegionName, RegionType, StateName, State, Metro, CountyName),\n    names_to = \"Monthly\",\n    values_to = \"Price\"\n  ) \nstr(metrofour[,c(1:11)])\n\n\n'data.frame':   13528 obs. of  11 variables:\n $ RegionID  : int  6181 12447 39051 17426 6915 40326 13271 18959 54296 38128 ...\n $ SizeRank  : int  0 1 2 3 4 5 6 7 8 9 ...\n $ RegionName: chr  \"New York\" \"Los Angeles\" \"Houston\" \"Chicago\" ...\n $ RegionType: chr  \"city\" \"city\" \"city\" \"city\" ...\n $ StateName : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ State     : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ Metro     : chr  \"New York-Newark-Jersey City, NY-NJ-PA\" \"Los Angeles-Long Beach-Anaheim, CA\" \"Houston-The Woodlands-Sugar Land, TX\" \"Chicago-Naperville-Elgin, IL-IN-WI\" ...\n $ CountyName: chr  \"Queens County\" \"Los Angeles County\" \"Harris County\" \"Cook County\" ...\n $ 2000.01.31: num  284723 302701 146377 179997 154391 ...\n $ 2000.02.29: num  286885 303000 146270 180472 154691 ...\n $ 2000.03.31: num  288711 304356 145921 181368 154943 ...\n\n\n\n\nShow the code\n#Converting the Date from factor to character\n\nhouse_clean <- house_price %>%\n            mutate(Monthly_parsed = as.Date(Monthly,\"%Y.%m.%d\"))\n\n\nhouse_clean[[\"Monthly\"]]<- as.character(house_clean$Monthly)\n\nhouse_price[[\"Monthly\"]]<- as.character(house_price $Monthly)\nsummary(house_clean)\n\n\n    RegionID         SizeRank      RegionName         RegionType       \n Min.   :  3300   Min.   :    0   Length:3720200     Length:3720200    \n 1st Qu.: 17381   1st Qu.: 3511   Class :character   Class :character  \n Median : 31963   Median : 7196   Mode  :character   Mode  :character  \n Mean   : 51628   Mean   : 8235                                        \n 3rd Qu.: 46317   3rd Qu.:11713                                        \n Max.   :827230   Max.   :28439                                        \n                                                                       \n  StateName            State              Metro            CountyName       \n Length:3720200     Length:3720200     Length:3720200     Length:3720200    \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   Monthly              Price         Monthly_parsed      \n Length:3720200     Min.   :  17773   Min.   :2000-01-31  \n Class :character   1st Qu.: 172701   1st Qu.:2005-09-30  \n Mode  :character   Median : 244810   Median :2011-06-30  \n                    Mean   : 321841   Mean   :2011-07-01  \n                    3rd Qu.: 369075   3rd Qu.:2017-03-31  \n                    Max.   :8337561   Max.   :2022-11-30  \n                    NA's   :1201126                       \n\n\nWe see some missing values in the Price variable, but before I deal with those values, I will filter my data to the cities that I am interested the most\n\n\nShow the code\npdx_data <- house_clean %>%\n  dplyr:::filter(RegionID== 13373)  %>%\n  dplyr:::filter(Monthly_parsed >= \"2014-01-01\")\n\nsummary(pdx_data)\n\n\n    RegionID        SizeRank   RegionName         RegionType       \n Min.   :13373   Min.   :22   Length:107         Length:107        \n 1st Qu.:13373   1st Qu.:22   Class :character   Class :character  \n Median :13373   Median :22   Mode  :character   Mode  :character  \n Mean   :13373   Mean   :22                                        \n 3rd Qu.:13373   3rd Qu.:22                                        \n Max.   :13373   Max.   :22                                        \n  StateName            State              Metro            CountyName       \n Length:107         Length:107         Length:107         Length:107        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   Monthly              Price        Monthly_parsed      \n Length:107         Min.   :410582   Min.   :2014-01-31  \n Class :character   1st Qu.:506597   1st Qu.:2016-04-15  \n Mode  :character   Median :564473   Median :2018-06-30  \n                    Mean   :563372   Mean   :2018-06-30  \n                    3rd Qu.:588606   3rd Qu.:2020-09-15  \n                    Max.   :758797   Max.   :2022-11-30  \n\n\nAfter filtering the data, we donâ€™t have any missing values\n\nCoerce to a tsibble with as_tsibble()\nA time series can be recorded as a tsibble object in R. tsibble objects extend tidy data frames (tibble objects) by introducing temporal structure, and to do it, we need to declare key and index. In this case, the Monthly_parsed containing the data-time is the index and the RegionID is the key. Other columns can be considered as measured variables.\n\n\nShow the code\ntsb_pdx <- pdx_data %>%\n                   select(RegionName,RegionID, Monthly_parsed, Price)\n\ntsb_pref_pdx <-tsb_pdx%>%\n  as_tsibble(key= RegionName, index= Monthly_parsed)%>%\n                   index_by(year_month = ~ yearmonth(.))\n\ntsibble_pdx <-tsb_pref_pdx%>%\n  select(-RegionID)%>%\n  as_tsibble(key= RegionName, index= year_month)%>%\n  mutate(Prices = Price/1000)"
  },
  {
    "objectID": "Models/TimeSeries/Time Series Project.html#data-visualization",
    "href": "Models/TimeSeries/Time Series Project.html#data-visualization",
    "title": "Time Series",
    "section": "Data Visualization",
    "text": "Data Visualization\nTo visualize the data, I could use the autoplot() command, but I rather to create my graph with ggplot.\n\n\nShow the code\nplot_pdx_house <- tsibble_pdx %>%\n  ggplot(aes(x= year_month, y= Prices)) +\n  geom_line(size=1, color= \"darkgreen\")+\n   \n    labs(y=\"Price in Thousands of Dollars \", \n       x= \" \",\n       title=\" Four Bedroom House Prices in Portland, OR, 2014-2022 \",\n       caption = \"data:https://www.zillow.com/research/data\")+\n  scale_y_continuous(labels=scales::dollar_format())+\n   theme_light()\n\n\nplot_pdx_house \n\n\n\n\n\nData is non- stationary, we can see a trend-cycle component in the graph above.\n\n\nShow the code\ntsibble_pdx %>%\ngg_subseries(Price/1000)+\n  labs(y= \"Price in Thousands of Dollars\",\n       x= \"Year\")+theme_minimal()+\n  scale_y_continuous(labels=scales::dollar_format())+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\nShow the code\ntsibble_pdx%>%\ngg_season(Price/1000, labels = \"both\")+\n  labs(x= \"\",\n       y= \"Price in Thousands of Dollars \", \n       title=\"Portland's Seasonal Plot\")+\n  \n  scale_y_continuous(labels=scales::dollar_format())+\n  theme_minimal()"
  },
  {
    "objectID": "Models/TimeSeries/Time Series Project.html#determining-stationarity",
    "href": "Models/TimeSeries/Time Series Project.html#determining-stationarity",
    "title": "Time Series",
    "section": "Determining Stationarity",
    "text": "Determining Stationarity\nIn our analysis, we use the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test (Kwiatkowski et al., 1992). In this test, the null hypothesis is that the data are stationary, and we look for evidence that the null hypothesis is false. Consequently, small p-values (e.g., less than 0.05) suggest that differencing is required. The test can be computed using the unitroot_kpss() function.\n\n\nShow the code\ntsibble_pdx%>%\n  features(Prices, unitroot_kpss)\n\n\n# A tibble: 1 Ã— 3\n  RegionName kpss_stat kpss_pvalue\n  <chr>          <dbl>       <dbl>\n1 Portland        1.97        0.01\n\n\nThe p-value is reported as 0.01 if it is less than 0.01, and as 0.1 if it is greater than 0.1. In this case, the test statistic (1.946) is bigger than the 1% critical value, so the p-value is less than 0.01, indicating that the null hypothesis is rejected. That is, the data are not stationary.\n\n\nShow the code\ntsibble_pdx %>% \n  features(Prices ,unitroot_ndiffs)\n\n\n# A tibble: 1 Ã— 2\n  RegionName ndiffs\n  <chr>       <int>\n1 Portland        1\n\n\nAs we saw from the KPSS tests above, one difference (d) is required to make the tsibble_pdx data stationary."
  },
  {
    "objectID": "Models/TimeSeries/Time Series Project.html#autocorrelation",
    "href": "Models/TimeSeries/Time Series Project.html#autocorrelation",
    "title": "Time Series",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\n\nShow the code\ntsibble_pdx %>%\n  gg_tsdisplay(Prices,\n                     plot_type='partial')+\n       labs(y=\"Thousands of Dollars \", \n       x= \" \")\n\n\n\n\n\nACF does not drop quickly to zero, moreover the value is large and positive (almost 1 in this case). All these are signs of a non-stationary time series. Therefore it should be differenced to obtain a stationary series.\nPACF value r1 is almost 1. All other values ri,i >1 are small. This is a sign of a non stationary process that should be differenced in order to obtain a stationary series.\nThe data are clearly non-stationary, so we will first take a seasonal difference. The seasonally differenced data are shown below:\n\n\nShow the code\ntsibble_pdx %>%\n  gg_tsdisplay(difference(Prices, 12),\n               plot_type='partial', lag=36) +\n  labs(title=\"Seasonally differenced\", y=\"\")\n\n\n\n\n\nOur aim now is to find an appropriate ARIMA model based on the ACF and PACF shown in the Double Differenced graph."
  },
  {
    "objectID": "Models/TimeSeries/Time Series Project.html#seasonal-arima-model",
    "href": "Models/TimeSeries/Time Series Project.html#seasonal-arima-model",
    "title": "Time Series",
    "section": "Seasonal Arima Model",
    "text": "Seasonal Arima Model\n\n\nShow the code\nall_fit <- tsibble_pdx%>%\n  model(\n    arima212012 = ARIMA(Prices ~ pdq(2,1,2)+ PDQ(0,1,2)),\n    arima210011 = ARIMA(Prices ~ pdq(2,1,0)+ PDQ(0,1,1)),\n    stepwise = ARIMA(Prices),\n    search = ARIMA(Prices,stepwise=FALSE))\n\n\n\n\nShow the code\nall_fit %>% pivot_longer(!RegionName,\n            names_to = \"Model name\", \n            values_to = \"Orders\")\n\n\n# A mable: 4 x 3\n# Key:     RegionName, Model name [4]\n  RegionName `Model name`                             Orders\n  <chr>      <chr>                                   <model>\n1 Portland   arima212012           <ARIMA(2,1,2)(0,1,2)[12]>\n2 Portland   arima210011           <ARIMA(2,1,0)(0,1,1)[12]>\n3 Portland   stepwise                <ARIMA(3,1,2) w/ drift>\n4 Portland   search       <ARIMA(2,1,3)(0,0,1)[12] w/ drift>\n\n\n\n\nShow the code\nglance(all_fit) %>% arrange(AICc) %>% select(.model:BIC)\n\n\n# A tibble: 4 Ã— 6\n  .model      sigma2 log_lik   AIC  AICc   BIC\n  <chr>        <dbl>   <dbl> <dbl> <dbl> <dbl>\n1 arima212012   2.81   -190.  394.  395.  412.\n2 search        2.14   -191.  399.  400.  420.\n3 stepwise      2.23   -194.  402.  403.  420.\n4 arima210011   4.53   -207.  423.  423.  433.\n\n\nOf these models, the best is the ARIMA(2,1,2)(0,1,2)[12]model (i.e., it has the smallest AICc value).\n\n\nShow the code\narima212012 <- tsibble_pdx %>%\n  model(arima212012 = ARIMA(Prices ~ pdq(2,1,2)+ PDQ(0,1,2)))%>%\n  report()\n\n\nSeries: Prices \nModel: ARIMA(2,1,2)(0,1,2)[12] \n\nCoefficients:\n         ar1     ar2     ma1     ma2     sma1    sma2\n      0.5178  0.0341  1.0092  0.9999  -0.8285  0.1308\ns.e.  0.1130  0.1206  0.0573  0.0692   0.1461  0.1514\n\nsigma^2 estimated as 2.813:  log likelihood=-189.9\nAIC=393.8   AICc=395.1   BIC=411.6\n\n\n\n\nShow the code\nall_fit %>% select(arima212012) %>%\n  gg_tsresiduals()\n\n\n\n\n\n\n\nShow the code\nall_fit %>% select(\"search\") %>%\n  gg_tsresiduals()\n\n\n\n\n\n\n\nShow the code\naugment(all_fit) %>%\n  filter(.model=='arima212012') %>%\n  features(.innov, ljung_box, lag = 36, dof = 6)\n\n\n# A tibble: 1 Ã— 4\n  RegionName .model      lb_stat lb_pvalue\n  <chr>      <chr>         <dbl>     <dbl>\n1 Portland   arima212012    36.7     0.186\n\n\n\n\nShow the code\ntsibble_pdx %>%\n  model(ARIMA(Prices ~ pdq(2,1,2) + PDQ(0,1,2))) %>%\n  forecast() %>%\n  autoplot(tsibble_pdx) +\n  labs(y=\" Thousands of $US \",\n       x =\" \",\n       title=\"Forecast from the ARIMA(2,1,2)(0,1,2)[12] model applied to the Portland House Prices data\")+\ntheme_minimal()\n\n\n\n\n\nShow the code\n##Price in Thousands of Dollars\ntsibble_pdx %>%\n  model(ARIMA(Prices ~ pdq(2,1,2) + PDQ(0,1,2))) %>%\n  forecast()\n\n\n# A fable: 24 x 5 [1M]\n# Key:     RegionName, .model [1]\n   RegionName .model                                  year_mâ€¦Â¹      Prices .mean\n   <chr>      <chr>                                      <mth>      <dist> <dbl>\n 1 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1â€¦ 2022 Dec N(735, 2.9)  735.\n 2 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1â€¦ 2023 Jan  N(737, 21)  737.\n 3 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1â€¦ 2023 Feb  N(740, 75)  740.\n 4 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1â€¦ 2023 Mar N(745, 156)  745.\n 5 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1â€¦ 2023 Apr N(750, 255)  750.\n 6 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1â€¦ 2023 May N(755, 365)  755.\n 7 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1â€¦ 2023 Jun N(758, 482)  758.\n 8 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1â€¦ 2023 Jul N(760, 604)  760.\n 9 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1â€¦ 2023 Aug N(762, 727)  762.\n10 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1â€¦ 2023 Sep N(764, 852)  764.\n# â€¦ with 14 more rows, and abbreviated variable name Â¹â€‹year_month"
  },
  {
    "objectID": "Models/TimeSeries/Time Series Project.html#ets",
    "href": "Models/TimeSeries/Time Series Project.html#ets",
    "title": "Time Series",
    "section": "ETS",
    "text": "ETS\n\n\nShow the code\nfit_ets <- tsibble_pdx %>%\n  model(ETS(Prices))\nreport(fit_ets)\n\n\nSeries: Prices \nModel: ETS(M,Ad,N) \n  Smoothing parameters:\n    alpha = 0.9997705 \n    beta  = 0.9997704 \n    phi   = 0.9088093 \n\n  Initial states:\n     l[0]    b[0]\n 406.6826 3.75712\n\n  sigma^2:  0\n\n     AIC     AICc      BIC \n659.0909 659.9309 675.1278 \n\n\nThe model selected is ETS(M,Ad,N)\n\n\nShow the code\ncomponents(fit_ets) %>%\n  autoplot() +\n  labs(title = \"ETS(M,Ad,N) components\")\n\n\n\n\n\nBecause this model has multiplicative errors, the innovation residuals are not equivalent to the regular residuals.\n\n\nShow the code\nfit_ets %>%\n    augment() %>%\n    select(.innov, .resid) %>%\n    pivot_longer(c(.innov, .resid)) %>%\n    autoplot()+\n   theme_fivethirtyeight()\n\n\n\n\n\n\n\nShow the code\nfit_ets%>%\n    gg_tsresiduals()\n\n\n\n\n\n\n\nShow the code\nfit_ets %>%\n  forecast(h = 24) %>%\n  autoplot(tsibble_pdx)+\n\n   theme_light()\n\n\n\n\n\n\n\nShow the code\nbind_rows(\n    arima212012 %>% accuracy(),\n    fit_ets %>% accuracy()) %>%\n  select(-ME, -MPE, -ACF1)\n\n\n# A tibble: 2 Ã— 8\n  RegionName .model      .type     RMSE   MAE  MAPE   MASE  RMSSE\n  <chr>      <chr>       <chr>    <dbl> <dbl> <dbl>  <dbl>  <dbl>\n1 Portland   arima212012 Training  1.52  1.13 0.193 0.0284 0.0308\n2 Portland   ETS(Prices) Training  2.27  1.62 0.274 0.0408 0.0461\n\n\nIn this case the ARIMA model seems to be more accurate model based on the test set RMSE, MAPE and MASE."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Indoor Air Pollution\n\n\n\n\n\n\nMarch 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFreedom in the World\n\n\n\n\n\n\nMarch 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransit Costs Project\n\n\n\n\n\n\nMarch 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHurricanes\n\n\n\nKarol Orozco\n\n\nFebruary 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHouse Prices in Portland, OR\n\n\n\nKarol Orozco\n\n\nDecember 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReported Bias Crimes in Portland\n\n\n\n\n\n\nFebruary 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCensus Data\n\n\n\n\n\n\nFebruary 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVoter Registrations Are Way, Way Down During The Pandemic\n\n\n\n\n\n\nMarch 6, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "social/biascrimes/biascrime.html",
    "href": "social/biascrimes/biascrime.html",
    "title": "Reported Bias Crimes in Portland",
    "section": "",
    "text": "This data was obtained from the PortlandMaps- Open Data website- Reported Bias Crime Statistics, Portland Police Bureau.\nA hate crime is a criminal offense committed against persons, property, or society that is motivated, in whole or in part, by an offenderâ€™s bias against an individualâ€™s or a groupâ€™s race, religion, ethnic/national origin, gender, age, disability or sexual orientation. (Definition developed at the 1998 IACP Summit on Hate Crime in America.) This data exploration aims to learn about the most common locations where these crimes occurred and their types."
  },
  {
    "objectID": "social/biascrimes/biascrime.html#insights",
    "href": "social/biascrimes/biascrime.html#insights",
    "title": "Reported Bias Crimes in Portland",
    "section": "Insights:",
    "text": "Insights:\n\nThe victims are from 2 to 83 years old, with a median of 32 (when the information was available)\nThe ages of the suspects are from 11 to over 80 years, with a median of 40 (when the information was available)\nMost hate crimes were reported in 2021, with a decline in 2022, but bias incidents have increased over the past two years.\nOver the past five years, the most common motivations for bias have been race, sexual orientation, and ethnicity.\nThe most common cases of bias reported have been against Black or African-American people, followed by Anti-Gay (Male) and Anti-White.\nThe places where these incidents occurred the most are Public Spaces (Streets/Roads/Sidewalks), residences, and Parking Lots.\n\n\n\nShow the code\npp1 <- crime %>%\n filter(!(Bias.Category %in% \"\")) %>%\n filter(!(Bias.Type %in% \"\")) %>%\n filter(!(Location.Type %in% \n \"\")) %>%\n filter(!(Primary.Offense.Type %in% c(\"\", \"Hit & Run-Property\"))) %>%\n filter(!(Suspect.Gender %in% \n \"\")) %>%\n filter(!(Suspect.Race %in% \"\")) %>%\n filter(!(Victim.Gender %in% \"\")) %>%\n filter(!(Victim.Race %in% \n \"\")) %>%\n filter(!(Victim.Suspect.Relationship %in% \"\")) %>%\n ggplot() +\n aes(x = Victim.Age, fill = Victim.Gender) +\n geom_histogram(bins = 30L) +\n scale_fill_viridis_d(option = \"viridis\") +\n labs(title = \"Number of Victims by Gender and Age Group \",  x=\"Age\", y=\"\") +\n theme_minimal() +\n theme(\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        legend.position = \"none\",\n        plot.title.position = \"plot\", \n        plot.title = element_text(lineheight = 0.5, face = \"bold\", size = 12, hjust = 0.5),\n        axis.text = element_text(size = 7),\n        axis.title = element_text(size= 8),\n        plot.background = element_rect(fill = \"#fafafa\", colour = \"#fafafa\"),\n        panel.background = element_rect(fill = \"#fafafa\", colour = \"#fafafa\"))\n\npp2 <-crime %>%\n filter(!(Bias.Category %in% \"\")) %>%\n filter(!(Bias.Type %in% \"\")) %>%\n filter(!(Location.Type %in% \n \"\")) %>%\n filter(!(Primary.Offense.Type %in% c(\"\", \"Hit & Run-Property\"))) %>%\n filter(!(Suspect.Gender %in% \n \"\")) %>%\n filter(!(Suspect.Race %in% \"\")) %>%\n filter(!(Victim.Gender %in% \"\")) %>%\n filter(!(Victim.Race %in% \n \"\")) %>%\n filter(!(Victim.Suspect.Relationship %in% \"\")) %>%\n ggplot() +\n aes(x = Suspect.Age, y = Suspect.Gender, fill = Suspect.Gender) +\n geom_boxplot() +\n  labs(x= \"Age\",\n       y= \"\")+\n scale_fill_viridis_d(option = \"viridis\", \n direction = 1) +\n labs(title = \"Number of Suspects by Gender and Age Group \",\n      x= \"Age\") +\n  theme_minimal()+\n theme(\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        legend.position = \"none\",\n        plot.title.position = \"plot\", \n        plot.subtitle = element_text(hjust = 0.5, lineheight = 0.4, size = 10),\n        plot.title = element_text(lineheight = 0.5, face = \"bold\", size = 12, hjust = 0.5),\n        axis.text = element_text(size = 7),\n        axis.title = element_text(size= 8),\n        plot.background = element_rect(fill = \"#fafafa\", colour = \"#fafafa\"),\n        panel.background = element_rect(fill = \"#fafafa\", colour = \"#fafafa\"))\n\n\n\npp1+pp2\n\n\n\n\n\n\n\nShow the code\nlibrary(directlabels)\nlibrary(gganimate)\n\nplot <- ggplot(crime_type) +\n aes(x = year, y = prov_freq, colour = Case.Type) +\n geom_line() +\n  \n    geom_point(aes(x = year, y = prov_freq, shape=Case.Type, color=Case.Type))+\n  \n scale_color_viridis(discrete = TRUE, option = \"D\")+\n  scale_fill_viridis(discrete = TRUE) +\n  \n labs(y = \"Number of Cases\", \n      x= \"Year\",\n title = \"Report of Bias by Year\", \n subtitle = \"Data current through September 2022\", \n \n caption = \"Police Bureau: The graph visualizes the current case status for all reported bias/hate incidents that have been reviewed by the assigned detective.\\nBias Incidents (instances in which bias occurred, but no crime occurred) began to be recorded in March 2020\") +\n    geom_dl(aes(label = Case.Type), method = list(dl.trans(x = x + 0.2), \"last.points\", cex = 0.8)) +\n  \n theme_minimal() +\n theme(\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        legend.position = \"none\",\n        plot.title.position = \"plot\", \n        plot.subtitle = element_text(hjust = 0.5, lineheight = 0.4, size = 10),\n        plot.title = element_text(lineheight = 0.5, face = \"bold\", size = 12, hjust = 0.5),\n        axis.text = element_text(size = 7),\n        axis.title = element_text(size= 8),\n        plot.background = element_rect(fill = \"#fafafa\", colour = \"#fafafa\"),\n        panel.background = element_rect(fill = \"#fafafa\", colour = \"#fafafa\"))\n\nplot +transition_reveal(year)\n\n\n\n\n\n\n\nShow the code\n## Bias Category per year\n\n## Bias Category & Type\nbc_type <- crime%>%\n  group_by(Bias.Type, Bias.Category, year)%>%\n  summarise(frequency = n())%>%\nfilter(!(Bias.Category == \"\" | Bias.Category ==\"None\"))\n\nbc_type %>%\n  filter(year>=2020)%>%\nggplot(aes(x = reorder(Bias.Category, -frequency, sum), y = frequency, fill = Bias.Category, colour = Bias.Category)) +\n geom_col() +\n  scale_color_viridis(discrete = TRUE, option = \"D\")+\n  scale_fill_viridis(discrete = TRUE) +\n theme_minimal() +\n    scale_y_continuous(limits = c(0, 70)) +\n\n\n  labs(x=\" \", y=\" \", title = \"Bias Motivation Categories for Victims of Single Bias Incidents since 2020\")+\n  coord_flip()+\n  theme(legend.position = \"none\", \n        panel.spacing = unit(1, \"lines\"),\n        plot.title.position = \"plot\", \n        plot.caption.position = \"plot\", \n        plot.caption = element_text(hjust = 0, lineheight = 0.4, size = 8),\n        plot.title = element_text(lineheight = 0.4, face = \"bold\", size = 12, hjust= 0.5),\n        strip.text = element_text(size = 10),\n        axis.text = element_text(size = 6.5),\n        plot.background = element_rect(fill = \"#fafafa\", colour = \"#fafafa\"),\n        panel.background = element_rect(fill = \"#fafafa\", colour = \"#fafafa\"))+\n  \n facet_grid(vars(year), scale= \"free_y\")\n\n\n\n\n\n\n\nShow the code\nbias_t <- bc_type %>%\nfilter(year>= 2020)\n\n\np <- ggplot(bias_t) +\n  aes(x = reorder(Bias.Type,-frequency,sum), y = frequency, colour= factor(Bias.Type), fill= Bias.Type) +\n  geom_col() +\n   scale_color_viridis(discrete = TRUE, option = \"D\")+\n  scale_fill_viridis(discrete = TRUE) +\n  labs(x = \"\", y= \"\", title = \"Frequency of Bias Type Since 2020\") +\n  coord_flip() +\n  theme_minimal() +\ntheme(legend.position = \"none\", \n        panel.background = element_rect(fill = \"#FAFAFA\", colour = \"#FAFAFA\"), \n        plot.background = element_rect(fill = \"#FAFAFA\", colour = \"#FAFAFA\"), \n        legend.background = element_rect(fill = \"transparent\", colour = \"transparent\"), \n        legend.key = element_rect(fill = \"transparent\", colour = \"transparent\"),\n        axis.text = element_text(family = \"dosis\", size= 8),\n        panel.grid.major = element_line(colour = \"#DEDEDE\"), \n        panel.grid.minor = element_blank())\n\np\n\n\n\n\n\n\n\nShow the code\n## Top Five Bias Incidents and Crimes Locations\n\np_loc <- crime %>%\n  filter(year >= 2020)%>%\n  filter(Case.Type== c(\"BIAS CRIME\", \"BIAS INCIDENT\"))%>%\n  group_by(Location.Type)%>%\n  summarise(frequency = n())%>%\nfilter(frequency >= 7)\n\n\n## Graph\n\nloc <- ggplot(p_loc) +\n aes(x = reorder(Location.Type, -frequency, sum), y = frequency, fill= factor(Location.Type)) +\n geom_col(width = 0.5) +\n  \n scale_color_viridis(discrete = TRUE, option = \"D\")+\n  scale_fill_viridis(discrete = TRUE) +\n  \n labs(y = \"Number of Cases\", x= \"\",\n title = \"Top Five Bias Incidents and Crimes Locations\", \n subtitle = \"Year 2020-22\")+\n  \n theme_minimal() +\n  \n theme(\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        legend.position = \"none\",\n        plot.title.position = \"plot\", \n        plot.subtitle = element_text(hjust = 0.5, lineheight = 0.4, size = 10),\n        plot.title = element_text(lineheight = 0.5, face = \"bold\", size = 12, hjust = 0.5),\n        axis.text = element_text(size = 7),\n        axis.title = element_text(size= 8, hjust = 0.5),\n        plot.background = element_rect(fill = \"#fafafa\", colour = \"#fafafa\"),\n        panel.background = element_rect(fill = \"#fafafa\", colour = \"#fafafa\"))\n\n\n\n  loc+transition_states(Location.Type, wrap = FALSE) + shadow_mark() + enter_grow() +  enter_fade()"
  },
  {
    "objectID": "social/FreedomintheWorld/Freedom.html",
    "href": "social/FreedomintheWorld/Freedom.html",
    "title": "Freedom in the World",
    "section": "",
    "text": "The flagship publication of Freedom House, Freedom in the World, is the benchmark for comparing assessments of civil liberties and political rights worldwide. Policymakers, the media, multinational businesses, community activists, and human rights advocates use the survey ratings and narrative reports on 195 countries and 15 connected and disputed territories published annually since 1972.\nThis data visualization explores the Political rights of African countries, where the scores range from 1 to 7, where 7 means more violations of political rights. Note that in 1972, South Africa was rated as â€œWhiteâ€ (2,3 Free) and â€œBlackâ€ (5,6 Not Free).\n\n\nShow the code\n# plot \nggplot(plot_data, aes(x = year, y = PR.x, fill = as.character(PR.y))) +\n  \n  geom_area() +\n  \n  facet_geo(~ country, grid = africa_countries_grid1, label = \"code\") +\n  \n  scale_x_continuous(limits = c(1995, 2020), breaks = c(2000, 2020)) +\n  \n  scale_y_continuous(limits = c(0, 10), breaks = c(0, 5, 10)) +\n  \n  coord_cartesian(expand = F) +\n  \n  labs(title = \"Freedom in the World: Political Rights, African Countries\", \n       subtitle = str_wrap_break(\"\", 70), \n       x = \"\", \n       y = \"\") +\n  \n   scale_fill_viridis_d(\"2020 Political\\nRights Index\") +\n  \n  guides(fill=guide_legend(ncol=4)) +\n  \n  theme_light() +\n   theme(\n     plot.background = element_rect(fill = \"darkgray\", colour = \"darkgray\"),\n        panel.background = element_rect(fill = \"darkgray\", colour = \"darkgray\"), \n     \n        plot.title = element_text(colour = \"black\", size=25, face = \"bold\", hjust = 0.5, family=\"josefin-slab\"), \n  \n        plot.caption = element_text(colour = \"#fafafa\", size=10, hjust = 0.5, family=\"josefin-slab\", \n                                     margin = margin(5, 0, 5, 0)), \n        strip.text = element_text(colour = \"black\", size=12, face= \"bold\",hjust = 0.5, family=\"josefin-slab\"), \n     \n        strip.background = element_rect(fill = \"darkgray\", colour =\"darkgray\"), \n        plot.margin = unit(c(0.3, 0.9, 0.3, 0.3), \"cm\"), \n        legend.background = element_rect(fill = \"darkgray\", colour = \"darkgray\"), \n        legend.key = element_rect(fill = \"darkgray\", colour = \"darkgray\"),\n        legend.position = c(0.85, 0.1),\n     \n        legend.text = element_text(colour = \"black\", size=9, hjust = 0.5, family=\"josefin-slab\"), \n        legend.title = element_text(colour = \"black\", size=10, hjust = 0.5, family=\"josefin-slab\"))"
  },
  {
    "objectID": "social/transit/transit.html",
    "href": "social/transit/transit.html",
    "title": "Transit Costs Project",
    "section": "",
    "text": "Politicians in the twenty-first century frequently advocate for establishing a massive public works program in the United States to create the necessary infrastructure. The Biden administrationâ€™s Bipartisan Infrastructure Bill (BIL) turned this excitement for infrastructure into law after a few tentative steps in the Obama administration and the Trump Administrationâ€™s persistent attempts at Infrastructure Week. BIL proposes spending nearly $1 trillion between fiscal years 2022 and 2026, of which more than $500 billion will be allocated to transportation, including $66 billion for mainline rail and $39 billion for other public transit (National Association of Counties, 2022); when the customary five-year appropriations are taken into account, non-mainline public transit is expected to receive between $91 and $108 billion (FTA n.d.).\nSpending money wisely is essential when such significant quantities are at stake. Governments invest in infrastructure, transportation, and public transit because they see them as opportunities to improve connectivity, spur economic activity, renovate aging infrastructure, cut emissions, produce high-paying jobs, and spark innovation rather than just handing out money to people as welfare (The White House 2022). Regrettably, as we will demonstrate in the graph below, the US transit infrastructure cost is among the highest in the world.\n\n\nShow the code\n#maximum cost per km for each country\ndf <- raw_df %>% group_by(country) %>% top_n(1, cost_km_millions) \n\n#obtain country names\ndf$end_year <- as.numeric(df$end_year) \ndf$country_name <- countrycode(df$country, \"ecb\", \"country.name\")\ndf$country_name[which(df$country == \"UK\")] <- \"United Kingdom\"\n\n#remove NA and get distinct names\ndf <- distinct(df)\ndf <- df %>% drop_na(c(\"country\", \"start_year\", \"end_year\", \"cost_km_millions\"))\n\n#plot\np <- ggplot(df, aes(colour=cost_km_millions)) +\n  #plot project duration\n  geom_segment(aes(x=as.numeric(start_year), xend=as.numeric(end_year), y=country_name, yend=country_name), size=4.5) +\n  #colour by cost\n  scale_colour_gradient2(low = \"lightgray\",\n                        mid = \"#00008B\",\n    high = \"#800080\",\n    midpoint = 2000,\n    space = \"Lab\",\n    guide = \"colourbar\",\n    aesthetics = \"colour\"\n  ) + \n  #order countries \n  scale_y_discrete(limits = factor(df$country_name[order(df$start_year)], levels=df$country_name[order(df$start_year)])) +\n  scale_x_continuous(breaks=seq(min(df$start_year), max(df$end_year), by = 3)) +\n  #note current year\n  geom_segment(aes(x = 2021, y = df$country_name[which.min(df$start_year)], xend = 2021, yend = df$country_name[which.max(df$start_year)]), colour = \"#70284a\", linetype=\"dashed\", size=1.5) +\n  \n  #visualisation\n  theme_classic() +\n  \n  theme(plot.background = element_rect(fill = \"#e6e6fa\"),\n        panel.background = element_rect(fill = \"#e6e6fa\"),\n        legend.background = element_rect(fill =\"#e6e6fa\"),\n        plot.title = element_text(colour = \"black\", size=18, face=\"bold\", hjust = 0.5),\n        plot.subtitle = element_text(colour = \"black\", size=12, hjust = 0.5),\n        axis.text.x= element_text(colour=\"black\", size=10),\n        axis.text.y= element_text(colour=\"black\", size=10)) +\n  xlab(\"\") + ylab(\"\") +  labs(title = \"Transit-infrastructure Costs\", \n                              subtitle = \"Construction timelines for the most expensive transit-infrastructure \\nprojects in each country\",\n                              colour = \"US Dollars \\n(millions)/km\") \n\np\n\n\n\n\n\nThe first completed Case Study can be found on Bostonâ€™s Green Line, although there is data from around the world! The raw data is available here."
  },
  {
    "objectID": "Spanish_index.html",
    "href": "Spanish_index.html",
    "title": "Karol Orozco E.",
    "section": "",
    "text": "Me han fascinado los nÃºmeros y la tecnologÃ­a desde que era una niÃ±a. Tengo una licenciatura en IngenierÃ­a Industrial y actualmente estoy cursando una maestrÃ­a en Ciencia de Datos. Durante mi carrera, he aprendido a recopilar y analizar datos de presupuestos, compensaciÃ³n de empleados, encuestas y tendencias del mercado. Me gusta profundizar en conjuntos de datos complejos y producir recomendaciones estratÃ©gicas perspicaces basadas en datos. Me siento cÃ³modo navegando por bases de datos, asegurando la integridad de los datos y capacitando al personal para usar la tecnologÃ­a.\nTambiÃ©n tengo una MaestrÃ­a en AdministraciÃ³n de Empresas de la Universidad de Willamette y cinco aÃ±os trabajando con instituciones gubernamentales que me han preparado con las herramientas necesarias para prosperar en entornos matriciales complejos.\n\n\nWillamette University | Portland, OR\nMaestrÃ­a en Ciencia de Datos | Agosto 2023\nBeca Presidencial; seleccionado en base a los logros profesionales\n\nWillamette University | Portland, OR\nMaestrÃ­a en AdministraciÃ³n de Empresas | Agosto 2022\nBeca sin fines de lucro George y Colleen Hoyt; seleccionado en base a los logros profesionales\nHonores Atkinson\nBeta Gamma Sigma: La Sociedad de Honor de Negocios Internacionales\n\nUniversidad AutonÃ³ma del Caribe | Barranquilla, Colombia\nLicenciatura, IngenierÃ­a Industrial | Julio 2014\n\n\n\n  \n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Github\n  \n  \n    \n     Email"
  },
  {
    "objectID": "viz/Hollywood Age Gaps/AgeGaps.html",
    "href": "viz/Hollywood Age Gaps/AgeGaps.html",
    "title": "Hollywood Age Gaps",
    "section": "",
    "text": "Show the code\np <- age_gaps %>% \n  select(release_year, age_difference) %>%\n  ggplot(aes(x = release_year,  y = age_difference)) +\n  geom_point(colour = alpha(\"#FF9A00\", 0.5)) +\n  geom_smooth(colour = \"white\", se = FALSE) +\n  labs(y = \"Age gap (years)\",\n       x= \"Release Year\",\n       title =  \"Age Gap in Hollywood Movies from 1935 to 2022\",\n       caption=\"Data from TidyTuesday | Chart by @Karol_Orozco\") +\n  theme_minimal()+\n  theme(text=element_text(family = \"chivo\", color=\"white\"), \n        panel.grid.minor = element_blank(),\n        panel.grid.major = element_line(color=\"black\", size=0.1),\n        axis.title=element_text(face=\"bold\", size= 15),\n        axis.text =element_text(color=\"white\"),\n        axis.title.y=element_text(margin=margin(r=10)),\n        plot.title = element_text(size = 20, face = \"bold\", \n                                  hjust = 0.5, vjust = 0.5),\n        plot.background = element_rect(fill=\"black\"),\n        plot.margin  = margin(20,20,10,10)) \n\n\np"
  },
  {
    "objectID": "viz/Hollywood Age Gaps/AgeGaps.html#reference",
    "href": "viz/Hollywood Age Gaps/AgeGaps.html#reference",
    "title": "Hollywood Age Gaps",
    "section": "Reference",
    "text": "Reference\nThe data this week comes from Hollywood Age Gap via Data Is Plural.\nThomas Mock (2022). Tidy Tuesday: A weekly data project aimed at the R ecosystem.GitHub"
  },
  {
    "objectID": "viz/Hurricanes Graph/hurricanes.html",
    "href": "viz/Hurricanes Graph/hurricanes.html",
    "title": "Hurricanes",
    "section": "",
    "text": "Hello there,\nToday, I will be recreating the graph from the article â€œWhy Past Hurricane Seasons Donâ€™t Tell Us Much About The Futureâ€ by Anna Wiederkehr.\nYou can find the raw data in the National Oceanic and Atmospheric Administrationâ€™s HURDAT2 database, which has records of all Atlantic basin tropical cyclones dating back to 1851.\nThis exercise focuses on practicing our data visualization skills with ggplot2, so Iâ€™ll skip the data manipulation steps. You can find the dataset for the graph in my GitHub repository.\n\n\nShow the code\n## library(ggplot2)\n## library(geomtextpath)\n## library(RCurl)\n## library(grid)\n## library(gridExtra)\n\nx <- getURL(\"https://raw.githubusercontent.com/karolo89/Raw_Data/main/Hurricane.csv\")\nhurricane <- read.csv(text = x)\n\n\n\n\nShow the code\ngraph1 <- ggplot(hurricane, aes(year,average, color = type)) +\n  geom_step(size=.7) +\n  \n## Adding the text\n  geom_textline(aes(label = ifelse(type == \"avg_h_15year\", \n                    \"All Hurricanes\",\n                    \"Major Hurricanes\"), \n                    y = average + .8), \n                text_smoothing = 50, \n                fontface = 2,\n                hjust = 0,  \n                linetype = 0, \n                size = 3) +\n  annotate(\"text\", x = 1886, y = 2.2, label = \"Category 3 - 5\", hjust = 0,\n           color = \"#3b2d74\", size= 2.5) +\n\n## Scales\n  \n  scale_x_continuous(breaks = seq(1860,2020,20)) +\n  scale_y_continuous(limits = c(0,8.5),\n                     breaks = seq(2,8,2),\n                     expand = c(0,0)) +\n  scale_color_manual(values = c(\"#735ad2\", \"#3b2d74\")) +\n\n## Labs\n  \n  labs(\n    y = \" \",\n    x = \" \", \n    title = \"15-year average recorded Atlantic basin hurricanes, 1851-2019\") +\n  \n## Theme \n  \n  theme_minimal() +\n  \n  theme(\n    \n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    \n    ## Customize x axis\n    axis.ticks.x = element_line(colour = \"#e1e1e1\"),\n    axis.ticks.length.x =unit(0.3,\"cm\"),\n    axis.line.x.bottom = element_line(colour = \"#313131\"),\n    \n    ## axis text\n\n    axis.text = element_text(size= 6, color= \"#8f8f8f\", \n                             face = \"bold\"),\n    \n    ## We don't want a legend\n    \n    legend.position = \"none\",\n    \n    ## title\n    plot.title = element_text(size =8, face = \"bold\", colour= \"#545454\",\n                              hjust = 0.50, vjust = 1)\n  )\ngraph1\n\n\n\n\n\nI used the https://imagecolorpicker.com/en tool to get the hex colors from the original graph.\nWe are still missing the footer, so letâ€™s add it.\n\n\nShow the code\nfooter<- grobTree( \n                  textGrob(\"      FiveThirtyEight\", x=unit(.05, \"npc\"), \n                      gp=gpar(col=\"#868686\", \n                              family=\"sans\", \n                              fontsize= 5.5,\n                              fontface= \"bold\"),\n                      hjust=0,\n                      vjust = -2),\n                  \n                  textGrob(\"SOURCES: HURDAT2, VECCHI ET AL      \",\n                        x=unit(1, \"npc\"), \n                        gp=gpar(col=\"#868686\", \n                                family= \"sans\",\n                                fontsize=5.5,\n                              fontface= \"bold\"),\n                        hjust=1,\n                      vjust = -2))\n                  \n# Plot All Together\n\n plt.final <- grid.arrange(graph1, footer, heights=unit(c(0.72, 0.03), \n                                                    c(\"npc\", \"npc\")))"
  },
  {
    "objectID": "viz/Interactive Viz/Interactive Instacart.html",
    "href": "viz/Interactive Viz/Interactive Instacart.html",
    "title": "Interactive Graphics- Instacart",
    "section": "",
    "text": "Show the code\nlibrary(gganimate) \n\nday_week <- orders %>%\n    mutate(day = as.factor(order_dow)) %>%\n    mutate(hour = as.factor(order_hour_of_day)) %>%\n    group_by(day,hour) %>%\n    dplyr::summarise(count = n()) %>%\n    arrange(desc(count))\n\n\nday_weekp <-day_week %>%\n    ggplot(aes(x=day, y=hour))+\n    geom_tile(aes(fill=count), colour = \"white\") + \n  \n    scale_fill_gradient(name= \"Number of\\nOrders\", low = \"#fff1e6\",high = \"#00835C\")+\n  \n    scale_x_discrete( position = \"top\",\n                    breaks = c(\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"),\n                    label = c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\",\"Thursday\", \"Friday\", \"Saturday\"),\n                    expand=c(0,0))+\n  \n   scale_y_discrete( \n                    breaks = c(\"0\", \"6\", \"12\", \"18\", \"23\"),\n                    label = c(\"12am\", \"6am\", \"12pm\", \"6pm\", \"11pm\"),\n                    expand=c(0,0))+\n  \n      labs(title=\"Which Day and What Time\\nDo Customers Order the Most?\",\n         x=\"\", \n         y=\"\",\n         caption = \"Maximum number of orders are placed between 9:00am and 4:00pm on Sunday and Monday. There is also a big number of orders during Firday\\nand Saturday.\")+\n  \n    theme_classic()+\n  \n    theme(\n      \n    axis.line=element_blank(),                                               \n    axis.ticks=element_line(size=0.4),\n    axis.text = element_text(size= 10, color= \"#00835C\"),\n    axis.line.x = element_line(color= \"#00835C\" ),\n    \n    plot.background=element_blank(),         \n    plot.title = element_text(size =10, face = \"bold\", hjust = 0.50, vjust = 1, colour = \"darkgreen\"),\n    plot.caption = element_text(hjust = 0, size = 7, margin = unit(c(0.5, 0.5, 0.5, 0.5), \"cm\"), color = \"#718c9e\"),\n\n    \n    panel.grid = element_blank(),\n    \n    legend.position = \"bottom\",\n    legend.title = element_text(size= 8),\n    legend.margin=margin(grid::unit(0,\"cm\")),\n    legend.key.width=grid::unit(2,\"cm\"),\n    legend.key.height=grid::unit(0.2,\"cm\")\n)\n\nday_weekp+ transition_manual(day, cumulative = TRUE)"
  },
  {
    "objectID": "viz/Interactive Viz/Interactive Instacart.html#plotly",
    "href": "viz/Interactive Viz/Interactive Instacart.html#plotly",
    "title": "Interactive Graphics- Instacart",
    "section": "Plotly",
    "text": "Plotly\n\n\nShow the code\nlibrary(plotly)\n\ntype <- data   %>%\n    group_by(product_id)%>% \n    dplyr::summarize(count = n()) %>% \n    top_n(20, wt = count) %>%\n    left_join(select(products, product_id, product_name, category), by=\"product_id\") %>%\n    arrange(desc(count))\n\n\nbest <- type %>% \n\nggplot(aes(x=reorder(product_name,count), y=count, color= category, text= paste0(product_name, \", Total Orders:\", count)))+    \n  \n  geom_point(size= 2)+\n    geom_segment(aes(x=reorder(product_name,count), \n                     xend=reorder(product_name,count), \n                     y=0, \n                     yend=count), size=0.8)+\n  \n    scale_y_continuous(labels = scales::comma) +\n  \n      labs(title=\"Bestsellers Products\",\n           subtitle = \"Organic vs Non-Organic\",\n           y=\"\", \n           x=\"\", \n           legend = \"\")+\n  \n    scale_color_manual(\"\", values = c(\"#FF8200\", \"#0AAD0A\"))+\n\n  \n    theme_minimal()+\n  \n    theme(\n       axis.text.x= element_text( size= 7),\n       axis.text.y= element_text( size= 8),\n\n       plot.title = element_text(hjust=0.5, size= 12, face = \"bold\", vjust = 0.5),\n       plot.subtitle = element_text(hjust=0.5, size= 9, vjust = 0.5),\n       \n       panel.grid.major.x = element_blank(),\n       panel.grid.major.y = element_blank(),\n       panel.grid.minor.y = element_blank(),\n       panel.grid = element_line(color = \"#e5e5e5\"))+\n  \n  \n  \n     coord_flip()\n\nggplotly(best, tooltip = \"text\")"
  },
  {
    "objectID": "viz/Interactive Viz/Interactive Instacart.html#ggiraph",
    "href": "viz/Interactive Viz/Interactive Instacart.html#ggiraph",
    "title": "Interactive Graphics- Instacart",
    "section": "ggiraph",
    "text": "ggiraph\n\n\nShow the code\nlibrary(ggiraph)\nlibrary(glue)\n\n\ntooltip_css <- \"background-color:#d8118c;color:white;padding:5px;border-radius:3px;\"\nbg_color <- \"#D7E0DA\"\nfont_color <- \"#1f3225\"\n\n\ndf3 <- data%>%\n  select(product_id, reordered, product_name)%>%\n  group_by(product_id, product_name)%>% \n  dplyr::summarize(proportion_reordered = mean(reordered), n=n())\n\n\ngg_scatter <- ggplot(\n  data = df3,\n  mapping = aes(x=n, y=proportion_reordered,\n                \n    # here we add iteractive aesthetics\n    tooltip = paste0(toupper(product_name), \":\", \n                   n))) +\n  geom_jitter_interactive(\n    size = 3, hover_nearest = TRUE)+\n\ngeom_smooth(aes(tooltip=\"smoothed line\", data_id=\"smooth\"), se= FALSE)+  \n  \n  labs(y = \"Proportion of reorders\",\n       title= \"Association between number of orders and probability of reordering\",\n       caption = \"Products with a high number of orders are naturally more likely to be reordered.\\nHowever, there seems to be a ceiling effect.\")+\n  theme_minimal()+\n  theme(text=element_text(family = \"chivo\", color=\"#1f3225\"), \n        panel.grid.minor = element_blank(),\n        panel.grid.major = element_line(color=\"#1f3225\", size=0.1),\n        axis.title=element_text(face=\"bold\", size= 10),\n        axis.text =element_text(color=\"#1f3225\"),\n        axis.title.y=element_text(margin=margin(r=10)),\n        plot.title = element_text(size = 13, face = \"bold\", \n                                  hjust = 0.5, vjust = 0.5),\n        plot.margin  = margin(20,20,20,20))\n\n## Customizing girafe animations\n\ngirafe(\n  ggobj = gg_scatter,\n  bg = bg_color,\n  options = list(\n    opts_tooltip(css = tooltip_css, opacity = 1),\n    opts_sizing(width = .7),\n    opts_zoom(max = 1),\n    opts_hover(\n      css = girafe_css(\n        css = glue(\"fill:{font_color};\"),\n        text = glue(\"stroke:none;fill:{font_color};fill-opacity:1;\")))))"
  },
  {
    "objectID": "viz/Interactive Viz/Interactive Instacart.html#reference",
    "href": "viz/Interactive Viz/Interactive Instacart.html#reference",
    "title": "Interactive Graphics- Instacart",
    "section": "Reference",
    "text": "Reference\nInstacart-market-basket-analysis by Jeremy Staley, Meg Risdal, sharathrao, Will Cukierski publisher by Kaggle, 2017. https://kaggle.com/competitions/instacart-market-basket-analysis"
  },
  {
    "objectID": "viz/Table/tables.html",
    "href": "viz/Table/tables.html",
    "title": "Tables",
    "section": "",
    "text": "The Palmer Archipelago penguins. Artwork by @allison_horst.\n\n\n\n\nShow the code\nlibrary(DT)\n\ndatatable(table,\n\nextensions = c('Select', 'Buttons'), options = list(\n    select = list(style = 'os', items = 'row'),\n    dom = 'Blfrtip',\n    rowId = 0,\n    buttons = c('selectAll', 'selectNone', 'selectRows', 'selectColumns', 'selectCells')\n  ),\n  selection = 'none',\n    \n          caption = htmltools::tags$caption(\n    style = 'caption-side: bottom; text-align: center;',\n    'Table 1: ', htmltools::em('Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network.')\n  ))\n\n\n\n\n\n\n\n\n\n\nGreat Resource: Modify the table output options\n\n\nShow the code\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(downloadthis)\nlibrary(emo)\n\nattach_excel <- table %>%\n  download_this(\n    output_name = \"Penguins\",\n    output_extension = \".xlsx\", # Excel file type\n    button_label = \"Download Excel\",\n    button_type = \"primary\", # change button type\n  )\n\ntable%>%\n  group_by(Penguin)%>%\n  gt(rowname_col = \"Sex\") %>%\n\n  tab_header(\n    title = md(paste0(\n      emo::ji(\"penguin\"), emo::ji(\"penguin\"), emo::ji(\"penguin\"),\n      \"Penguins are fun to summarize!\",\n      emo::ji(\"penguin\"), emo::ji(\"penguin\"), emo::ji(\"penguin\"))),\n  # Use markdown syntax with md()\n    subtitle = md(\"The palmerpenguins: Palmer Archipelago (Antarctica) penguin data\"))%>%\n  \n  \ntab_footnote(\n    footnote = \"Measurements in millimeters (mm)\", \n    locations = cells_column_labels(\n      columns = 4:6 # note\n      ))%>% \n  \ntab_footnote(\n    footnote = \"Measurements in gram (g)\", \n    locations = cells_column_labels(\n      columns = 7:7 # note\n      ))%>% \n  tab_source_note(source_note = \"Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. | Table: Karol Orozco\")%>% \n  \n tab_options(\n    row_group.border.top.width = px(3),\n    row_group.border.top.color = \"gray\",\n    row_group.border.bottom.color = \"gray\",\n    \n    table_body.hlines.color = \"white\",\n    table.border.top.color = \"white\",\n    table.border.top.width = px(3),\n    table.border.bottom.color = \"white\",\n    table.border.bottom.width = px(3),\n    \n    column_labels.border.bottom.color = \"gray\",\n    column_labels.border.bottom.width = px(2),\n    column_labels.background.color = \"black\",\n    \n    heading.background.color = \"#eebf00\",\n    heading.border.bottom.color = \"#eebd00\",\n    heading.border.bottom.width = \"5px\")%>% \n  \n  tab_style(\n    style = cell_text(color = \"white\"),\n    locations = list(\n      cells_row_groups(),\n      cells_column_labels(everything())))%>%\n  \n   tab_style(\n    locations = cells_row_groups(groups = \"Adelie\"),\n    style = cell_fill(color = \"darkorange\"))%>%\n  \n   tab_style(\n    locations = cells_row_groups(groups = \"Chinstrap\"),\n    style = cell_fill(color = \"darkorchid\"))%>%\n  \n   tab_style(\n    locations = cells_row_groups(groups = \"Gentoo\"),\n    style = cell_fill(color = \"cyan4\"))%>%\n\n  \n  tab_source_note(attach_excel)\n\n\n\n\n\n\n  \n    \n      ğŸ§ğŸ§ğŸ§Penguins are fun to summarize!ğŸ§ğŸ§ğŸ§\n    \n    \n      The palmerpenguins: Palmer Archipelago (Antarctica) penguin data\n    \n  \n  \n    \n      \n      Count\n      Culmen_Length1\n      Culmen_Depth1\n      Flipper_Length1\n      Body_Mass2\n    \n  \n  \n    \n      Adelie\n    \n    female\n73\n37.26\n17.62\n187.79\n3368.84\n    male\n73\n40.39\n19.07\n192.41\n4043.49\n    \n      Chinstrap\n    \n    female\n34\n46.57\n17.59\n191.74\n3527.21\n    male\n34\n51.09\n19.25\n199.91\n3938.97\n    \n      Gentoo\n    \n    female\n58\n45.56\n14.24\n212.71\n4679.74\n    male\n61\n49.47\n15.72\n221.54\n5484.84\n  \n  \n    \n      Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. | Table: Karol Orozco\n    \n    \n      \n   Download Excel\n\n    \n  \n  \n    \n      1 Measurements in millimeters (mm)\n    \n    \n      2 Measurements in gram (g)\n    \n  \n\n\n\n\n\n\n\n\nShow the code\ndf3 <- penguins%>%\n  group_by(species) %>% \n  summarize(across(where(is.numeric), mean, na.rm = TRUE))%>% \n  dplyr::mutate_if(is.numeric, round, 2)  %>%\n  rename(Culmen_Length = bill_length_mm , \n         Culmen_Depth = bill_depth_mm,  \n         Flipper_Length= flipper_length_mm, \n         Body_Mass= body_mass_g,\n         Penguin= species)%>%\n  mutate(Photo = case_when(\n    str_detect(Penguin,'Adelie') ~ \"https://github.com/karolo89/karolo_website/blob/main/viz/Table/adellie.png?raw=true\", \n    str_detect(Penguin,'Chinstrap') ~ \"https://github.com/karolo89/karolo_website/blob/main/viz/Table/chinstrap.png?raw=true\",\n    str_detect(Penguin,'Gentoo') ~ \"https://github.com/karolo89/karolo_website/blob/main/viz/Table/gentoo.png?raw=true\"))%>%\n  \n  select(\"Photo\", \"Penguin\", \"Culmen_Length\", \"Culmen_Depth\", \"Flipper_Length\", \"Body_Mass\")\n\ndf3\n\n\n# A tibble: 3 Ã— 6\n  Photo                                  Penguin Culmeâ€¦Â¹ Culmeâ€¦Â² Flippâ€¦Â³ Body_â€¦â´\n  <chr>                                  <fct>     <dbl>   <dbl>   <dbl>   <dbl>\n1 https://github.com/karolo89/karolo_weâ€¦ Adelie     38.8    18.4    190.   3706.\n2 https://github.com/karolo89/karolo_weâ€¦ Chinstâ€¦    48.8    18.4    196.   3733.\n3 https://github.com/karolo89/karolo_weâ€¦ Gentoo     47.6    15      217.   5092.\n# â€¦ with abbreviated variable names Â¹â€‹Culmen_Length, Â²â€‹Culmen_Depth,\n#   Â³â€‹Flipper_Length, â´â€‹Body_Mass\n\n\n\n\nShow the code\ndf3 %>%\n  gt()%>%\n  tab_header(\n    title =  \"Penguins are fun to summarize!\")%>%\n  gtExtras::gt_theme_nytimes()%>%\n  # Add flag images\n  gtExtras::gt_img_rows(columns = Photo, height = 20)%>%\n  opt_align_table_header(align = \"center\")%>% \n  tab_source_note(source_note = \"Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. | Table: Karol Orozco\")%>% \n  \n tab_options(\n\n    table_body.hlines.color = \"white\",\n    table.border.top.color = \"white\",\n    table.border.top.width = px(3),\n    table.border.bottom.color = \"white\",\n    table.border.bottom.width = px(3),\n    \n    column_labels.border.bottom.color = \"gray\",\n    column_labels.border.bottom.width = px(2),\n    column_labels.background.color = \"black\",\n    \n    heading.background.color = \"cyan4\",\n    heading.border.bottom.color = \"cyan4\",\n    heading.border.bottom.width = \"5px\")\n\n\n\n\n\n\n  \n    \n      Penguins are fun to summarize!\n    \n    \n  \n  \n    \n      Photo\n      Penguin\n      Culmen_Length\n      Culmen_Depth\n      Flipper_Length\n      Body_Mass\n    \n  \n  \n    \nAdelie\n38.82\n18.35\n190.10\n3706.16\n    \nChinstrap\n48.83\n18.42\n195.82\n3733.09\n    \nGentoo\n47.57\n15.00\n217.24\n5092.44\n  \n  \n    \n      Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. | Table: Karol Orozco\n    \n  \n  \n\n\n\n\n\n\n\n\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/. doi: 10.5281/zenodo.3960218."
  },
  {
    "objectID": "viz/TidyTuesday/PetCats.html",
    "href": "viz/TidyTuesday/PetCats.html",
    "title": "Pet Cats UK",
    "section": "",
    "text": "This dataset is full of exciting columns that provide insight into the secret lives of cats. I must admit that dogs are my favorite pets, but many of my friends love cats, and I always hear stories about how independent their pets are, which made me want to learn more about them.\nWell, I was born in Colombia, and during my time there, I always remember the cats running and fighting on my roof, which would keep me up at night for a few hours. Then, a few weeks later, I heard tiny meows and noticed the presence of some kittens, which made me suspect that it was another kind of fun that was going on there.\nAccording to the American Society for the Prevention of Cruelty to Animals ASPCA, a female cat can have an average of four to six kittens per litter and may have one to two litters per year. Some cat owners prefer to spay or neuter their cats.\nThe TidyTuesday, weekly data project, posted on January 31st, 2023, a dataset from the Movebank for Animal Tracking Data, which includes catsâ€™ characteristics (such as age, sex, neuter status, hunting habits) and time-stamped GPS pings. Iâ€™ll take a look at the reproductive status of the cats and improve the graph offered by the Twitter user @OluwafemOyedele.\nYou can find the data on the TidyTuesday Github site- â€œPet Cats UKâ€.\nHere is the list of cat names, in case you need some inspiration."
  },
  {
    "objectID": "viz/TidyTuesday/PetCats.html#reference",
    "href": "viz/TidyTuesday/PetCats.html#reference",
    "title": "Pet Cats UK",
    "section": "Reference",
    "text": "Reference\nKays R, Dunn RR, Parsons AW, Mcdonald B, Perkins T, Powers S, Shell L, McDonald JL, Cole H, Kikillus H, Woods L, Tindle H, Roetman P (2020) The small home ranges and large local ecological impacts of pet cats. Animal Conservation. doi:10.1111/acv.12563\nMcDonald JL, Cole H (2020) Data from: The small home ranges and large local ecological impacts of pet cats [United Kingdom]. Movebank Data Repository. doi:10.5441/001/1.pf315732\nThomas Mock (2022). Tidy Tuesday: A weekly data project aimed at the R ecosystem.GitHub"
  },
  {
    "objectID": "social/RegressionModel/RegressionModel.html",
    "href": "social/RegressionModel/RegressionModel.html",
    "title": "House Prices in Portland, OR",
    "section": "",
    "text": "The goal is to build a classification model to predict the type of median housing prices in Portland, OR and its metropolitan area.\n\n\nShow the code\nclean_data %>% \n  count(price_category, \n        name =\"total\") %>%\n  mutate(percent = total/sum(total)*100,\n         percent = round(percent, 2)) %>%\n gt() %>%\n  tab_header(\n    title = \"Portland, OR and its Metropolitan Area Median House Prices\",\n    subtitle = \"Above and below 551,000$\"\n  ) %>%\n  cols_label(\n    price_category = \"Price\",\n    total = \"Total\",\n    percent = \"Percent\"\n  ) %>% \n  fmt_number(\n    columns = vars(total),\n    suffixing = TRUE\n  )  \n\n\n\n\n\n\n  \n    \n      Portland, OR and its Metropolitan Area Median House Prices\n    \n    \n      Above and below 551,000$\n    \n  \n  \n    \n      Price\n      Total\n      Percent\n    \n  \n  \n    above\n7.25K\n50.02\n    below\n7.24K\n49.98\n  \n  \n  \n\n\n\n\n\n\nShow the code\nqmplot(x = longitude, \n       y = latitude, \n       data = clean_data, \n       geom = \"point\", \n       color = price_category, \n       alpha = 0.4) +\n  scale_alpha(guide = 'none')\n\n\n\n\n\nData Splitting\n\n\nShow the code\n# Fix the random numbers by setting the seed \n# This enables the analysis to be reproducible \nset.seed(504)\n\n# Put 3/4 of the data into the training set \ndata_split <- initial_split(houses_pdx, \n                           prop = 3/4)\n\n# Create dataframes for the two sets:\ntrain_data <- training(data_split) \ntest_data <- testing(data_split)\n\n\nValidaton Set\n\n\nShow the code\nhouse_folds <-\n vfold_cv(train_data, \n          v = 5, \n          strata = price_category) \n\n\n\n\nShow the code\npdx_rec <-\n  recipe(price_category ~ .,\n         data = train_data) %>%\n  update_role(longitude, latitude, \n              new_role = \"ID\") %>% \n  \n  step_naomit(everything(), skip = TRUE) %>% \n  \n  step_novel(all_nominal(), -all_outcomes()) %>% # converts all nominal variables to factors and takes care of other issues related to categorical variables.\n  \n  step_normalize(all_numeric(), -all_outcomes(), \n                 -longitude, -latitude) %>% # step_normalize() normalizes (center and scales) the numeric variables to have a standard deviation of one and a mean of zero\n  \n  step_dummy(all_nominal(), -all_outcomes()) %>% #converts our factor columns into numeric binary (0 and 1) variables.\n  \n  step_zv(all_numeric(), -all_outcomes()) %>% ## step_zv(): removes any numeric variables that have zero variance.\n  \n  step_corr(all_predictors(), threshold = 0.7, method = \"spearman\") # step_corr(): will remove predictor variables that have large correlations with other predictor variables.\n\n\n\n\nShow the code\nprep_data <- \n  pdx_rec %>% # use the recipe object\n  prep() %>% # perform the recipe on training data\n  juice() # extract only the preprocessed dataframe"
  },
  {
    "objectID": "social/RegressionModel/RegressionModel.html#get-the-data",
    "href": "social/RegressionModel/RegressionModel.html#get-the-data",
    "title": "House Prices in Portland, OR",
    "section": "Get the Data",
    "text": "Get the Data\n\n\nShow the code\nraw_pdx <- read.csv(\"https://raw.githubusercontent.com/karolo89/Raw_Data/main/PORTLAND%20HOUSE.csv\", stringsAsFactors=TRUE)"
  },
  {
    "objectID": "social/RegressionModel/RegressionModel.html#prepare-the-data",
    "href": "social/RegressionModel/RegressionModel.html#prepare-the-data",
    "title": "House Prices in Portland, OR",
    "section": "Prepare the Data",
    "text": "Prepare the Data\nThis data has 25731 obs. of 32 variables\n\n\nShow the code\n## raw_pdx <- raw_pdx%>%select(-id)\n\nhead(raw_pdx)\n\n\n  id yearBuilt     City latitude longitude zipcode bathrooms bedrooms\n1  1      2007 Fairview 45.54357 -122.4418   97024         3        3\n2  2      2001 Fairview 45.54758 -122.4532   97024         3        3\n3  3      1982  Gresham 45.48823 -122.4444   97080         3        4\n4  4      1953 Portland 45.52663 -122.4641   97230         1        3\n5  5      1967  Gresham 45.51124 -122.4315   97030         3        6\n6  6      1967  Gresham 45.48799 -122.4162   97080         2        3\n  DateListed  DateSold daysOnZillow      homeType lastSoldPrice livingArea\n1  4/26/2021 5/21/2021           25     TOWNHOUSE        315400       1806\n2   3/1/2021 4/23/2021           53 SINGLE_FAMILY        400000       1518\n3  5/24/2021  6/4/2021           11 SINGLE_FAMILY        512000       2724\n4  5/24/2021  6/4/2021           11 SINGLE_FAMILY        348000       1217\n5  5/18/2021  6/1/2021           14     APARTMENT        510000       2400\n6  5/18/2021  6/1/2021           14 SINGLE_FAMILY        404200       1150\n  lotSize  price priceHistory.1.price propertyTaxRate hasCooling hasFireplace\n1    1555 315400               212000            1.12      FALSE         TRUE\n2    3484 400000               375000            1.12       TRUE         TRUE\n3    9583 512000               479000            1.12       TRUE         TRUE\n4   13939 348000               339500            1.12         NA         TRUE\n5    8545 510000               252450            1.12       TRUE           NA\n6    7000 404200               204500            1.12      FALSE         TRUE\n  hasGarage hasHeating hasView ElementarySchooldistance ElementarySchools\n1     FALSE       TRUE   FALSE                      0.4        Elementary\n2     FALSE       TRUE    TRUE                      1.2        Elementary\n3     FALSE       TRUE    TRUE                      0.8        Elementary\n4     FALSE       TRUE   FALSE                      0.8        Elementary\n5     FALSE       TRUE   FALSE                      0.3        Elementary\n6     FALSE       TRUE   FALSE                      0.4        Elementary\n  ElementarySchoolrating MiddleSchooldistance schoolsMiddlelevel\n1                      5                  1.1             Middle\n2                      5                  1.0             Middle\n3                      5                  1.7             Middle\n4                      2                  0.7             Middle\n5                      2                  0.9             Middle\n6                      2                  0.4             Middle\n  MiddleSchoolsrating HighSchooldistance HighSchoollevel HighSchoolRating\n1                   2                2.6            High                3\n2                   2                3.4            High                3\n3                   6                1.4            High                3\n4                   2                3.8            High                3\n5                   6                0.3            High                3\n6                   6                1.4            High                3\n\n\n\nMissing data\n\n\nShow the code\nclean_data <- raw_pdx %>%\n  filter(!is.na(yearBuilt))%>%\n  filter(!is.na(longitude))%>%\n  filter(!is.na(bedrooms))%>%\n  filter(!is.na(daysOnZillow))%>%\n  filter(!is.na(livingArea))%>%\n  filter(!is.na(priceHistory.1.price))%>%\n  filter(!is.na(hasFireplace))%>%\n  filter(!is.na(latitude))%>%\n  filter(!is.na(hasHeating))%>%\n  filter(!is.na(hasCooling))%>%\n  filter(!is.na(bathrooms))%>%\n  filter(!is.na(lotSize))%>%\n  filter(!is.na(propertyTaxRate))%>%\n  filter(!is.na(ElementarySchooldistance))%>%\n  filter(!is.na(MiddleSchooldistance))%>%\n  filter(!is.na(HighSchooldistance))%>%\n  filter(!is.na(ElementarySchoolrating))%>%\n  filter(!is.na(MiddleSchoolsrating))%>%\n  filter(!is.na(HighSchoolRating))\n\n\n\n\nShow the code\nsummary(clean_data)\n\n\n       id          yearBuilt             City         latitude    \n Min.   :    1   Min.   :   0   Portland   :5232   Min.   :45.26  \n 1st Qu.: 6207   1st Qu.:1965   Beaverton  :1456   1st Qu.:45.42  \n Median :15156   Median :1989   Hillsboro  :1194   Median :45.47  \n Mean   :13622   Mean   :1981   Lake Oswego: 942   Mean   :45.47  \n 3rd Qu.:20440   3rd Qu.:2003   Tigard     : 913   3rd Qu.:45.52  \n Max.   :25730   Max.   :2021   Gresham    : 845   Max.   :45.62  \n                                (Other)    :3906                  \n   longitude         zipcode        bathrooms        bedrooms     \n Min.   :-123.1   97229  :  834   Min.   : 0.00   Min.   : 0.000  \n 1st Qu.:-122.8   97045  :  713   1st Qu.: 2.00   1st Qu.: 3.000  \n Median :-122.7   97007  :  706   Median : 3.00   Median : 3.000  \n Mean   :-122.7   97086  :  632   Mean   : 2.78   Mean   : 3.568  \n 3rd Qu.:-122.6   97123  :  599   3rd Qu.: 3.00   3rd Qu.: 4.000  \n Max.   :-122.3   97068  :  573   Max.   :10.00   Max.   :10.000  \n                  (Other):10431                                   \n     DateListed          DateSold      daysOnZillow              homeType    \n 8/9/2019 :   71   5/28/2021 :  159   Min.   :  1   APARTMENT        :   38  \n 6/4/2021 :   66   6/30/2021 :  142   1st Qu.: 80   CONDO            :  136  \n 3/30/2021:   65   10/30/2020:  137   Median :192   HOME_TYPE_UNKNOWN:    2  \n 8/6/2019 :   64   7/31/2020 :  128   Mean   :187   SINGLE_FAMILY    :13659  \n 8/19/2019:   62   4/30/2021 :  127   3rd Qu.:294   TOWNHOUSE        :  653  \n 8/23/2019:   60   9/30/2020 :  122   Max.   :422                            \n (Other)  :14100   (Other)   :13673                                          \n lastSoldPrice       livingArea       lotSize             price        \n Min.   :    443   Min.   :  416   Min.   :       0   Min.   :    500  \n 1st Qu.: 450000   1st Qu.: 1664   1st Qu.:    4791   1st Qu.: 450000  \n Median : 551000   Median : 2206   Median :    7405   Median : 551000  \n Mean   : 634458   Mean   : 2399   Mean   :   17065   Mean   : 634827  \n 3rd Qu.: 710000   3rd Qu.: 2892   3rd Qu.:   10018   3rd Qu.: 710000  \n Max.   :6300000   Max.   :14014   Max.   :18992160   Max.   :6300000  \n                                                                       \n priceHistory.1.price propertyTaxRate hasCooling      hasFireplace   \n Min.   :    895      Min.   :1.010   Mode :logical   Mode :logical  \n 1st Qu.: 415000      1st Qu.:1.080   FALSE:807       FALSE:645      \n Median : 525000      Median :1.120   TRUE :13681     TRUE :13843    \n Mean   : 594080      Mean   :1.113                                  \n 3rd Qu.: 679992      3rd Qu.:1.130                                  \n Max.   :6888000      Max.   :1.130                                  \n                                                                     \n hasGarage       hasHeating       hasView        ElementarySchooldistance\n Mode :logical   Mode :logical   Mode :logical   Min.   :0.0000          \n FALSE:14150     FALSE:24        FALSE:8574      1st Qu.:0.4000          \n TRUE :338       TRUE :14464     TRUE :5914      Median :0.6000          \n                                                 Mean   :0.8057          \n                                                 3rd Qu.:1.0000          \n                                                 Max.   :9.4000          \n                                                                         \n  ElementarySchools ElementarySchoolrating MiddleSchooldistance\n           :    0   7      :2862           Min.   : 0.000      \n Elementary:13077   5      :2359           1st Qu.: 0.800      \n Primary   : 1411   6      :2323           Median : 1.300      \n                    8      :1606           Mean   : 1.546      \n                    3      :1368           3rd Qu.: 2.100      \n                    4      :1332           Max.   :11.900      \n                    (Other):2638                               \n  schoolsMiddlelevel MiddleSchoolsrating HighSchooldistance HighSchoollevel\n           :    0    5      :2347        Min.   : 0.100         :    0     \n Elementary:    3    8      :2262        1st Qu.: 1.000     High:14488     \n High      :    0    3      :2238        Median : 1.700                    \n Middle    :14485    6      :2104        Mean   : 1.911                    \n                     7      :1655        3rd Qu.: 2.500                    \n                     4      :1595        Max.   :10.800                    \n                     (Other):2287                                          \n HighSchoolRating\n 5      :3804    \n 8      :2365    \n 3      :1962    \n 6      :1787    \n 9      :1746    \n 4      :1230    \n (Other):1594"
  },
  {
    "objectID": "social/RegressionModel/RegressionModel.html#take-a-look-at-the-data",
    "href": "social/RegressionModel/RegressionModel.html#take-a-look-at-the-data",
    "title": "House Prices in Portland, OR",
    "section": "Take a look at the Data",
    "text": "Take a look at the Data\n\n\nShow the code\nclean_data %>% \n  count(price_category, \n        name =\"total\") %>%\n  mutate(percent = total/sum(total)*100,\n         percent = round(percent, 2)) %>%\n gt() %>%\n  tab_header(\n    title = \"Portland, OR and its Metropolitan Area Median House Prices\",\n    subtitle = \"Above and below 551,000$\"\n  ) %>%\n  cols_label(\n    price_category = \"Price\",\n    total = \"Total\",\n    percent = \"Percent\"\n  ) %>% \n  fmt_number(\n    columns = vars(total),\n    suffixing = TRUE\n  )  \n\n\nWarning: Since gt v0.3.0, `columns = vars(...)` has been deprecated.\nâ€¢ Please use `columns = c(...)` instead.\nSince gt v0.3.0, `columns = vars(...)` has been deprecated.\nâ€¢ Please use `columns = c(...)` instead.\n\n\n\n\n\n\n  \n    \n      Portland, OR and its Metropolitan Area Median House Prices\n    \n    \n      Above and below 551,000$\n    \n  \n  \n    \n      Price\n      Total\n      Percent\n    \n  \n  \n    above\n7.25K\n50.02\n    below\n7.24K\n49.98\n  \n  \n  \n\n\n\n\n\n\nShow the code\nqmplot(x = longitude, \n       y = latitude, \n       data = clean_data, \n       geom = \"point\", \n       color = price_category, \n       alpha = 0.4) +\n  scale_alpha(guide = 'none')\n\n\nâ„¹ Using `zoom = 10`\n\n\nâ„¹ Map tiles by Stamen Design, under CC BY 3.0. Data by OpenStreetMap, under ODbL.\n\n\n\n\n\n\n\nShow the code\nhouses_pdx <-\n  clean_data %>% \n  select( # select our predictors\n    longitude, \n    latitude, \n    price_category,\n    bathrooms, \n    yearBuilt, \n    homeType,\n    bedrooms, \n    livingArea, \n    lotSize,\n    MiddleSchooldistance,\n    ElementarySchooldistance,\n    HighSchooldistance)\n\nglimpse(houses_pdx)\n\n\nRows: 14,488\nColumns: 12\n$ longitude                <dbl> -122.4418, -122.4532, -122.4444, -122.4162, -â€¦\n$ latitude                 <dbl> 45.54357, 45.54758, 45.48823, 45.48799, 45.49â€¦\n$ price_category           <fct> below, below, below, below, below, below, belâ€¦\n$ bathrooms                <dbl> 3.0, 3.0, 3.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, â€¦\n$ yearBuilt                <dbl> 2007, 2001, 1982, 1967, 1978, 2018, 2006, 201â€¦\n$ homeType                 <fct> TOWNHOUSE, SINGLE_FAMILY, SINGLE_FAMILY, SINGâ€¦\n$ bedrooms                 <dbl> 3, 3, 4, 3, 3, 4, 3, 3, 4, 4, 3, 2, 4, 3, 4, â€¦\n$ livingArea               <dbl> 1806, 1518, 2724, 1150, 2036, 1947, 1548, 220â€¦\n$ lotSize                  <dbl> 1555, 3484, 9583, 7000, 6969, 4791, 5009, 522â€¦\n$ MiddleSchooldistance     <dbl> 1.1, 1.0, 1.7, 0.4, 2.1, 2.5, 0.5, 1.8, 0.3, â€¦\n$ ElementarySchooldistance <dbl> 0.4, 1.2, 0.8, 0.4, 1.0, 0.3, 0.5, 1.0, 0.1, â€¦\n$ HighSchooldistance       <dbl> 2.6, 3.4, 1.4, 1.4, 1.4, 2.2, 1.5, 1.4, 0.9, â€¦\n\n\n\n\nShow the code\npdx_long <- houses_pdx %>% \n  select(-longitude,-latitude, -homeType, -yearBuilt, -lotSize)%>%\n    pivot_longer(!price_category, names_to = \"features\", values_to = \"values\")\n\n\n# Print the first 10 rows\npdx_long %>% \n  slice_head(n = 10)\n\n\n# A tibble: 10 Ã— 3\n   price_category features                 values\n   <fct>          <chr>                     <dbl>\n 1 below          bathrooms                   3  \n 2 below          bedrooms                    3  \n 3 below          livingArea               1806  \n 4 below          MiddleSchooldistance        1.1\n 5 below          ElementarySchooldistance    0.4\n 6 below          HighSchooldistance          2.6\n 7 below          bathrooms                   3  \n 8 below          bedrooms                    3  \n 9 below          livingArea               1518  \n10 below          MiddleSchooldistance        1  \n\n\n\n\nShow the code\ntheme_set(theme_light())\n\n# Make a box plot for each predictor feature\npdx_long %>% \n  ggplot(mapping = aes(x = price_category, y = values, fill = features)) +\n  geom_boxplot() + \n  facet_wrap(~ features, scales = \"free\", ncol = 4) +\n  scale_color_viridis_d(option = \"plasma\", end = .7) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nData Splitting\n\n\nShow the code\n# Fix the random numbers by setting the seed \n# This enables the analysis to be reproducible \nset.seed(504)\n\n# Put 3/4 of the data into the training set \ndata_split <- initial_split(houses_pdx, \n                           prop = 3/4)\n\n# Create dataframes for the two sets:\ntrain_data <- training(data_split) \ntest_data <- testing(data_split)\n\n\n\n\nValidaton Set\n\n\nShow the code\nhouse_folds <-\n vfold_cv(train_data, \n          v = 5, \n          strata = price_category) \n\n\n\n\nShow the code\npdx_rec <-\n  recipe(price_category ~ .,\n         data = train_data) %>%\n  update_role(longitude, latitude, \n              new_role = \"ID\") %>% \n  \n  step_naomit(everything(), skip = TRUE) %>% \n  \n  step_novel(all_nominal(), -all_outcomes()) %>% # converts all nominal variables to factors and takes care of other issues related to categorical variables.\n  \n  step_normalize(all_numeric(), -all_outcomes(), \n                 -longitude, -latitude) %>% # step_normalize() normalizes (center and scales) the numeric variables to have a standard deviation of one and a mean of zero\n  \n  step_dummy(all_nominal(), -all_outcomes()) %>% #converts our factor columns into numeric binary (0 and 1) variables.\n  \n  step_zv(all_numeric(), -all_outcomes()) %>% ## step_zv(): removes any numeric variables that have zero variance.\n  \n  step_corr(all_predictors(), threshold = 0.7, method = \"spearman\") # step_corr(): will remove predictor variables that have large correlations with other predictor variables.\n\n\n\n\nShow the code\nsummary(pdx_rec)\n\n\n# A tibble: 12 Ã— 4\n   variable                 type      role      source  \n   <chr>                    <list>    <chr>     <chr>   \n 1 longitude                <chr [2]> ID        original\n 2 latitude                 <chr [2]> ID        original\n 3 bathrooms                <chr [2]> predictor original\n 4 yearBuilt                <chr [2]> predictor original\n 5 homeType                 <chr [3]> predictor original\n 6 bedrooms                 <chr [2]> predictor original\n 7 livingArea               <chr [2]> predictor original\n 8 lotSize                  <chr [2]> predictor original\n 9 MiddleSchooldistance     <chr [2]> predictor original\n10 ElementarySchooldistance <chr [2]> predictor original\n11 HighSchooldistance       <chr [2]> predictor original\n12 price_category           <chr [3]> outcome   original\n\n\n\n\nShow the code\nprep_data <- \n  pdx_rec %>% # use the recipe object\n  prep() %>% # perform the recipe on training data\n  juice() # extract only the preprocessed dataframe"
  },
  {
    "objectID": "social/RegressionModel/RegressionModel.html#the-model--logistic-regression",
    "href": "social/RegressionModel/RegressionModel.html#the-model--logistic-regression",
    "title": "House Prices in Portland, OR",
    "section": "The Model- Logistic regression",
    "text": "The Model- Logistic regression\n\n\nShow the code\nlog_spec <- # your model specification\n  logistic_reg() %>%  # model type\n  set_engine(engine = \"glm\") %>%  # model engine\n  set_mode(\"classification\") # model mode\n\n# Show your model specification\nlog_spec\n\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\nShow the code\npdx_wflow <- # new workflow object\n workflow() %>% # use workflow function\n add_recipe(pdx_rec) %>%   # use the new recipe\n add_model(log_spec)   # add your model spec\n\npdx_wflow\n\n\nâ•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPreprocessor: Recipe\nModel: logistic_reg()\n\nâ”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n6 Recipe Steps\n\nâ€¢ step_naomit()\nâ€¢ step_novel()\nâ€¢ step_normalize()\nâ€¢ step_dummy()\nâ€¢ step_zv()\nâ€¢ step_corr()\n\nâ”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\nShow the code\n# save model coefficients for a fitted model object from a workflow\n\nget_model <- function(x) {\n  pull_workflow_fit(x) %>% tidy()\n}\n\n# same as before with one exception\nlog_res_2 <- \n  pdx_wflow %>% \n  fit_resamples(\n    resamples = house_folds, \n    metrics = metric_set(\n      recall, precision, f_meas, \n      accuracy, kap,\n      roc_auc, sens, spec),\n      control = control_resamples(\n      save_pred = TRUE,\n      extract = get_model) # use extract and our new function\n    ) \n\n\n## All of the results can be flattened and collected using:\n\n\n\nall_coef <- map_dfr(log_res_2$.extracts, ~ .x[[1]][[1]])\nfilter(all_coef, term == \"bedrooms\")\n\n\n# A tibble: 5 Ã— 5\n  term     estimate std.error statistic  p.value\n  <chr>       <dbl>     <dbl>     <dbl>    <dbl>\n1 bedrooms   0.0748    0.0424      1.76 0.0777  \n2 bedrooms   0.129     0.0433      2.99 0.00284 \n3 bedrooms   0.116     0.0423      2.74 0.00623 \n4 bedrooms   0.118     0.0425      2.78 0.00550 \n5 bedrooms   0.147     0.0430      3.42 0.000618\n\n\n\nPerformance metrics\nShow performance for every single fold:\n\n\nShow the code\nlog_res_2 %>%  collect_metrics(summarize = FALSE)\n\n\n# A tibble: 40 Ã— 5\n   id    .metric   .estimator .estimate .config             \n   <chr> <chr>     <chr>          <dbl> <chr>               \n 1 Fold1 recall    binary         0.788 Preprocessor1_Model1\n 2 Fold1 precision binary         0.834 Preprocessor1_Model1\n 3 Fold1 f_meas    binary         0.810 Preprocessor1_Model1\n 4 Fold1 accuracy  binary         0.816 Preprocessor1_Model1\n 5 Fold1 kap       binary         0.631 Preprocessor1_Model1\n 6 Fold1 sens      binary         0.788 Preprocessor1_Model1\n 7 Fold1 spec      binary         0.843 Preprocessor1_Model1\n 8 Fold1 roc_auc   binary         0.902 Preprocessor1_Model1\n 9 Fold2 recall    binary         0.783 Preprocessor1_Model1\n10 Fold2 precision binary         0.824 Preprocessor1_Model1\n# â€¦ with 30 more rows\n\n\n\n\nCollect predictions\nTo obtain the actual model predictions, we use the function collect_predictions and save the result as log_pred:\n\n\nShow the code\nlog_pred <- \n  log_res_2 %>%\n  collect_predictions()\n\n\nlog_pred %>% \n  conf_mat(price_category, .pred_class) \n\n\n          Truth\nPrediction above below\n     above  4344   856\n     below  1093  4573\n\n\n\n\nShow the code\nlog_pred %>% \n  conf_mat(price_category, .pred_class) %>% \n  autoplot(type = \"heatmap\")+\n  theme_minimal()\n\n\n\n\n\n\n\nROC Curve\n\n\nShow the code\nlog_pred %>% \n  group_by(id) %>% # id contains our folds\n  roc_curve(price_category, .pred_above) %>% \n  autoplot()+\n  theme_minimal()"
  },
  {
    "objectID": "social/RegressionModel/RegressionModel.html#use-the-workflow-to-train-our-model",
    "href": "social/RegressionModel/RegressionModel.html#use-the-workflow-to-train-our-model",
    "title": "House Prices in Portland, OR",
    "section": "Use the workflow to train our model",
    "text": "Use the workflow to train our model\n\n\nShow the code\npdx_fit <- fit(pdx_wflow, train_data)\n\n\nThis allows us to use the model trained by this workflow to predict labels for our test set, and compare the performance metrics with the basic model we created previously.\n\n\nShow the code\npdx_fit %>% ## display results\npull_workflow_fit() %>%\ntidy()%>%\n  filter(p.value < 0.05)\n\n\n# A tibble: 8 Ã— 5\n  term                 estimate std.error statistic  p.value\n  <chr>                   <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)           -0.426     0.0296    -14.4  5.43e-47\n2 yearBuilt              0.221     0.0482      4.59 4.49e- 6\n3 bedrooms               0.119     0.0381      3.12 1.82e- 3\n4 livingArea            -2.88      0.0689    -41.8  0       \n5 lotSize               -0.0448    0.0202     -2.21 2.68e- 2\n6 MiddleSchooldistance  -0.171     0.0310     -5.52 3.33e- 8\n7 HighSchooldistance     0.0805    0.0303      2.66 7.86e- 3\n8 homeType_TOWNHOUSE     0.998     0.179       5.59 2.24e- 8\n\n\n\n\nShow the code\nlibrary(vip)\n\npdx_fit %>%\nextract_fit_parsnip() %>%\n   vip(num_features = 5)+\n  theme_minimal()\n\n\n\n\n\nThe two most important predictors in whether the median house value is above or below 551,000 dollars were the Living Area and the home type: Townhouse\n\n\nShow the code\n# Make predictions on the test set\npred_results <- test_data %>% \n  select(price_category) %>% \n  bind_cols(pdx_fit %>% \n              predict(new_data = test_data)) %>% \n  bind_cols(pdx_fit %>% \n              predict(new_data = test_data, type = \"prob\"))\n\n# Print the results\npred_results %>% \n  slice_head(n = 10)\n\n\n   price_category .pred_class .pred_above .pred_below\n1           below       above  0.79554296   0.2044570\n2           below       below  0.26469350   0.7353065\n3           below       below  0.47600771   0.5239923\n4           below       below  0.02353063   0.9764694\n5           below       below  0.06272184   0.9372782\n6           below       below  0.10491967   0.8950803\n7           below       below  0.06923625   0.9307637\n8           below       below  0.03314398   0.9668560\n9           below       below  0.04580355   0.9541965\n10          below       above  0.83998089   0.1600191\n\n\nLetâ€™s take a look at the confusion matrix:\n\n\nShow the code\npred_results%>% \n  conf_mat(price_category, .pred_class) %>% \n  autoplot(type = \"heatmap\")+\n  theme_minimal()"
  },
  {
    "objectID": "social/RegressionModel/RegressionModel.html#reference",
    "href": "social/RegressionModel/RegressionModel.html#reference",
    "title": "House Prices in Portland, OR",
    "section": "Reference",
    "text": "Reference\nTidymodels- https://www.tidymodels.org/"
  },
  {
    "objectID": "social/vote/vote.html",
    "href": "social/vote/vote.html",
    "title": "Voter Registrations Are Way, Way Down During The Pandemic",
    "section": "",
    "text": "Letâ€™s practice our visualization skills with ggplot2 and recreate the graph from the Voter Registrations Are Way, Way Down During The Pandemic article.\n\n\nShow the code\nggplot(vreg, aes(x= Month, y= change, fill = Color))+\ngeom_col()+ \ngeom_hline( yintercept = 0, color= \"black\")+\n\ngeom_rect(data = data.frame(Jurisdiction = \"Arizona\"), \n              aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n              color = \"lightgrey\", \n              fill = \"white\", \n              alpha = 0, \n              linetype = \"dotted\", \n              inherit.aes = FALSE)+\n  geom_rect(data = data.frame(Jurisdiction = \"California\"), \n            aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n            color = \"lightgrey\", \n            fill = \"white\", \n            alpha = 0, \n            linetype = \"dotted\", \n            inherit.aes = FALSE)+\n   geom_rect(data = data.frame(Jurisdiction = \"Colorado\"), \n             aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n             color = \"lightgrey\", \n             fill = \"white\", \n             alpha = 0, \n             linetype = \"dotted\", \n             inherit.aes = FALSE)+\n  geom_rect(data = data.frame(Jurisdiction = \"Delaware\"), \n            aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n            color = \"lightgrey\", fill = \"white\", \n            alpha = 0, \n            linetype = \"dotted\", \n            inherit.aes = FALSE)+\n  geom_rect(data = data.frame(Jurisdiction = \"Florida\"), \n            aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n            color = \"lightgrey\", \n            fill = \"white\", \n            alpha = 0, \n            linetype = \"dotted\", inherit.aes = FALSE)+\n  geom_rect(data = data.frame(Jurisdiction = \"Georgia\"), \n            aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n            color = \"lightgrey\", \n            fill = \"white\", \n            alpha = 0, \n            linetype = \"dotted\", \n            inherit.aes = FALSE)+\n  geom_rect(data = data.frame(Jurisdiction = \"Illinois\"), \n            aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n            color = \"lightgrey\", \n            fill = \"white\", \n            alpha = 0, \n            linetype = \"dotted\", \n            inherit.aes = FALSE)+\n  \n  facet_wrap(~Jurisdiction, scales = \"free_y\")+\n\n  scale_fill_identity(guide= FALSE)+\n  scale_x_discrete(limits=c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\"), \n                   breaks=c(\"Jan\",\"May\"))+\n  scale_y_continuous(labels = label_number_si(a =! 0), n.breaks = 4)+\n\n  labs(\n        x=\"\",\n        y=\"\",\n      title = \"Voter registration dropped dramatically during the pandemic\",\n      subtitle = \"Difference in the number of newly registered voters for each month in 2020 compared to the same month in 2016\",\n      caption = \"Some states treat voters who move between counties within a state as new registrants because they're unregistered from their old county and nearly registered in the new ones.          \",\n      \n      tag= \"FiveThirtyEight\") +\n\n    theme_classic()+\n    theme(\n          axis.line.y=element_blank(),\n          axis.line.x = element_blank(),\n          axis.ticks = element_blank(),\n          axis.text.y = element_text(size = 6.5, color = \"gray\"), \n          axis.text.x = element_text(size= 6.5, color = \"gray\"),\n          \n          \n          plot.title = element_text(size =9, face = \"bold\", hjust = 0.55),\n          plot.title.position = \"plot\",\n          plot.subtitle= element_text(size = 8, hjust = 0.55),\n          \n          plot.caption = element_text(hjust = 0, size = 6, color = \"grey50\",\n                                      margin = margin(r=5)),\n          plot.background = element_rect(fill= \"white\"),\n          plot.tag.position = \"bottom\",\n          plot.tag = element_text(size= 5, color = \"gray\", hjust =0.1, \n                                  margin=margin(t=1, \n                                                r=5, \n                                                b=1, \n                                                l=20, \n                                                unit=\"pt\")),\n          panel.grid.minor.y = element_blank(),\n          panel.grid.major.y = element_line(size= 0.1, color= \"lightgrey\",\n                                            linetype= \"solid\"),\n          panel.background = element_rect(fill = \"white\"),\n          panel.border = element_blank(),\n          panel.spacing = unit(1, \"lines\"),\n          \n          strip.background= element_rect(fill= \"white\", linetype = \"blank\"),\n          strip.text = element_text(color= \"black\", face= \"bold\"),\n          strip.text.x = element_text(face = \"bold\", size= 7),\n\n          \n          legend.title = element_blank(),\n          legend.position = \"none\"\n    \n)"
  },
  {
    "objectID": "social/vote/vote.html#overview",
    "href": "social/vote/vote.html#overview",
    "title": "Voter Registrations Are Way, Way Down During The Pandemic",
    "section": "Overview",
    "text": "Overview\n\n\nShow the code\nggplot(vreg, aes(x= Month, y= change, fill = Color))+\ngeom_col()+ \ngeom_hline( yintercept = 0, color= \"black\")+\n\ngeom_rect(data = data.frame(Jurisdiction = \"Arizona\"), \n              aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n              color = \"lightgrey\", \n              fill = \"white\", \n              alpha = 0, \n              linetype = \"dotted\", \n              inherit.aes = FALSE)+\n  geom_rect(data = data.frame(Jurisdiction = \"California\"), \n            aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n            color = \"lightgrey\", \n            fill = \"white\", \n            alpha = 0, \n            linetype = \"dotted\", \n            inherit.aes = FALSE)+\n   geom_rect(data = data.frame(Jurisdiction = \"Colorado\"), \n             aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n             color = \"lightgrey\", \n             fill = \"white\", \n             alpha = 0, \n             linetype = \"dotted\", \n             inherit.aes = FALSE)+\n  geom_rect(data = data.frame(Jurisdiction = \"Delaware\"), \n            aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n            color = \"lightgrey\", fill = \"white\", \n            alpha = 0, \n            linetype = \"dotted\", \n            inherit.aes = FALSE)+\n  geom_rect(data = data.frame(Jurisdiction = \"Florida\"), \n            aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n            color = \"lightgrey\", \n            fill = \"white\", \n            alpha = 0, \n            linetype = \"dotted\", inherit.aes = FALSE)+\n  geom_rect(data = data.frame(Jurisdiction = \"Georgia\"), \n            aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n            color = \"lightgrey\", \n            fill = \"white\", \n            alpha = 0, \n            linetype = \"dotted\", \n            inherit.aes = FALSE)+\n  geom_rect(data = data.frame(Jurisdiction = \"Illinois\"), \n            aes(xmin = 4.5, xmax= 5.5, ymin= -Inf, ymax = Inf), \n            color = \"lightgrey\", \n            fill = \"white\", \n            alpha = 0, \n            linetype = \"dotted\", \n            inherit.aes = FALSE)+\n  \n  facet_wrap(~Jurisdiction, scales = \"free_y\")+\n\n  scale_fill_identity(guide= FALSE)+\n  scale_x_discrete(limits=c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\"), \n                   breaks=c(\"Jan\",\"May\"))+\n  scale_y_continuous(labels = label_number_si(a =! 0), n.breaks = 4)+\n\n  labs(\n        x=\"\",\n        y=\"\",\n      title = \"Voter registration dropped dramatically during the pandemic\",\n      subtitle = \"Difference in the number of newly registered voters for each month in 2020 compared to the same month in 2016\",\n      caption = \"Some states treat voters who move between counties within a state as new registrants because they're unregistered from their old county and nearly registered in the new ones.          \",\n      \n      tag= \"FiveThirtyEight\") +\n\n    theme_classic()+\n    theme(\n          axis.line.y=element_blank(),\n          axis.line.x = element_blank(),\n          axis.ticks = element_blank(),\n          axis.text.y = element_text(size = 6.5, color = \"gray\"), \n          axis.text.x = element_text(size= 6.5, color = \"gray\"),\n          \n          \n          plot.title = element_text(size =9, face = \"bold\", hjust = 0.55),\n          plot.title.position = \"plot\",\n          plot.subtitle= element_text(size = 8, hjust = 0.55),\n          \n          plot.caption = element_text(hjust = 0, size = 6, color = \"grey50\",\n                                      margin = margin(r=5)),\n          plot.background = element_rect(fill= \"white\"),\n          plot.tag.position = \"bottom\",\n          plot.tag = element_text(size= 5, color = \"gray\", hjust =0.1, \n                                  margin=margin(t=1, \n                                                r=5, \n                                                b=1, \n                                                l=20, \n                                                unit=\"pt\")),\n          panel.grid.minor.y = element_blank(),\n          panel.grid.major.y = element_line(size= 0.1, color= \"lightgrey\",\n                                            linetype= \"solid\"),\n          panel.background = element_rect(fill = \"white\"),\n          panel.border = element_blank(),\n          panel.spacing = unit(1, \"lines\"),\n          \n          strip.background= element_rect(fill= \"white\", linetype = \"blank\"),\n          strip.text = element_text(color= \"black\", face= \"bold\"),\n          strip.text.x = element_text(face = \"bold\", size= 7),\n\n          \n          legend.title = element_blank(),\n          legend.position = \"none\"\n    \n)"
  },
  {
    "objectID": "social/Hurricanes Graph/hurricanes.html",
    "href": "social/Hurricanes Graph/hurricanes.html",
    "title": "Hurricanes",
    "section": "",
    "text": "Hello there,\nToday, I will be recreating the graph from the article â€œWhy Past Hurricane Seasons Donâ€™t Tell Us Much About The Futureâ€ by Anna Wiederkehr.\nYou can find the raw data in the National Oceanic and Atmospheric Administrationâ€™s HURDAT2 database, which has records of all Atlantic basin tropical cyclones dating back to 1851.\nThis exercise focuses on practicing our data visualization skills with ggplot2, so Iâ€™ll skip the data manipulation steps. You can find the dataset for the graph in my GitHub repository.\n\n\nShow the code\n## library(ggplot2)\n## library(geomtextpath)\n## library(RCurl)\n## library(grid)\n## library(gridExtra)\n\nx <- getURL(\"https://raw.githubusercontent.com/karolo89/Raw_Data/main/Hurricane.csv\")\nhurricane <- read.csv(text = x)\n\ngraph1 <- ggplot(hurricane, aes(year,average, color = type)) +\n  geom_step(size=.7) +\n  \n## Adding the text\n  geom_textline(aes(label = ifelse(type == \"avg_h_15year\", \n                    \"All Hurricanes\",\n                    \"Major Hurricanes\"), \n                    y = average + .8), \n                text_smoothing = 50, \n                fontface = 2,\n                hjust = 0,  \n                linetype = 0, \n                size = 3) +\n  annotate(\"text\", x = 1886, y = 2.2, label = \"Category 3 - 5\", hjust = 0,\n           color = \"#3b2d74\", size= 2.5) +\n\n## Scales\n  \n  scale_x_continuous(breaks = seq(1860,2020,20)) +\n  scale_y_continuous(limits = c(0,8.5),\n                     breaks = seq(2,8,2),\n                     expand = c(0,0)) +\n  scale_color_manual(values = c(\"#735ad2\", \"#3b2d74\")) +\n\n## Labs\n  \n  labs(\n    y = \" \",\n    x = \" \", \n    title = \"15-year average recorded Atlantic basin hurricanes, 1851-2019\") +\n  \n## Theme \n  \n  theme_minimal() +\n  \n  theme(\n    \n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    \n    ## Customize x axis\n    axis.ticks.x = element_line(colour = \"#e1e1e1\"),\n    axis.ticks.length.x =unit(0.3,\"cm\"),\n    axis.line.x.bottom = element_line(colour = \"#313131\"),\n    \n    ## axis text\n\n    axis.text = element_text(size= 6, color= \"#8f8f8f\", \n                             face = \"bold\"),\n    \n    ## We don't want a legend\n    \n    legend.position = \"none\",\n    \n    ## title\n    plot.title = element_text(size =8, face = \"bold\", colour= \"#545454\",\n                              hjust = 0.50, vjust = 1)\n  )\ngraph1\n\n\n\n\n\nI used the Image Color Picker tool to get the hex colors from the original graph.\nWe are still missing the footer, so letâ€™s add it.\n\n\nShow the code\nfooter<- grobTree( \n                  textGrob(\"      FiveThirtyEight\", x=unit(.05, \"npc\"), \n                      gp=gpar(col=\"#868686\", \n                              family=\"sans\", \n                              fontsize= 5.5,\n                              fontface= \"bold\"),\n                      hjust=0,\n                      vjust = -2),\n                  \n                  textGrob(\"SOURCES: HURDAT2, VECCHI ET AL      \",\n                        x=unit(1, \"npc\"), \n                        gp=gpar(col=\"#868686\", \n                                family= \"sans\",\n                                fontsize=5.5,\n                              fontface= \"bold\"),\n                        hjust=1,\n                      vjust = -2))\n                  \n# Plot All Together\n\n plt.final <- grid.arrange(graph1, footer, heights=unit(c(0.72, 0.03), \n                                                    c(\"npc\", \"npc\")))"
  },
  {
    "objectID": "social/Spanish/spanish.html",
    "href": "social/Spanish/spanish.html",
    "title": "Karol Orozco",
    "section": "",
    "text": "the easy way. the tradeoff is that itâ€™s harder to save your output.\ndata = read.csv(file.choose())\n\n\nthe harder way. retrieve and set your filepath to wherever your files are. This is known as your â€˜working directory.â€™\ngetwd() setwd(â€œC:/Users/karol/Desktop/census-masterâ€) data = read.csv(â€œoecd.csvâ€)\n\n\nto look at your data, just type its name.\n\n\nwhen you read in data, it becomes a DATA FRAME in R.\ndata\n\n\nbut usually all you want are the column names and the number of rows. Or a summary of the DATA FRAME.\ncolnames(data)\nnrow(data) head(data) summary(data) View(data)\n\n\nYou can isolate a column of a DATA FRAME so that itâ€™s just a VECTOR.\n\n\nThis is easiest using a dollar sign. Then, you can look at some descriptive statistics of the VECTOR.\npop = data$totpop20 pop length(pop) mean(pop)\nmedian(pop) min(pop) max(pop) summary(pop)\n\n\nwhat if we are interested in looking only at regions with unemployment greater than or equal to 10% ?\n\n\nYou can create a new DATA FRAME using SUBSET\n\n\nNote that == is â€œequals exactlyâ€ and != is â€œnot equalsâ€ and non-numeric values such as place names need to be placed in quotes when subsetting\nhi_unemp = subset(data, unemp20 >= 0.1) nrow(hi_unemp) nrow(hi_unemp)/nrow(data)\n\n\nYou can make a very basic histogram, then a fancier one by passing more arguments to the command â€œhistâ€\nhist(data\\(unemp20) hist(data\\)unemp20, breaks = 20, xlim = c(0,max(data$unemp20, na.rm=T)), xlab=â€œ2020 Unemployment Rateâ€, ylab = â€œNumber of Regionsâ€, col = â€œblueâ€, border = â€œredâ€, main = â€œUnemployment in World Regionsâ€)\n\nQUIZ QUESTIONS - CHALLENGE YOURSELF\n\n\n\nWhat is the unemployment rate in the Central Bohemia Region (code CZ02) of the Czech Republic (code CZ)?\n\n\nMake a histogram of 2019 income levels in Italian regions. Italyâ€™s country code is â€œITâ€ and you can use the subset command with â€˜exactly equalsâ€™ . In the plot window, click â€œexport.â€\n\n\n\n\n(2) DESCRIBING DATA IN R\n\n\n\n\n\n\nSome useful tips\nls() # list current objects (data frames, vectors, model outputs, etc.) rm(hi_unemp) # remove an object - the matrix unemp, in this example rm(list=ls()) # remove all objects from memory options(scipen=999) # avoids printing things in scientific notation options(stringsAsFactors=FALSE) # avoids reading in factors when reading in files\n\n\nReturn to the OECD data\nsetwd(â€œC:/Users/kane/Dropbox/usc_emup/R_bootcampâ€) data = read.csv(â€œoecd.csvâ€)\n\n\nLetâ€™s make some percentage - and other - variables using mathematical operations\ndata\\(pctsenior20 = data\\)pop65plus20/data\\(totpop20 summary(data\\)pctsenior20) View(data) # clicking on column headers sorts the data temporarily\n\n\nCompare senior share in 2005 versus 2020\ndata\\(pctsenior05 = data\\)pop65plus05/data\\(totpop05 data\\)pctsrincr = data\\(pctsenior20 - data\\)pctsenior05 hist(data$pctsrincr)\n\n\nSort in code. Actually, this makes a new data frame which sorts by the desired column.\ndata2 = data[order(data\\(pctsrincr),] head(data2) data2 = data[order(-data\\)pctsrincr),] # this repeats the above, but the negative sign makes it in descending order data2[1:10,] # data frames can be sliced using brackets. The format is row, column. This isolates rows 1 through 10, and leaves the columns untouched data2[1:10,1:5] # this isolates rows 1-10 and columns 1-5\n\n\nAdding a rank field\nsrrank = seq(1:length(data2$pctsrincr)) # seq makes a vector from 1 until a specified value srrank data2 = cbind(data2, srrank) # cbind stands for column bind. this pastes srrank to data2 and overwrites data2 with the result.\n\n\nMake comparative histograms\npar(mfrow=c(1,2)) # par stands for graphical parameter. mfrow specifies the number of rows and columns in the plot window hist(data\\(pctsenior05) # left plot hist(data\\)pctsenior20) # right plot par(mfrow=c(1,2)) # doing this again clears the plots hist(data\\(pctsenior05, breaks=seq(0,0.35,0.05), main=\"2005\", ylab=\"World Regions\", xlab=\"Percent Senior\", col=\"forestgreen\") hist(data\\)pctsenior20, breaks=seq(0,0.35,0.05), main=â€œ2020â€, ylab=â€œWorld Regionsâ€, xlab=â€œPercent Seniorâ€, col=â€œdodgerblueâ€)\n\n\nAdding a guideline (abline) to each histogram. can be vertical (v) or horizontal (h)\npar(mfrow=c(1,2)) hist(data\\(pctsenior05, breaks=seq(0,0.35,0.05), main=\"2005\", ylab=\"World Regions\", xlab=\"Percent Senior\", col=\"forestgreen\") abline(v=mean(data\\)pctsenior05, na.rm=T), col=â€œblackâ€, lty=3, lwd=2) hist(data\\(pctsenior20, breaks=seq(0,0.35,0.05), main=\"2020\", ylab=\"World Regions\", xlab=\"Percent Senior\", col=\"dodgerblue\") abline(v=mean(data\\)pctsenior20, na.rm=T), col=â€œblackâ€, lty=3, lwd=2)\n\n\nMake a side-by-side boxplot to compare\nboxplot(data\\(pctsenior05, data\\)pctsenior20)\n\nCORRELATION ANALYSIS\n\n\n\nHow are income and PM2.5 air pollution levels related in world regions?\nsummary(data) View(data) cor.test(data\\(inc19, data\\)pm2pt5_19) # the entire output of a Pearsonâ€™s correlation test. help(cor.test)\n\n\nScatterplot\npar(mfrow=c(1,1)) # reset graphics parameters to a single plot plot(data\\(inc19, data\\)pm2pt5_19) # plot command will plot 2 vectors against each other\n\n\nScatterplot and best-fit line. Actually requires you to run a simple linear regression (lm command)\nx = data\\(inc19 # assign this vector a shorter, more convenient name. y = data\\)pm2pt5_19 model = lm(y ~ x) # runs a 2-variable regression and saves the output to an object Iâ€™ve called â€˜modelâ€™ summary(model) # regression output outcorr = cor.test(data\\(inc19, data\\)pm2pt5_19) # sometimes itâ€™s convenient to assign the output of a test to a model so you can manipulate it later outcorr outcorr$estimate\nplot(x,y, xlab=â€œ2019 Personal Income (\\(USD)\", ylab = \"Average air pollution level (PM2.5)\",  main = \"Income and particulate emissions across world regions\") abline(model) # abline can also add model results to a plot legend(\"topright\", inset=0.045, legend=c(\"r =\", round(outcorr\\)estimate,4)))\n\n\nA cleaned-up version, starting with the line â€œplotâ€ above\nplot(x, y, xlab=â€œ2019 Personal Income (\\(USD)\", ylab = \"Average air pollution level (PM2.5)\",  main = \"Income and particulate emissions across world regions\", pch=20, font.main=4,  ylim=c(0,40), xlim=c(5000,70000)) abline(model, col=\"blue\", lwd=2) rp = vector('expression', 3) rp[1] = substitute(expression(italic(B)[1] == MYVALUE3), list(MYVALUE3=format(summary(model)\\)coefficients[2], dig=4)))[2] rp[2] = substitute(expression(italic(p) == MYVALUE2), list(MYVALUE2=format(summary(model)\\(coefficients[2,4], dig=3)))[2] rp[3] = substitute(expression(italic(r) == MYVALUE), list(MYVALUE=format(sqrt(summary(model)\\)r.squared), dig=4)))[2] legend(â€toprightâ€, legend=rp, bty=â€˜nâ€™) text(data\\(inc19, data\\)pm2pt5_19-1, labels=data$Country, cex=0.5)\n\nDEALING WITH MISSING VALUES\nsummary(data) mean(data\\(broadband19) mean(data\\)broadband19, na.rm=T) data3 = data[!is.na(data$broadband19),] # isolate rows wherein broadband19 is NA, remove them, and save to â€˜data3â€™ nrow(data) nrow(data3) # notice how many fewer observations are now in data4\n\n\n\nOther helpful operations on a data frame\ndata$pctsrincr = NULL # remove this column data4 = data[c(1:4)] # extract just the first 4 columns\n\nQUIZ QUESTIONS - CHALLENGE YOURSELF\n\n\n\n(3) How many world regions have a 2020 population density over 500/sqkm?\n\n\n(4) What is the correlation coefficient between COVID vaccination rates and broadband access in world? Round to 2 decimal places\n\n\n\n\n(3) WORKING WITH MULTIPLE DATA SOURCES\n\n\n\n\n\nMERGE DATASETS FROM DIFFERENT SOURCES AND ANALYZE THEM\n\n\n\nWhat predicts domestic migration in California counties? Housing cost, or job growth?\n\n\nDownload county migration data which I have pre-processed. Data reflect changes ending 7/1 of year listed. Source: CA Depâ€™t of Finance, http://www.dof.ca.gov/Forecasting/Demographics/Estimates/E-6/\ndof = read.csv(â€œca_dof_popchg_e6.csvâ€) head(dof) nrow(dof)\n\n\nDownload Census data the old fashioned way (median home values)\n\n\nGo do data.census.gov then choose Advanced search\n\n\nThen, Geography -> Counties -> California -> check all counties in California\n\n\nThen, enter B25077 in â€˜Table IDâ€™ and hit SEARCH at the bottom-right which is median value of owner-occupied housing units\n\n\nSelect the first entry (itâ€™s a Table)\n\n\nIn the bar above the data, select â€˜Transposeâ€™\n\n\nThen, select â€˜ZIPâ€™ to download (â€˜csvâ€™ wonâ€™t work as a method to prevent you from not downloading metadata)\n\n\nRun this fancy version of read.csv to read in WITHOUT the double-line-header the Bureau puts in. MAKE SURE the file name matches yours!\nacs = read.csv(text=readLines(â€œ./ACSDT5Y2020.B25077_data_with_overlays_2022-05-24T172640.csvâ€)[(-2)]) head(acs)\n\n\nJob info has been processed for you and is available from the bureau of labor statistics (https://data.bls.gov/cew/apps/data_views/data_views.htm#tab=Tables)\nbls = read.csv(â€œblsemp_ca.csvâ€) head(bls)\n\n\nJoin other data to â€˜dofâ€™ data using â€œmatchâ€ (will work regardless of whether data are sorted, similar to a v-lookup in Excel)\n\n\ncreates a new column in â€˜dofâ€™ called â€˜hoval20,â€™ which is equivalent to the column B25077_001E in the acs data\n\n\nrows are matched using the unique id in â€˜dofâ€™ (id) and â€˜acsâ€™ (GEO.id2)\ndof\\(hoval20 = acs\\)B25077_001E[match(dof\\(id, acs\\)GEO_ID)] dof\\(emp21 = bls\\)emp_jun21[match(dof\\(id, bls\\)GEOID)] # same thing for bls data dof\\(emp15 = bls\\)emp_jun15[match(dof\\(id, bls\\)GEOID)] # one time for each row\n\n\nYou can make a percent growth variable using basic math:\ndof\\(popgr1721 = (dof\\)pop21 - dof\\(pop17)/dof\\)pop17 dof\\(empgr1521 = (dof\\)emp21 - dof\\(emp15)/dof\\)emp15\n\nANALYZE THE DATA USING PLOTS AND A REGRESSION MODEL\n\n\n\nCompare the size of a county with its median home value.\n\n\nHistograms help you understand how the data are distributed!\nhist(dof\\(pop21, breaks=20) # Taking the logarithm of non-zero data can get rid of skew issues # Right skew is common in social science data - high values pull up the means. Using a log transformation is particularly useful when comparing data, e.g. understanding the correlation between two variables. hist(log(dof\\)pop21)) boxplot(dof\\(popgr1721, dof\\)empgr1521) # side-by-side boxplot.\n\n\nMake a scatterplot and check the correlation\nx = dof\\(hoval20 y = log(dof\\)pop21) plot(x,y) cor.test(x, y)\n\n\nMake a fancier scatterplot with a best-fit line\nmodel = lm(y ~ x) # stores the results of a simple linear regression in the object â€˜modelâ€™ plot(x, y, ylab=â€œLog of Population, 2021â€, xlab=â€œMedian Home Value, 2020â€, main=â€œCalifornia Countiesâ€) # see help(par) to view all plot options abline(model, col=â€œdodgerblueâ€, lwd=3)\nrp = vector(â€˜expressionâ€™, 3) rp[1] = substitute(expression(italic(B)[1] == MYVALUE3), list(MYVALUE3=format(summary(model)\\(coefficients[2], dig=4)))[2] rp[2] = substitute(expression(italic(p) == MYVALUE2), list(MYVALUE2=format(summary(model)\\)coefficients[2,4], dig=3)))[2] rp[3] = substitute(expression(italic(r) == MYVALUE), list(MYVALUE=round(sqrt(summary(model)$r.squared), 4)))[2] legend(â€œbottomrightâ€, legend=rp, bty=â€˜nâ€™) # can put this at any corner of the plot\n\n\n\n\n(4) SUMMARIZING DATA AT DIFFERENT SPATIAL SCALES\n\n\n\n\nADDITIONAL FUNCTIONALITY WITH PACKAGES\n\n\n\n\nIn addition to the functions included with the base version of R, there are THOUSANDS of packages with added functionality.\n\n\nMaking a package is fairly easy, so if someone develops a new method/technique/visualization/etc., itâ€™s fairly easy to implement in R.\n\n\nVisit a CRAN mirror page to see all the packages available: https://mirror.las.iastate.edu/CRAN/\n\n\nInstall packages using a single line of code (or, Tools -> Install Packages)\ninstall.packages(â€œsfâ€) # a basic mapping package install.packages(â€œdoByâ€) # allows for some kinds of data summarization install.packages(â€œforeignâ€) # packages allows you to read/write .dbf files\n\n\nEach time you use R, youâ€™ll need to activate the package.\n\n\nA good practice is to include this at the beginning of any script which uses the package (this intro script is called a â€˜preambleâ€™)\nlibrary(sf) library(foreign) library(doBy)\n\nSUMMARY BY\n\n\n\nA handy trick is to be able to summarize data at a different level, or spatial scale.\n\n\nFirst, see how many unique values are in the â€˜mpoâ€™ field.\nunique(unlist(dof\\(mpo)) length(unique(unlist(dof\\)mpo)))\n\n\nReturn to the OECD data for a moment to see how many regions are in each country\nsetwd(â€œC:/Users/kane/Dropbox/usc_emup/R_tutorialsâ€) data = read.csv(â€œoecd.csvâ€)\n\n\nMake a sorted table and a barplot of the number of regions per country\ntable(data\\(Country) sort(table(data\\)Country), decreasing=TRUE) # the sort command, with the argument decreasing=TRUE will sort the table in decreasing order. barplot(sort(table(data$Country), decreasing=TRUE), cex.names=0.8, main=â€œNumber of regions per countryâ€) # a barplot can be made (can also add colors, etc.). cex.names makes the text smaller so all the country names fit.\n\n\nUse the summaryBy command (part of the doBy package) to summarize by larger spatial unit\n\n\na new data frame will be created which summarizes the variables to the left of the tilde at the unit defined to the right of the tilde\ndofsum = summaryBy(pop21 + emp21 ~ mpo, data=dof, FUN=sum) # takes the sum total of emp21 and pop21 by MPO dofsum dofmean = summaryBy(hoval20 + immig21 ~ mpo, data=dof, FUN=mean) # mean across counties in each MPO. Ensure this makes sense! dofmean\n\n\nWrite the table output as a .csv to your working directory\nwrite.csv(dofsum, â€œdofsumtable.csvâ€)\n\n\nMake a single barplot by MPO\nbarplot(dofsum\\(pop21.sum, names.arg=dofsum\\)mpo)\n\n\nMake the barplot look better\nbarplot(dofsum\\(pop21.sum, names.arg=dofmean\\)mpo, axes=F, col=â€œdodgerblueâ€, main=â€œ2021 Population by MPO (millions)â€) axis(2, at=seq(0,20000000,5000000), lab=seq(0,20,5)) abline(h=seq(0,20000000,5000000), lty=3, col=â€œdarkgreyâ€) box()\n\n\nSummarize some of the OECD data. Note that rate or percentage data doesnâ€™t collapse as well.\nout = summaryBy(areasqkm + totpop20 + co2cap08 ~ Country, data=data, FUN=sum) out2 = out[order(-out\\(co2cap08.sum),] # sort by CO2 out2 cor.test(out2\\)co2cap08.sum, out2$totpop20.sum) # are larger countries higher emitters?\n\nREGRESSION ANALYSIS - THE VERY BASICS\n\n\n\nthe first command (lm) makes a linear model, and stores the output\n\n\nthe dependent variable goes first, followed by a tilde and the independent variables.\n\n\nfor convenience, you can declare the data frame separately and skip the dollar signs\n\n\nThis shows that net domestic migration in 2021 was a function of smaller population, lower home value, and higher job growth.\nm = lm(dommig21 ~ pop21 + hoval20 + empgr1521, data=dof) summary(m)\n\n\n\n\n(5) MANIPULATING SHAPEFILES\n\n\n\n\n\n\nGIS data often lack efficient or comprehensive data joining capability\n\n\nBy reading and writing the .dbf portion of an ESRI shapefile using the â€˜foreignâ€™ package, this task is a breeze\n\n\nBE VERY CAREFUL not to add/delete rows, reorder, or accidentally overwrite the .dbf with something else, as this may ruin the shapefile\n\n\nRead in the .dbf portion of the California counties shapefile\nshape = read.dbf(â€œcb_2017_ca_county_500k.dbfâ€)\nhead(shape)\n\n\nWhen matching, you may have to watch out for strings, leading zeroes, or other issues that may cause ID fields not to match\n\n\nHere, we can make a new numeric field in the shapefile to match with our dof file\n\n\nThe nested as.numeric(as.character()) commands pretty reliably coerce things into numeric format\nshape\\(GEOID2 = as.numeric(as.character(shape\\)GEOID))\n\n\nMatch a couple of variables from the DOF data, then summarize to see if it worked\nshape\\(pop21 = dof\\)pop21[match(shape\\(GEOID2, dof\\)id_old)] shape\\(hoval20 = dof\\)hoval20[match(shape\\(GEOID2, dof\\)id_old)] shape\\(emp21 = dof\\)emp21[match(shape\\(GEOID2, dof\\)id_old)] summary(shape)\n\n\nCAREFULLY write new dbf. Then youâ€™ll be able to open in a GIS software and map the new field!\nwrite.dbf(shape, â€œcb_2017_ca_county_500k.dbfâ€)\n\n\n\n\n(6) MAPPING MADE SIMPLE(ISH)\n\n\nlibrary(sf) #### SIMPLE MAP USING THE SF PACKAGE #### # Read in a county shapefile # dsn â€œ.â€ indicates the shapefile resides in the current working directory # no file extension (e.g., .shp) is required for the layer input counties = read_sf(dsn = â€œ.â€, layer = â€œcb_2017_ca_county_500kâ€) head(counties) cnty = data.frame(counties) # if you just want to extract the attribute/data table\n\n\n\n\nDraw a chloropleth map of the â€œALANDâ€ field in the counties shapefile\nplot(counties[â€œALANDâ€])\n\n\nPlot a variable which may actually be meaningful\nplot(counties[â€œpop21â€])\n\n\nA more advanced version\n\n\nReview plot options at https://cran.r-project.org/web/packages/sf/vignettes/sf5.html#geometry_with_attributes:_sf\nplot(counties[â€œpop21â€], breaks=â€œjenksâ€, key.pos=2, pal=sf.colors(10), main=â€œ2021 Population in California countiesâ€, axes=TRUE)\n\n\n\n\n(7) Using the Census API\n\n\n\n\n\nInstall TidyCensus & tigris, and load other packages weâ€™ll need to use the Census API\ninstall.packages(â€œtidycensusâ€) install.packages(â€œtigrisâ€) library(tidycensus) library(tigris) library(sf) library(doBy)\n\n\nRequest a key from the Census to access their data\n\n\n\nYouâ€™ll need to request a Census Key to access their API.\n\n\nThis takes only one minute. Then, enter your key below (mine is in there now)\n\n\nhttp://api.census.gov/data/key_signup.html\ncensus_api_key(â€œ5de05eeaa3869c3180eed1726a07e0f27e6c9e20â€, install=TRUE, overwrite=TRUE)\n\n\nExtract your first variable from the decennial census using the function get_decennial\nmedrent00 = get_decennial(geography = â€œstateâ€, variables = â€œH060001â€, sumfile=â€œsf3â€, year = 2000)\n\n\nMake a barplot\nbarplot(medrent00\\(value, names.arg=medrent00\\)NAME, cex.names=0.7, las=2)\n\n\nMake a better barplot\nmedrent00b = medrent00[order(-medrent00\\(value),] # sort descending by making a new data frame barplot(medrent00b\\)value, names.arg=medrent00b$NAME, cex.names=0.7, las=2, main=â€œState median rent, 2000â€, col=â€œdodgerblueâ€) #cex.names makes labels smaller, las=2 rotates them. abline(h=seq(0,700,100), col=â€œdarkgreyâ€, lty=3) box()\n\nFINDING GOOD CENSUS VARIABLES TO USE\n\n\n\nFocusing on the most recent ACS 5-year estimates (2020 at the time of this writing) is a good way to go.\n\n\nAll the Census API available datasets are at https://api.census.gov/data.html\n\n\nSee also, â€œuseful_2020_census_vars_KKguide.xlsxâ€\n\n\nYou can also extract a list of variables using the code here.\nacsvars = load_variables(2020, â€œacs5â€, cache = TRUE) head(acsvars) nrow(acsvars) # tells us how many variables length(unique(unlist(acsvars$concept))) # tells us how many unique â€œconceptsâ€ there are write.csv(acsvars, â€œacsvars.csvâ€) # export to .csv so you explore easily in Excel\n\nASSEMBLE A SET OF TRACT-LEVEL VARIABLES FOR A COUNTY\n\n\n\nGet a single variable (Median rent)\ntr = get_acs(geography=â€œtractâ€, state=â€œCAâ€, county=â€œOrangeâ€, variables=â€œB25031_001â€, year=2020, geometry=TRUE) tr\\(medrent = tr\\)estimate # create a renamed to avoid future confusion tr$estimate = NULL # get rid of the old one\n\n\nAdd additional variables with a single extraction by putting them into a list.\n\n\nTotal population: B00101_001\n\n\nMedian household income: B19013_001\n\n\nMedian age of housing: B25035_001\nvarlist = c(â€œB01001_001â€, â€œB19013_001â€, â€œB25035_001â€, â€œB08101_009â€) tr_plus = get_acs(geography=â€œtractâ€, state=â€œCAâ€, county=â€œOrangeâ€, variables=varlist, year=2020)\n\n\nUse a match command to bind each new variable to the original data frame (vent). name the variables something intuitive.\n\n\nSince the data are stored long, each match operation requires a subset operation first.\ntr_a = subset(tr_plus, variable==â€œB01001_001â€) tr\\(totpop = tr_a\\)estimate[match(tr\\(GEOID, tr_a\\)GEOID)]\ntr_b = subset(tr_plus, variable==â€œB19013_001â€) tr\\(medhhinc = tr_b\\)estimate[match(tr\\(GEOID, tr_b\\)GEOID)]\ntr_c = subset(tr_plus, variable==â€œB25035_001â€) tr\\(medhomeage = tr_c\\)estimate[match(tr\\(GEOID, tr_c\\)GEOID)]\n\n\nRemember you can plot this as a map too!\nplot(tr[â€˜medhhincâ€™])\n\n\nFinally, we can export this to a shapefile so we can use it elsewhere, too.\nst_write(tr, â€œorange_merge.shpâ€)\n\n\n\n\n8.) ALL VARIABLES AT THE TRACT LEVEL\n\n\n\n\n\n\nIâ€™ve built this loop to extract all of â€œKevinâ€™s commonly used variablesâ€ in one step.\n\n\nExtract first variable (total population) + geometry\n\n\nYou can declare your parameters below:\nmystate = â€œCAâ€ mycounty = â€œImperialâ€ myyear = 2020 mysurvey = â€œacs5â€ cnty = get_acs(geography=â€œtractâ€, state=mystate, county=mycounty, variables=â€œB01001_001â€, year=myyear, survey=mysurvey, geometry=TRUE)\n\n\nExtract all other variables\ncolnames(cnty)[4] = â€œtotpopâ€ varlist = c(â€˜B19001_001â€™, â€˜B01002_001â€™, â€˜B03002_001â€™, â€˜B03002_003â€™, â€˜B03002_004â€™, â€˜B03002_012â€™, â€˜B05001_001â€™, â€˜B05001_006â€™, â€˜B07001_001â€™, â€˜B07001_017â€™, â€˜B07001_033â€™, â€˜B07001_049â€™, â€˜B07001_065â€™, â€˜B08014_001â€™, â€˜B08014_002â€™, â€˜B08014_003â€™, â€˜B08014_004â€™, â€˜B08014_005â€™, â€˜B08014_006â€™, â€˜B08014_007â€™, â€˜B08101_001â€™, â€˜B08101_009â€™, â€˜B08101_033â€™, â€˜B08101_017â€™, â€˜B08101_025â€™, â€˜B08101_041â€™, â€˜B08101_049â€™, â€˜B15003_001â€™, â€˜B15003_002â€™, â€˜B15003_003â€™, â€˜B15003_004â€™, â€˜B15003_005â€™, â€˜B15003_006â€™, â€˜B15003_007â€™, â€˜B15003_008â€™, â€˜B15003_009â€™, â€˜B15003_010â€™, â€˜B15003_011â€™, â€˜B15003_012â€™, â€˜B15003_013â€™, â€˜B15003_014â€™, â€˜B15003_015â€™, â€˜B15003_016â€™, â€˜B15003_017â€™, â€˜B15003_018â€™, â€˜B15003_019â€™, â€˜B15003_020â€™, â€˜B15003_021â€™, â€˜B15003_022â€™, â€˜B15003_023â€™, â€˜B15003_024â€™, â€˜B15003_025â€™, â€˜B19001_002â€™, â€˜B19001_003â€™, â€˜B19001_004â€™, â€˜B19001_005â€™, â€˜B19001_006â€™, â€˜B19001_007â€™, â€˜B19001_008â€™, â€˜B19001_009â€™, â€˜B19001_010â€™, â€˜B19001_011â€™, â€˜B19001_012â€™, â€˜B19001_013â€™, â€˜B19001_014â€™, â€˜B19001_015â€™, â€˜B19001_016â€™, â€˜B19001_017â€™, â€˜B19013_001â€™, â€˜B23025_001â€™, â€˜B23025_002â€™, â€˜B25031_001â€™, â€˜B25077_001â€™, â€˜B25034_001â€™, â€˜B25034_002â€™, â€˜B25034_003â€™, â€˜B25034_004â€™, â€˜B25034_005â€™, â€˜B25034_006â€™, â€˜B25034_007â€™, â€˜B25034_008â€™, â€˜B25034_009â€™, â€˜B25034_010â€™, â€˜B25034_011â€™, â€˜B25035_001â€™, â€˜B25024_002â€™, â€˜B25024_003â€™, â€˜B25024_004â€™, â€˜B25024_005â€™, â€˜B25024_006â€™, â€˜B25024_007â€™, â€˜B25024_008â€™, â€˜B25024_009â€™, â€˜B25024_010â€™, â€˜B25024_011â€™, â€˜B25003_001â€™, â€˜B25003_002â€™, â€˜B25003_003â€™, â€˜B01001_003â€™, â€˜B01001_004â€™, â€˜B01001_005â€™, â€˜B01001_006â€™, â€˜B01001_020â€™, â€˜B01001_021â€™, â€˜B01001_022â€™, â€˜B01001_023â€™, â€˜B01001_024â€™, â€˜B01001_025â€™, â€˜B01001_027â€™, â€˜B01001_028â€™, â€˜B01001_029â€™, â€˜B01001_030â€™, â€˜B01001_044â€™, â€˜B01001_045â€™, â€˜B01001_046â€™, â€˜B01001_047â€™, â€˜B01001_048â€™, â€˜B01001_049â€™) cnty2 = get_acs(geography=â€œtractâ€, state=mystate, county=mycounty, variables=varlist, year=myyear, survey=mysurvey, geometry=TRUE) varnames = c(â€˜totHHâ€™, â€˜medageâ€™, â€˜tot4raceâ€™, â€˜wnhâ€™, â€˜bnhâ€™, â€˜hispâ€™, â€˜tot4_citâ€™, â€˜noncitâ€™, â€˜tot4_mvâ€™, â€˜no_mvâ€™, â€˜in_cnty_mvâ€™, â€˜in_st_mvâ€™, â€˜out_st_mvâ€™, â€˜tot4_vehicâ€™, â€˜vehic0â€™, â€˜vehic1â€™, â€˜vehic2â€™, â€˜vehic3â€™, â€˜vehic4â€™, â€˜vehic5plâ€™, â€˜tot4_commâ€™, â€˜comm_sovâ€™, â€˜comm_walkâ€™, â€˜comm_poolâ€™, â€˜comm_transâ€™, â€˜comm_othâ€™, â€˜comm_wahâ€™, â€˜tot4_educâ€™, â€˜educ_noneâ€™, â€˜educ_nursâ€™, â€˜educ_kindâ€™, â€˜educ_1stâ€™, â€˜educ_2ndâ€™, â€˜educ_3rdâ€™, â€˜educ_4thâ€™, â€˜educ_5thâ€™, â€˜educ_6thâ€™, â€˜educ_7thâ€™, â€˜educ_8thâ€™, â€˜educ_9thâ€™, â€˜educ_10thâ€™, â€˜educ_11thâ€™, â€˜educ_12thâ€™, â€˜educ_hsâ€™, â€˜educ_gedâ€™, â€˜educ_cnud1â€™, â€˜educ_someâ€™, â€˜educ_assoâ€™, â€˜educ_bachâ€™, â€˜educ_mastâ€™, â€˜educ_profâ€™, â€˜educ_doctâ€™, â€˜hincund10â€™, â€˜hinc1014â€™, â€˜hinc1519â€™, â€˜hinc2024â€™, â€˜hinc2529â€™, â€˜hinc3034â€™, â€˜hinc3539â€™, â€˜hinc4044â€™, â€˜hinc4549â€™, â€˜hinc5059â€™, â€˜hinc6074â€™, â€˜hinc7599â€™, â€˜hinc100124â€™, â€˜hinc125149â€™, â€˜hinc150199â€™, â€˜hinc200plâ€™, â€˜medhhincâ€™, â€˜tot4_workâ€™, â€˜empâ€™, â€˜medrentâ€™, â€˜medhovalâ€™, â€˜totHUâ€™, â€˜HU2014â€™, â€˜HU1013â€™, â€˜HU9â€™, â€˜HU9099â€™, â€˜HU8089â€™, â€˜HU7079â€™, â€˜HU6069â€™, â€˜HU5059â€™, â€˜HU4049â€™, â€˜HU1939â€™, â€˜medyrbltâ€™, â€˜HU_sfdâ€™, â€˜HU_sfaâ€™, â€˜HU_mf2â€™, â€˜HU_mf3_4â€™, â€˜HU_mf5_9â€™, â€˜HU_mf1019â€™, â€˜HU_mf2049â€™, â€˜HU_mf50plâ€™, â€˜HU_mobâ€™, â€˜HU_boatRVâ€™, â€˜tot4_tenuâ€™, â€˜ownerHHâ€™, â€˜renterHHâ€™, â€˜male0005â€™, â€˜male0509â€™, â€˜male1014â€™, â€˜male1517â€™, â€˜male6566â€™, â€˜male6769â€™, â€˜male7074â€™, â€˜male7579â€™, â€˜male8084â€™, â€˜male8500â€™, â€˜female0005â€™,â€˜female0509â€™, â€˜female1014â€™, â€˜female1517â€™, â€˜female6566â€™, â€˜female6769â€™, â€˜female7074â€™, â€˜female7579â€™, â€˜female8084â€™, â€˜female8500â€™)\nfor(i in 1:length(unique(unlist(cnty2\\(variable)))){  join = subset(cnty2, variable==varlist[i])  cnty[,ncol(cnty)+1] = join\\)estimate[match(cnty\\(GEOID, join\\)GEOID)] colnames(cnty)[ncol(cnty)] = varnames[i]\n}\n\n\nPrep some useful summary variables\ncnty\\(hhinc_sub20 = cnty\\)hincund10 + cnty\\(hinc1014 + cnty\\)hinc1519 cnty\\(hhinc_over100 = cnty\\)hinc100124 + cnty\\(hinc125149 + cnty\\)hinc150199 + cnty\\(hinc200pl cnty\\)BAplus = cnty\\(educ_doct + cnty\\)educ_prof + cnty\\(educ_mast + cnty\\)educ_bach cnty\\(noHS = cnty\\)tot4_educ - cnty\\(BAplus - cnty\\)educ_asso - cnty\\(educ_some - cnty\\)educ_cnud1 - cnty\\(educ_ged - cnty\\)educ_hs cnty\\(age0017 = cnty\\)male0005 + cnty\\(male0509 + cnty\\)male1014 + cnty\\(male1517 + cnty\\)female0005 + cnty\\(female0509 + cnty\\)female1014 + cnty\\(female1517 cnty\\)age65plus = cnty\\(male6566 + cnty\\)male6769 + cnty\\(male7074 + cnty\\)male7579 + cnty\\(male8084 + cnty\\)male8500 + cnty\\(female6566 + cnty\\)female6769 + cnty\\(female7074 + cnty\\)female7579 + cnty\\(female8084 + cnty\\)female8500\n\n\nCarefully write to a .shp\nst_write(cnty, â€œIM_tracts_ACS20.shpâ€)\n\n\nCarefully write to a .csv\nwrite.csv(cnty, â€œIM_tracts_ACS20.csvâ€)\n\n\nView a variable\nplot(cnty[â€˜medhhincâ€™])"
  },
  {
    "objectID": "census.html",
    "href": "census.html",
    "title": "Getting Census data with tidycensus",
    "section": "",
    "text": "Show the code\nv20 <- load_variables(year= 2021,\n                      dataset = \"acs5\",\n                      cache = TRUE)\n\n\nhispanic <- v20%>%\n  filter(grepl(\"HISPANIC OR LATINO\", concept))\n\nwrite.csv(hispanic,\"C:/Users/karol/Desktop/Projects/karolo_website/data/hispanic.csv\", row.names=FALSE)"
  },
  {
    "objectID": "census.html#household-income-and-median-age--2017-2021-5-year-acs",
    "href": "census.html#household-income-and-median-age--2017-2021-5-year-acs",
    "title": "Getting Census data with tidycensus",
    "section": "Household Income and Median Age- 2017-2021 5-year ACS",
    "text": "Household Income and Median Age- 2017-2021 5-year ACS\n\n\nShow the code\nincome <- get_acs(geography = \"county\",    \n                  state = \"OR\",\n                  variables = c(hhincome = \"B19013_001\"))\n\n\nGetting data from the 2017-2021 5-year ACS\n\n\nShow the code\nsummary(income)\n\n\n    GEOID               NAME             variable            estimate    \n Length:36          Length:36          Length:36          Min.   :42095  \n Class :character   Class :character   Class :character   1st Qu.:52293  \n Mode  :character   Mode  :character   Mode  :character   Median :58454  \n                                                          Mean   :60885  \n                                                          3rd Qu.:65843  \n                                                          Max.   :92025  \n      moe      \n Min.   :1208  \n 1st Qu.:2214  \n Median :3271  \n Mean   :3679  \n 3rd Qu.:4725  \n Max.   :8250  \n\n\nShow the code\nmedian_income <-58453.5\n\n## Oregon hh_income median =58453.5, Mean =60885\n\n\nthree_counties <- income %>%\n filter(NAME %in% c(\"Clackamas County, Oregon\", \"Multnomah County, Oregon\", \"Washington County, Oregon\"))"
  },
  {
    "objectID": "census.html#household-income-2017-2021-5-year-acs",
    "href": "census.html#household-income-2017-2021-5-year-acs",
    "title": "Getting Census data with tidycensus",
    "section": "Household Income 2017-2021 5-year ACS",
    "text": "Household Income 2017-2021 5-year ACS\n\n\nShow the code\nincome <- get_acs(geography = \"county\",    \n                  state = \"OR\",\n                  variables = c(hhincome = \"B19013_001\"))\n\nsummary(income)\n\n\n    GEOID               NAME             variable            estimate    \n Length:36          Length:36          Length:36          Min.   :42095  \n Class :character   Class :character   Class :character   1st Qu.:52293  \n Mode  :character   Mode  :character   Mode  :character   Median :58454  \n                                                          Mean   :60885  \n                                                          3rd Qu.:65843  \n                                                          Max.   :92025  \n      moe      \n Min.   :1208  \n 1st Qu.:2214  \n Median :3271  \n Mean   :3679  \n 3rd Qu.:4725  \n Max.   :8250  \n\n\nShow the code\nmedian_income <-58453.5\n\n## Oregon hh_income median =58453.5, Mean =60885\n\n\nthree_counties <- income %>%\n filter(NAME %in% c(\"Clackamas County, Oregon\", \"Multnomah County, Oregon\", \"Washington County, Oregon\"))\n\nhead(three_counties)\n\n\n# A tibble: 3 Ã— 5\n  GEOID NAME                      variable estimate   moe\n  <chr> <chr>                     <chr>       <dbl> <dbl>\n1 41005 Clackamas County, Oregon  hhincome    88517  1424\n2 41051 Multnomah County, Oregon  hhincome    76290  1208\n3 41067 Washington County, Oregon hhincome    92025  1484"
  },
  {
    "objectID": "social/Census/biascrime.html",
    "href": "social/Census/biascrime.html",
    "title": "Census Data",
    "section": "",
    "text": "Show the code\n# Many many variables are included in the ACS. The ACS has 1 and 5 year estimates. Use the following code to see what variables are available:\n\n# Set a year of interest\nthis.year = 2021\n\n# This looks at the 5 year estimates\n# You can also do \"acs1\"\nvars <- load_variables(year = 2021,\n                      dataset = \"acs5\",\n                      cache = TRUE)\n\n# There are 27886 possible variables \ndim(vars)\n\n\n[1] 27886     4\n\n\n\n\nShow the code\n## Names for variable types\n# Gives five year estimates\n\n#OR\n\norDem <- get_acs(geography = \"tract\", year=this.year,\n                  state = \"OR\", \n                  geometry = TRUE,\n                  variables = c(popululation = \"B02001_001\",\n                                median.gross.rent = \"B25064_001\",\n                                median.household.income = \"B19013_001\",\n                                rent.burden = \"B25071_001\",\n                                white = \"B03002_003\", \n                                af.am = \"B03002_004\",\n                                hispanic = \"B03002_012\",\n                                am.ind = \"B03002_005\",\n                                asian = \"B03002_006\",\n                                nh.pi = \"B03002_007\",\n                                multiple = \"B03002_009\",\n                                other = \"B03002_008\"))\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |=================================================================     |  94%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |======================================================================| 100%\n\n\nShow the code\nhead(orDem)\n\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -122.9903 ymin: 44.9539 xmax: -122.9767 ymax: 44.96986\nGeodetic CRS:  NAD83\n        GEOID                                      NAME     variable estimate\n1 41047001607 Census Tract 16.07, Marion County, Oregon popululation     4442\n2 41047001607 Census Tract 16.07, Marion County, Oregon        white     1359\n3 41047001607 Census Tract 16.07, Marion County, Oregon        af.am        0\n4 41047001607 Census Tract 16.07, Marion County, Oregon       am.ind       33\n5 41047001607 Census Tract 16.07, Marion County, Oregon        asian      214\n6 41047001607 Census Tract 16.07, Marion County, Oregon        nh.pi      982\n  moe                       geometry\n1 868 MULTIPOLYGON (((-122.9903 4...\n2 600 MULTIPOLYGON (((-122.9903 4...\n3  13 MULTIPOLYGON (((-122.9903 4...\n4  56 MULTIPOLYGON (((-122.9903 4...\n5 182 MULTIPOLYGON (((-122.9903 4...\n6 465 MULTIPOLYGON (((-122.9903 4...\n\n\nShow the code\n## Weâ€™re going to need to do a little data wrangling so that we have a tidydata format to spread the column named â€˜variableâ€™.\norPct<-as.data.frame(orDem)[,c(1,3:4)]%>%\n  spread(variable, estimate)%>%\n  mutate(checkTot = white+af.am+hispanic+am.ind+ # looks good!\n           asian+nh.pi+multiple+other)%>%\n  mutate(pct.white = white/checkTot,\n         pct.af.am = af.am/checkTot,\n         pct.hispanic = hispanic/checkTot,\n         pct.am.ind = am.ind/checkTot,\n         pct.asian = asian/checkTot,\n         pct.nh.pi = nh.pi/checkTot,\n         pct.multiple = multiple/checkTot, \n         pct.other = other/checkTot, \n         year = this.year)\n\nhead(orPct)\n\n\n        GEOID af.am am.ind asian hispanic median.gross.rent\n1 41001950100    26     10     3      108               844\n2 41001950200     9      9     9      106               706\n3 41001950300    94     77     3       15               625\n4 41001950400    15      0    27      223               787\n5 41001950500    37     56     0      239               643\n6 41001950600     6     83    16      105               731\n  median.household.income multiple nh.pi other popululation rent.burden white\n1                   51211       84    13     0         2579        27.0  2335\n2                   45297      102     5     9         3332        34.8  3083\n3                   42237       41    11     0         2845        35.4  2604\n4                   52843        0     0     0         2829        30.6  2564\n5                   54318      123     0     0         2579        29.5  2124\n6                   48000       53     3     0         2375        26.1  2109\n  checkTot pct.white   pct.af.am pct.hispanic  pct.am.ind   pct.asian\n1     2579 0.9053897 0.010081427  0.041876696 0.003877472 0.001163242\n2     3332 0.9252701 0.002701080  0.031812725 0.002701080 0.002701080\n3     2845 0.9152900 0.033040422  0.005272408 0.027065026 0.001054482\n4     2829 0.9063273 0.005302227  0.078826440 0.000000000 0.009544008\n5     2579 0.8235750 0.014346646  0.092671578 0.021713843 0.000000000\n6     2375 0.8880000 0.002526316  0.044210526 0.034947368 0.006736842\n    pct.nh.pi pct.multiple  pct.other year\n1 0.005040713   0.03257076 0.00000000 2021\n2 0.001500600   0.03061224 0.00270108 2021\n3 0.003866432   0.01441125 0.00000000 2021\n4 0.000000000   0.00000000 0.00000000 2021\n5 0.000000000   0.04769290 0.00000000 2021\n6 0.001263158   0.02231579 0.00000000 2021\n\n\n\n\nShow the code\n# Set a year first\nthis.year = 2021\n\n### BASE PLOT EXAMPLE\n### Washington\nor_tracts <- tracts(state = 'OR', \n                    cb = T, year = this.year)\n\n# This is the structure of spatial data \nhead(or_tracts)\n\n\nSimple feature collection with 6 features and 13 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -122.9903 ymin: 44.88572 xmax: -118.7871 ymax: 45.7816\nGeodetic CRS:  NAD83\n  STATEFP COUNTYFP TRACTCE             AFFGEOID       GEOID   NAME\n1      41      047  001607 1400000US41047001607 41047001607  16.07\n2      41      005  980000 1400000US41005980000 41005980000   9800\n3      41      051  001301 1400000US41051001301 41051001301  13.01\n4      41      067  031516 1400000US41067031516 41067031516 315.16\n5      41      059  950500 1400000US41059950500 41059950500   9505\n6      41      051  005103 1400000US41051005103 41051005103  51.03\n             NAMELSAD STUSPS        NAMELSADCO STATE_NAME LSAD      ALAND\n1  Census Tract 16.07     OR     Marion County     Oregon   CT    1814790\n2   Census Tract 9800     OR  Clackamas County     Oregon   CT 1634590508\n3  Census Tract 13.01     OR  Multnomah County     Oregon   CT     744911\n4 Census Tract 315.16     OR Washington County     Oregon   CT    1879977\n5   Census Tract 9505     OR   Umatilla County     Oregon   CT 1073211610\n6  Census Tract 51.03     OR  Multnomah County     Oregon   CT     433965\n   AWATER                       geometry\n1       0 MULTIPOLYGON (((-122.9903 4...\n2 6684470 MULTIPOLYGON (((-122.266 45...\n3       0 MULTIPOLYGON (((-122.6348 4...\n4       0 MULTIPOLYGON (((-122.8068 4...\n5       0 MULTIPOLYGON (((-119.4346 4...\n6  155315 MULTIPOLYGON (((-122.6776 4...\n\n\nShow the code\nplot(or_tracts)\n\n\n\n\n\nShow the code\n# GGPLOT\nggplot(or_tracts) + \n  geom_sf() + \n  coord_sf()\n\n\n\n\n\n\n\nShow the code\n### GET CENSUS DATA\n### B25077_001E: MEDIAN HOME VALUE\nor <- get_acs(geography = \"tract\", year=this.year,\n              state = \"OR\", \n              variables = \"B25077_001E\")%>%\n  mutate(GEO_ID=paste0(\"1400000US\", GEOID))\n\nhead(or)\n\n\n# A tibble: 6 Ã— 6\n  GEOID       NAME                                  variaâ€¦Â¹ estimâ€¦Â²   moe GEO_ID\n  <chr>       <chr>                                 <chr>     <dbl> <dbl> <chr> \n1 41001950100 Census Tract 9501, Baker County, Oreâ€¦ B25077â€¦  265000 46736 14000â€¦\n2 41001950200 Census Tract 9502, Baker County, Oreâ€¦ B25077â€¦  175200 40020 14000â€¦\n3 41001950300 Census Tract 9503, Baker County, Oreâ€¦ B25077â€¦  140600 22155 14000â€¦\n4 41001950400 Census Tract 9504, Baker County, Oreâ€¦ B25077â€¦  190500 36472 14000â€¦\n5 41001950500 Census Tract 9505, Baker County, Oreâ€¦ B25077â€¦  191100 11383 14000â€¦\n6 41001950600 Census Tract 9506, Baker County, Oreâ€¦ B25077â€¦  237500 46978 14000â€¦\n# â€¦ with abbreviated variable names Â¹â€‹variable, Â²â€‹estimate\n\n\n\n\nShow the code\n## USE GEO_JOIN TO COMBINE SPATIAL DATA AND OTHER DATA FRAMES\n\njoinOR<- geo_join(or_tracts, or, \n                 by_sp=\"GEOID\", by_df=\"GEOID\")\n\n## USE TMAP PACKAGE\ntm_shape(joinOR)+\n  tm_fill(\"estimate\", style = \"quantile\", n=7, palette = \"Greens\")+\n  tm_legend(bg.color=\"white\", bg.alpha=0.6)+\n  tm_style(\"gray\")\n\n\n\n\n\nShow the code\n## SET GEOMETRY\noregon <- get_acs(geography = \"tract\", year=this.year,\n               state = \"OR\",\n               variables = \"B25077_001E\",\n               geometry = TRUE)\n\n#Plotting with MAPVIEW \nmapview(oregon, zcol = \"estimate\", legend = TRUE, \n        lwd=.25)\n\n\n\n\n\n\n\n\n\nShow the code\n## Plotting with LEAFLET\n\n## Leaflet with reactive\npal<-colorNumeric(\"Greens\", domain=0:ceiling(max(oregon$estimate, na.rm=TRUE)))\npopup<-paste(\"Tract: \", as.character(substring(oregon$GEOID, 6, 11)), \"<br>\",\n             \"Median Home Value: \", as.character(oregon$estimate))\nleaflet()%>%\n  addProviderTiles(\"CartoDB.Positron\")%>%\n  addPolygons(data=oregon,\n              fillColor= ~pal(oregon$estimate),\n              fillOpacity = 0.7,\n              weight = 0.4,\n              smoothFactor = 0.2,\n              popup = popup)"
  },
  {
    "objectID": "social/Census/census.html",
    "href": "social/Census/census.html",
    "title": "Census Data",
    "section": "",
    "text": "Show the code\n# Set a year first\nthis.year = 2021\n\n### BASE PLOT EXAMPLE\n### Washington\nor_tracts <- tracts(state = 'OR', \n                    cb = T, year = this.year)\n\n# This is the structure of spatial data \nhead(or_tracts)\n\n\nSimple feature collection with 6 features and 13 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -122.9903 ymin: 44.88572 xmax: -118.7871 ymax: 45.7816\nGeodetic CRS:  NAD83\n  STATEFP COUNTYFP TRACTCE             AFFGEOID       GEOID   NAME\n1      41      047  001607 1400000US41047001607 41047001607  16.07\n2      41      005  980000 1400000US41005980000 41005980000   9800\n3      41      051  001301 1400000US41051001301 41051001301  13.01\n4      41      067  031516 1400000US41067031516 41067031516 315.16\n5      41      059  950500 1400000US41059950500 41059950500   9505\n6      41      051  005103 1400000US41051005103 41051005103  51.03\n             NAMELSAD STUSPS        NAMELSADCO STATE_NAME LSAD      ALAND\n1  Census Tract 16.07     OR     Marion County     Oregon   CT    1814790\n2   Census Tract 9800     OR  Clackamas County     Oregon   CT 1634590508\n3  Census Tract 13.01     OR  Multnomah County     Oregon   CT     744911\n4 Census Tract 315.16     OR Washington County     Oregon   CT    1879977\n5   Census Tract 9505     OR   Umatilla County     Oregon   CT 1073211610\n6  Census Tract 51.03     OR  Multnomah County     Oregon   CT     433965\n   AWATER                       geometry\n1       0 MULTIPOLYGON (((-122.9903 4...\n2 6684470 MULTIPOLYGON (((-122.266 45...\n3       0 MULTIPOLYGON (((-122.6348 4...\n4       0 MULTIPOLYGON (((-122.8068 4...\n5       0 MULTIPOLYGON (((-119.4346 4...\n6  155315 MULTIPOLYGON (((-122.6776 4...\n\n\nShow the code\nplot(or_tracts)\n\n\n\n\n\nShow the code\n# GGPLOT\nggplot(or_tracts) + \n  geom_sf() + \n  coord_sf()\n\n\n\n\n\nShow the code\n### GET CENSUS DATA\n### B25077_001E: MEDIAN HOME VALUE\nor <- get_acs(geography = \"tract\", year=this.year,\n              state = \"OR\", \n              variables = \"B25077_001E\")%>%\n  mutate(GEO_ID=paste0(\"1400000US\", GEOID))\n\nhead(or)\n\n\n# A tibble: 6 Ã— 6\n  GEOID       NAME                                  variaâ€¦Â¹ estimâ€¦Â²   moe GEO_ID\n  <chr>       <chr>                                 <chr>     <dbl> <dbl> <chr> \n1 41001950100 Census Tract 9501, Baker County, Oreâ€¦ B25077â€¦  265000 46736 14000â€¦\n2 41001950200 Census Tract 9502, Baker County, Oreâ€¦ B25077â€¦  175200 40020 14000â€¦\n3 41001950300 Census Tract 9503, Baker County, Oreâ€¦ B25077â€¦  140600 22155 14000â€¦\n4 41001950400 Census Tract 9504, Baker County, Oreâ€¦ B25077â€¦  190500 36472 14000â€¦\n5 41001950500 Census Tract 9505, Baker County, Oreâ€¦ B25077â€¦  191100 11383 14000â€¦\n6 41001950600 Census Tract 9506, Baker County, Oreâ€¦ B25077â€¦  237500 46978 14000â€¦\n# â€¦ with abbreviated variable names Â¹â€‹variable, Â²â€‹estimate\n\n\n\n\nShow the code\njoinrace<- geo_join(or_tracts, orPct, \n                 by_sp=\"GEOID\", by_df=\"GEOID\")\n\n## SET GEOMETRY\noregon <- get_acs(geography = \"tract\", year=this.year,\n               state = \"OR\",\n               variables = \"B25077_001E\",\n               geometry = TRUE)\n\n#Plotting with MAPVIEW \nmapview(oregon, zcol = \"estimate\", legend = TRUE, \n        lwd=.25)\n\n\n\n\n\n\n\n\n\nShow the code\n## USE GEO_JOIN TO COMBINE SPATIAL DATA AND OTHER DATA FRAMES\n\njoinOR<- geo_join(or_tracts, or, \n                 by_sp=\"GEOID\", by_df=\"GEOID\")\n\n## USE TMAP PACKAGE\ntm_shape(joinOR)+\n  tm_fill(\"estimate\", style = \"quantile\", n=7, palette = \"Greens\")+\n  tm_legend(bg.color=\"white\", bg.alpha=0.6)+\n  tm_style(\"gray\")\n\n\n\n\n\nShow the code\n## SET GEOMETRY\noregon <- get_acs(geography = \"tract\", year=this.year,\n               state = \"OR\",\n               variables = \"B25077_001E\",\n               geometry = TRUE)\n\n#Plotting with MAPVIEW \nmapview(oregon, zcol = \"estimate\", legend = TRUE, \n        lwd=.25)\n\n\n\n\n\n\n\n\n\nShow the code\n## Plotting with LEAFLET\n\n## Leaflet with reactive\npal<-colorNumeric(\"Greens\", domain=0:ceiling(max(oregon$estimate, na.rm=TRUE)))\npopup<-paste(\"Tract: \", as.character(substring(oregon$GEOID, 6, 11)), \"<br>\",\n             \"Median Home Value: \", as.character(oregon$estimate))\nleaflet()%>%\n  addProviderTiles(\"CartoDB.Positron\")%>%\n  addPolygons(data=oregon,\n              fillColor= ~pal(oregon$estimate),\n              fillOpacity = 0.7,\n              weight = 0.4,\n              smoothFactor = 0.2,\n              popup = popup)\n\n\n\n\n\n\n\n\nShow the code\n## Total Hispanic/Latino\nor <- get_acs(geography = \"county\", variables = \"B03001_001\", state = 'OR', year = 2021)\n\nor %>%\n ggplot(aes(x = estimate, y = reorder(NAME, estimate))) +\n geom_point(color = \"red\", size = 1) +\n  labs(title = \"Latino Population by county in Oregon\",\n       subtitle = \"2018-2021 American Community Survey\",\n       y = \"\",\n       x = \"ACS estimate\")\n\n\n\n\n\n\n\nShow the code\noregon <- get_acs(geography = \"tract\", year=2021,\n               state = \"OR\",\n               variables = \"B03001_001\",\n               geometry = TRUE)\n\n#Plotting with MAPVIEW \nmapview(oregon, zcol = \"estimate\", legend = TRUE, \n        lwd=.25)"
  },
  {
    "objectID": "social/pollution/pollution.html",
    "href": "social/pollution/pollution.html",
    "title": "Indoor Air Pollution",
    "section": "",
    "text": "Show the code\np\n\n\n\n\n\n\n\nShow the code\n# red box\nrect <- rectGrob(\n  x = unit(0.75, \"in\"),\n  y = unit(1, \"npc\"),\n  width = unit(1, \"in\"),\n  height = unit(0.25, \"in\"),\n  hjust = 0, vjust = 1,\n  gp = gpar(col = \"#E3120B\", fill = \"#E3120B\", alpha = 1)\n)\n\n\nggdraw(p) + draw_grob(rect)"
  }
]