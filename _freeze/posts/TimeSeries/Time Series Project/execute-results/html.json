{
  "hash": "190f9043f86f9a05663d79647e39b73f",
  "result": {
    "markdown": "---\ntitle: \"Time Series\"\nauthor: \"Karol Orozco\"\ndate: \"2022-12-04\"\nimage: \"forecast.jpg\"\n---\n\n\n## Background\n\nIn this project, I will perform Time series analysis using the Zillow Home Value Index (ZHVI) dataset: A smoothed, seasonally adjusted measure of the typical home value and market changes across Portland, OR, four bedroom houses. It reflects the typical value for homes in the 35th to 65th percentile range.\n\nHere is the link: https://www.zillow.com/research/data/\n\n\n\n\n\n## The Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmetrofour <- read.csv(\"https://raw.githubusercontent.com/karolo89/Raw_Data/main/City_zhvi_bdrmcnt_4_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")\n\nstr(metrofour[,c(1:11)])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t13528 obs. of  11 variables:\n $ RegionID   : int  6181 12447 39051 17426 6915 40326 13271 18959 54296 38128 ...\n $ SizeRank   : int  0 1 2 3 4 5 6 7 8 9 ...\n $ RegionName : chr  \"New York\" \"Los Angeles\" \"Houston\" \"Chicago\" ...\n $ RegionType : chr  \"city\" \"city\" \"city\" \"city\" ...\n $ StateName  : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ State      : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ Metro      : chr  \"New York-Newark-Jersey City, NY-NJ-PA\" \"Los Angeles-Long Beach-Anaheim, CA\" \"Houston-The Woodlands-Sugar Land, TX\" \"Chicago-Naperville-Elgin, IL-IN-WI\" ...\n $ CountyName : chr  \"Queens County\" \"Los Angeles County\" \"Harris County\" \"Cook County\" ...\n $ X2000.01.31: num  284723 302701 146377 179997 154391 ...\n $ X2000.02.29: num  286885 303000 146270 180472 154691 ...\n $ X2000.03.31: num  288711 304356 145921 181368 154943 ...\n```\n:::\n:::\n\n\nWe have to make this dataset tidy. Tidy Data is a way of structuring data so that it can be easily understood by people and analyzed by machines.\n\nI need to remove the X at the beginning of the dates (X2000.01.31,X2000.02.29,...)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(metrofour) <- sub(\"^X\", \"\", names(metrofour))\n\nstr(metrofour[,c(1:11)])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t13528 obs. of  11 variables:\n $ RegionID  : int  6181 12447 39051 17426 6915 40326 13271 18959 54296 38128 ...\n $ SizeRank  : int  0 1 2 3 4 5 6 7 8 9 ...\n $ RegionName: chr  \"New York\" \"Los Angeles\" \"Houston\" \"Chicago\" ...\n $ RegionType: chr  \"city\" \"city\" \"city\" \"city\" ...\n $ StateName : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ State     : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ Metro     : chr  \"New York-Newark-Jersey City, NY-NJ-PA\" \"Los Angeles-Long Beach-Anaheim, CA\" \"Houston-The Woodlands-Sugar Land, TX\" \"Chicago-Naperville-Elgin, IL-IN-WI\" ...\n $ CountyName: chr  \"Queens County\" \"Los Angeles County\" \"Harris County\" \"Cook County\" ...\n $ 2000.01.31: num  284723 302701 146377 179997 154391 ...\n $ 2000.02.29: num  286885 303000 146270 180472 154691 ...\n $ 2000.03.31: num  288711 304356 145921 181368 154943 ...\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhouse_price <- metrofour %>% \n  pivot_longer(-c(RegionID, SizeRank, RegionName, RegionType, StateName, State, Metro, CountyName),\n    names_to = \"Monthly\",\n    values_to = \"Price\"\n  ) \nstr(metrofour[,c(1:11)])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t13528 obs. of  11 variables:\n $ RegionID  : int  6181 12447 39051 17426 6915 40326 13271 18959 54296 38128 ...\n $ SizeRank  : int  0 1 2 3 4 5 6 7 8 9 ...\n $ RegionName: chr  \"New York\" \"Los Angeles\" \"Houston\" \"Chicago\" ...\n $ RegionType: chr  \"city\" \"city\" \"city\" \"city\" ...\n $ StateName : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ State     : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ Metro     : chr  \"New York-Newark-Jersey City, NY-NJ-PA\" \"Los Angeles-Long Beach-Anaheim, CA\" \"Houston-The Woodlands-Sugar Land, TX\" \"Chicago-Naperville-Elgin, IL-IN-WI\" ...\n $ CountyName: chr  \"Queens County\" \"Los Angeles County\" \"Harris County\" \"Cook County\" ...\n $ 2000.01.31: num  284723 302701 146377 179997 154391 ...\n $ 2000.02.29: num  286885 303000 146270 180472 154691 ...\n $ 2000.03.31: num  288711 304356 145921 181368 154943 ...\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Converting the Date from factor to character\n\nhouse_clean <- house_price %>%\n            mutate(Monthly_parsed = as.Date(Monthly,\"%Y.%m.%d\"))\n\n\nhouse_clean[[\"Monthly\"]]<- as.character(house_clean$Monthly)\n\nhouse_price[[\"Monthly\"]]<- as.character(house_price $Monthly)\nsummary(house_clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    RegionID         SizeRank      RegionName         RegionType       \n Min.   :  3300   Min.   :    0   Length:3720200     Length:3720200    \n 1st Qu.: 17381   1st Qu.: 3511   Class :character   Class :character  \n Median : 31963   Median : 7196   Mode  :character   Mode  :character  \n Mean   : 51628   Mean   : 8235                                        \n 3rd Qu.: 46317   3rd Qu.:11713                                        \n Max.   :827230   Max.   :28439                                        \n                                                                       \n  StateName            State              Metro            CountyName       \n Length:3720200     Length:3720200     Length:3720200     Length:3720200    \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   Monthly              Price         Monthly_parsed      \n Length:3720200     Min.   :  17773   Min.   :2000-01-31  \n Class :character   1st Qu.: 172701   1st Qu.:2005-09-30  \n Mode  :character   Median : 244810   Median :2011-06-30  \n                    Mean   : 321841   Mean   :2011-07-01  \n                    3rd Qu.: 369075   3rd Qu.:2017-03-31  \n                    Max.   :8337561   Max.   :2022-11-30  \n                    NA's   :1201126                       \n```\n:::\n:::\n\n\nWe see some missing values in the Price variable, but before I deal with those values, I will filter my data to the cities that I am interested the most\n\n\n::: {.cell}\n\n```{.r .cell-code}\npdx_data <- house_clean %>%\n  dplyr:::filter(RegionID== 13373)  %>%\n  dplyr:::filter(Monthly_parsed >= \"2014-01-01\")\n\nsummary(pdx_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    RegionID        SizeRank   RegionName         RegionType       \n Min.   :13373   Min.   :22   Length:107         Length:107        \n 1st Qu.:13373   1st Qu.:22   Class :character   Class :character  \n Median :13373   Median :22   Mode  :character   Mode  :character  \n Mean   :13373   Mean   :22                                        \n 3rd Qu.:13373   3rd Qu.:22                                        \n Max.   :13373   Max.   :22                                        \n  StateName            State              Metro            CountyName       \n Length:107         Length:107         Length:107         Length:107        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   Monthly              Price        Monthly_parsed      \n Length:107         Min.   :410582   Min.   :2014-01-31  \n Class :character   1st Qu.:506597   1st Qu.:2016-04-15  \n Mode  :character   Median :564473   Median :2018-06-30  \n                    Mean   :563372   Mean   :2018-06-30  \n                    3rd Qu.:588606   3rd Qu.:2020-09-15  \n                    Max.   :758797   Max.   :2022-11-30  \n```\n:::\n:::\n\n\nAfter filtering the data, we don't have any missing values\n\n### Coerce to a tsibble with as_tsibble()\n\nA time series can be recorded as a tsibble object in R. tsibble objects extend tidy data frames (tibble objects) by introducing temporal structure, and to do it, we need to declare key and index. In this case, the Monthly_parsed containing the data-time is the index and the RegionID is the key. Other columns can be considered as measured variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsb_pdx <- pdx_data %>%\n                   select(RegionName,RegionID, Monthly_parsed, Price)\n\ntsb_pref_pdx <-tsb_pdx%>%\n  as_tsibble(key= RegionName, index= Monthly_parsed)%>%\n                   index_by(year_month = ~ yearmonth(.))\n\ntsibble_pdx <-tsb_pref_pdx%>%\n  select(-RegionID)%>%\n  as_tsibble(key= RegionName, index= year_month)%>%\n  mutate(Prices = Price/1000)\n```\n:::\n\n\n## Data Visualization\n\nTo visualize the data, I could use the autoplot() command, but I rather to create my graph with ggplot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pdx_house <- tsibble_pdx %>%\n  ggplot(aes(x= year_month, y= Prices)) +\n  geom_line(size=1, color= \"darkgreen\")+\n   \n    labs(y=\"Price in Thousands of Dollars \", \n       x= \" \",\n       title=\" Four Bedroom House Prices in Portland, OR, 2014-2022 \",\n       caption = \"data:https://www.zillow.com/research/data\")+\n  scale_y_continuous(labels=scales::dollar_format())+\n   theme_light()\n\n\nplot_pdx_house \n```\n\n::: {.cell-output-display}\n![](Time-Series-Project_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nData is non- stationary, we can see a trend-cycle component in the graph above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsibble_pdx %>%\ngg_subseries(Price/1000)+\n  labs(y= \"Price in Thousands of Dollars\",\n       x= \"Year\")+theme_minimal()+\n  scale_y_continuous(labels=scales::dollar_format())+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n```\n\n::: {.cell-output-display}\n![](Time-Series-Project_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntsibble_pdx%>%\ngg_season(Price/1000, labels = \"both\")+\n  labs(x= \"\",\n       y= \"Price in Thousands of Dollars \", \n       title=\"Portland's Seasonal Plot\")+\n  \n  scale_y_continuous(labels=scales::dollar_format())+\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Time-Series-Project_files/figure-html/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n## Determining Stationarity\n\nIn our analysis, we use the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test (Kwiatkowski et al., 1992). In this test, the null hypothesis is that the data are stationary, and we look for evidence that the null hypothesis is false. Consequently, small p-values (e.g., less than 0.05) suggest that differencing is required. The test can be computed using the unitroot_kpss() function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsibble_pdx%>%\n  features(Prices, unitroot_kpss)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  RegionName kpss_stat kpss_pvalue\n  <chr>          <dbl>       <dbl>\n1 Portland        1.97        0.01\n```\n:::\n:::\n\n\nThe p-value is reported as 0.01 if it is less than 0.01, and as 0.1 if it is greater than 0.1. In this case, the test statistic (1.946) is bigger than the 1% critical value, so the p-value is less than 0.01, indicating that the null hypothesis is rejected. That is, the data are not stationary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsibble_pdx %>% \n  features(Prices ,unitroot_ndiffs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  RegionName ndiffs\n  <chr>       <int>\n1 Portland        1\n```\n:::\n:::\n\n\nAs we saw from the KPSS tests above, one difference (d) is required to make the tsibble_pdx data stationary.\n\n## Autocorrelation\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsibble_pdx %>%\n  gg_tsdisplay(Prices,\n                     plot_type='partial')+\n       labs(y=\"Thousands of Dollars \", \n       x= \" \")\n```\n\n::: {.cell-output-display}\n![](Time-Series-Project_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nACF does not drop quickly to zero, moreover the value is large and positive (almost 1 in this case). All these are signs of a non-stationary time series. Therefore it should be differenced to obtain a stationary series.\n\nPACF value r1 is almost 1. All other values ri,i \\>1 are small. This is a sign of a non stationary process that should be differenced in order to obtain a stationary series.\n\nThe data are clearly non-stationary, so we will first take a seasonal difference. The seasonally differenced data are shown below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsibble_pdx %>%\n  gg_tsdisplay(difference(Prices, 12),\n               plot_type='partial', lag=36) +\n  labs(title=\"Seasonally differenced\", y=\"\")\n```\n\n::: {.cell-output-display}\n![](Time-Series-Project_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nOur aim now is to find an appropriate ARIMA model based on the ACF and PACF shown in the Double Differenced graph.\n\n## Seasonal Arima Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_fit <- tsibble_pdx%>%\n  model(\n    arima212012 = ARIMA(Prices ~ pdq(2,1,2)+ PDQ(0,1,2)),\n    arima210011 = ARIMA(Prices ~ pdq(2,1,0)+ PDQ(0,1,1)),\n    stepwise = ARIMA(Prices),\n    search = ARIMA(Prices,stepwise=FALSE))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nall_fit %>% pivot_longer(!RegionName,\n            names_to = \"Model name\", \n            values_to = \"Orders\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A mable: 4 x 3\n# Key:     RegionName, Model name [4]\n  RegionName `Model name`                             Orders\n  <chr>      <chr>                                   <model>\n1 Portland   arima212012           <ARIMA(2,1,2)(0,1,2)[12]>\n2 Portland   arima210011           <ARIMA(2,1,0)(0,1,1)[12]>\n3 Portland   stepwise                <ARIMA(3,1,2) w/ drift>\n4 Portland   search       <ARIMA(2,1,3)(0,0,1)[12] w/ drift>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(all_fit) %>% arrange(AICc) %>% select(.model:BIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 6\n  .model      sigma2 log_lik   AIC  AICc   BIC\n  <chr>        <dbl>   <dbl> <dbl> <dbl> <dbl>\n1 arima212012   2.81   -190.  394.  395.  412.\n2 search        2.14   -191.  399.  400.  420.\n3 stepwise      2.23   -194.  402.  403.  420.\n4 arima210011   4.53   -207.  423.  423.  433.\n```\n:::\n:::\n\n\nOf these models, the best is the ARIMA(2,1,2)(0,1,2)\\[12\\]model (i.e., it has the smallest AICc value).\n\n\n::: {.cell}\n\n```{.r .cell-code}\narima212012 <- tsibble_pdx %>%\n  model(arima212012 = ARIMA(Prices ~ pdq(2,1,2)+ PDQ(0,1,2)))%>%\n  report()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: Prices \nModel: ARIMA(2,1,2)(0,1,2)[12] \n\nCoefficients:\n         ar1     ar2     ma1     ma2     sma1    sma2\n      0.5178  0.0341  1.0092  0.9999  -0.8285  0.1308\ns.e.  0.1130  0.1206  0.0573  0.0692   0.1461  0.1514\n\nsigma^2 estimated as 2.813:  log likelihood=-189.9\nAIC=393.8   AICc=395.1   BIC=411.6\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nall_fit %>% select(arima212012) %>%\n  gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](Time-Series-Project_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nall_fit %>% select(\"search\") %>%\n  gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](Time-Series-Project_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(all_fit) %>%\n  filter(.model=='arima212012') %>%\n  features(.innov, ljung_box, lag = 36, dof = 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  RegionName .model      lb_stat lb_pvalue\n  <chr>      <chr>         <dbl>     <dbl>\n1 Portland   arima212012    36.7     0.186\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntsibble_pdx %>%\n  model(ARIMA(Prices ~ pdq(2,1,2) + PDQ(0,1,2))) %>%\n  forecast() %>%\n  autoplot(tsibble_pdx) +\n  labs(y=\" Thousands of $US \",\n       x =\" \",\n       title=\"Forecast from the ARIMA(2,1,2)(0,1,2)[12] model applied to the Portland House Prices data\")+\ntheme_minimal()\n```\n\n::: {.cell-output-display}\n![](Time-Series-Project_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n\n```{.r .cell-code}\n##Price in Thousands of Dollars\ntsibble_pdx %>%\n  model(ARIMA(Prices ~ pdq(2,1,2) + PDQ(0,1,2))) %>%\n  forecast()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A fable: 24 x 5 [1M]\n# Key:     RegionName, .model [1]\n   RegionName .model                                  year_m…¹      Prices .mean\n   <chr>      <chr>                                      <mth>      <dist> <dbl>\n 1 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2022 Dec N(735, 2.9)  735.\n 2 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Jan  N(737, 21)  737.\n 3 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Feb  N(740, 75)  740.\n 4 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Mar N(745, 156)  745.\n 5 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Apr N(750, 255)  750.\n 6 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 May N(755, 365)  755.\n 7 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Jun N(758, 482)  758.\n 8 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Jul N(760, 604)  760.\n 9 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Aug N(762, 727)  762.\n10 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Sep N(764, 852)  764.\n# … with 14 more rows, and abbreviated variable name ¹​year_month\n```\n:::\n:::\n\n\n## ETS\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_ets <- tsibble_pdx %>%\n  model(ETS(Prices))\nreport(fit_ets)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: Prices \nModel: ETS(M,Ad,N) \n  Smoothing parameters:\n    alpha = 0.9997705 \n    beta  = 0.9997704 \n    phi   = 0.9088093 \n\n  Initial states:\n     l[0]    b[0]\n 406.6826 3.75712\n\n  sigma^2:  0\n\n     AIC     AICc      BIC \n659.0909 659.9309 675.1278 \n```\n:::\n:::\n\n\nThe model selected is ETS(M,Ad,N)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncomponents(fit_ets) %>%\n  autoplot() +\n  labs(title = \"ETS(M,Ad,N) components\")\n```\n\n::: {.cell-output-display}\n![](Time-Series-Project_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nBecause this model has multiplicative errors, the innovation residuals are not equivalent to the regular residuals.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_ets %>%\n    augment() %>%\n    select(.innov, .resid) %>%\n    pivot_longer(c(.innov, .resid)) %>%\n    autoplot()+\n   theme_fivethirtyeight()\n```\n\n::: {.cell-output-display}\n![](Time-Series-Project_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_ets%>%\n    gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](Time-Series-Project_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_ets %>%\n  forecast(h = 24) %>%\n  autoplot(tsibble_pdx)+\n\n   theme_light()\n```\n\n::: {.cell-output-display}\n![](Time-Series-Project_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbind_rows(\n    arima212012 %>% accuracy(),\n    fit_ets %>% accuracy()) %>%\n  select(-ME, -MPE, -ACF1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 8\n  RegionName .model      .type     RMSE   MAE  MAPE   MASE  RMSSE\n  <chr>      <chr>       <chr>    <dbl> <dbl> <dbl>  <dbl>  <dbl>\n1 Portland   arima212012 Training  1.52  1.13 0.193 0.0284 0.0308\n2 Portland   ETS(Prices) Training  2.27  1.62 0.274 0.0408 0.0461\n```\n:::\n:::\n\n\nIn this case the ARIMA model seems to be more accurate model based on the test set RMSE, MAPE and MASE.\n",
    "supporting": [
      "Time-Series-Project_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}